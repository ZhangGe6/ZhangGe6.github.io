---
title: 一些基本概念
author: Zhang Ge
date: 2021-01-14 09:54:00 +0800
categories: [专业积累, 基础知识]
tags: []
---

# 动机

一些以往不清楚的基本概念。学习后做下记录。
# 名词
- **网络的depth、width、cardinality**
	
	- `depth`：整个网络的层数
	- `width`：每一层的通道(channel)数
	- `cardinality`:首次出现在ResNeXt中， 原文解释为‘the size of the set of transformations’，可理解为同一层网络中的支路数。
	
	>The architecture of neural networks often specified by the **width** and the **depth** of the networks. The depth of a network is defined as its number of layers (including output layer but excluding input layer); while the width of a network is defined to be the maximal number of nodes in a layer. The number of input nodes, i.e. the input dimension, is denoted as $n$.
	>
	>[参考-The Expressive Power of Neural Networks - NIPS Proceedings](https://papers.nips.cc/paper/7203-the-expressive-power-of-neural-networks-a-view-from-the-width.pdf)

	
	
- **FLOPs**
	
	- floating point operations，意指浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度。
	- 与FLOPS（注意全大写）区分开来，FLOPS是floating point operations per second的缩写，意指每秒浮点运算次数，理解为计算速度。是一个衡量硬件性能的指标。
	
- **正则项-regularizer**

  - 正则化，更形象的名字叫**规则化**，**就是向你的模型加入某些规则，加入先验，缩小解空间，减小求出错误解的可能性。**[参考-知乎](https://www.zhihu.com/question/20924039/answer/73941774)
  
- **分类和回归的区别**

  - <u>解释1</u>：分类问题的输出度量空间是**定性**的，只有分类“正确”与“错误”之分，不存在一个量来衡量正确或错误的**程度**是多大；而回归问题的输出度量空间是**定量**的，有一个量来衡量回归结果和真实情况的定量距离。[参考-知乎](https://www.zhihu.com/question/21329754/answer/204957456)
  - <u>解释2</u>：输入变量与输出变量均为连续变量的预测问题是回归问题；输出变量为有限个离散变量的预测问题成为分类问题；——李航《统计学习方法》
  - 以目标检测的一个ROI的预测为例，对这个ROI所属类别的预测就是一个分类问题，要么预测对，要么预测错，而不说分类错误的程度有多大；对这个ROI的bounding box的预测就是一个回归问题，有定量的方法来度量预测框与实际框之间的距离（如MSE）。
  - 平时所说的分类预测得到的是一个向量，代表预测为各个类别的概率，这实际上是一种分类问题的回归化处理，也给分类的损失函数的构造带来了方便（如交叉熵损失）。


# 结构
- [分组卷积](https://www.cnblogs.com/shine-lee/p/10243114.html)
- 上采样(upsampling)、反池化(unpooling)、反卷积(deconvolution)的理解
	- 待整理，参考[CSDN](https://blog.csdn.net/A_a_ron/article/details/79181108)、[知乎](https://www.zhihu.com/question/328891283)、[distill](https://distill.pub/2016/deconv-checkerboard/)、[Medium](https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1)
- [RNN](https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9)，[LSTM & GRU](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)，同一位作者的两篇Medium博客，配合动画讲得很清晰。

# 操作

- **gumble softmax** [参考1](https://wiki.lzhbrian.me/notes/differientiable-sampling-and-argmax)  [参考2](https://kexue.fm/archives/6705) [参考3](https://zhuanlan.zhihu.com/p/144140006)

