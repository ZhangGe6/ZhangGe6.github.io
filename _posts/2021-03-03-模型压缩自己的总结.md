---
title: 模型压缩自己的总结
author: Zhang Ge
date: 2021-03-03 08:45:00 +0800
categories: [专业积累, 模型压缩]
tags: []
math: true
---



# 关于STE(straight through estimator)
以二值网络为例，对网络进行训练时，需要对参数做$$sign$$操作后参与运算，有

$$
q = sign(r)
$$

但由于$$sign$$函数在$$r=0$$处不可导，在$$r \neq 0$$处导数为$$0$$，因此反向传播在这一步是失效的，不能有效实现参数的更新。

## STE本身的模样

straight through estimator，直通估计。非常直白，顾名思义就是在反向传播的过程中，当梯度传递遇到$$sign$$函数时，直接跳过$$sign$$函数，把二值参数的梯度作为对应的浮点型参数的梯度来进行参数更新，即


$$
g_r = g_q
$$


![](/assets/img/20210309/STE.png)

<center>直通估计(STE)示意</center>

## 改良——“饱和STE”

在BinaryNet中作者发现，对activation进行二值化时（可视为激活函数），当$$sign$$函数的输入的绝对值大于1的时候，将梯度置0，可以得到更好的实验结果，即


$$
g_r = g_q \mathbf{1}_{|r \le 1|}
$$


暂且可以叫做“饱和STE”，这也是后面不少工作对activation沿用的处理方法。

# 关于早期量化文章的迷思

主要是BinaryConnect和BinaryNet这两篇。

- BinaryConnect只对weight使用进行量化。在反向传播过程中，使用最原始的STE。另外，对实值的weights单独加了一个截断函数clip(x,-1,1)。

- BinaryNet同时对weight和activation进行量化。在反向传播过程中，对activation使用饱和STE。对实值的weights同样加了一个截断函数clip(x,-1,1)。

## QA

【Q】：**对weights施加截断函数clip(x,-1,1)的目的？**

【A】：如果实值weights是没有设置边界的，这样它就有可能会一直累加到特别大的值，从而与二值化的weights之间的量化误差越来越大，积重难返，所以作者对实值的weights单独加了一个截断函数clip(x,-1,1)，将其限制在-1和+1之间，这样使得实值weights和二值化weights的距离更近。[参考](https://zhuanlan.zhihu.com/p/270184068)





# 参考

[二值网络，围绕STE的那些事儿](https://zhuanlan.zhihu.com/p/72681647)

[二值化神经网络(BNN)综述](https://zhuanlan.zhihu.com/p/270184068)

