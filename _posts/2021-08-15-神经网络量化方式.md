---
title: 神经网络量化方式
author: Zhang Ge
date: 2021-08-15 20:49:00 +0800
categories: [专业积累, 模型压缩]
tags: []
math: true
---

> 本文内容来自[作者](https://zhangge6.github.io/)个人的总结积累，如果有错误之处，非常感谢大家提出，我是在学习中。未经允许，也请勿转载（防止错误扩散...）。

# 关于本页

本文记录遇到过的神经网络量化方式。

## 线性量化

**非对称量化**：浮点数的最小值/最大值映射到整型数的最小值/最大值。有`zeropoint`的概念(下文将介绍)。

**对称量化**：以浮点数的绝对值最大值设定一个浮点数对称区间，并将其映射到对应的整型数对称区间上。

<center class="half">
    <img src="/assets/img/20210815/quant_asym.png" width="300"/> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<img src="/assets/img/20210815/quant_sym.png" width="300"/>
</center>

<center>非对称量化(左)与对称量化(右)示意图</center>

**两种量化方式比较：**

- 非对称量化由于浮点数的最小值/最大值映射到整型数的最小值/最大值，能够充分利用映射后的整型数区间。而对称量化则在浮点数据bias比较严重时使整型数区间有较大浪费，一个典型的例子是ReLU的输出。
- 对称量化相比于非对称量化实现更加简单。

<u>以下简要介绍非对称量化及其反量化。</u>


### 量化

将浮点数$$x_{f}\in [x_{f}^{min}, x_{f}^{max}]$$​量化到$$n$$​bit表示的整型数$$x_q\in[0,2^n-1]$$​，则有


$$
x_q = round\left((x_f - x_f^{min}) \frac{2^n-1}{x_f^{max} - x_f^{min}}\right) 
\tag{1}
$$


设$$scale = \frac{x_f^{max} - x_f^{min}}{2^n - 1}$$​​，则有


$$
x_q = round\left(\frac{x_f - x_f^{min}}{scale}\right) 
    = round\left(\frac{x_f}{scale} - \frac{x_f^{min}}{scale} \right)
 \tag{2}
$$


设$$zeropoint = \frac{x_f^{min}}{scale}$$​​，则有


$$
x_q = round\left(\frac{x_f}{scale} - zeropoint\right) \tag{3}
$$


实践中一般对$$zeropoint$$​​​取整，这样0.0量化后就会被(无误差地)映射到一个整型数。令$$zeropoint = round(zeropoint)$$​​​​，因此有了另一个我们常见的量化公式


$$
x_q = round\left(\frac{x_f}{scale} - zeropoint\right) = round\left(\frac{x_f}{scale}\right) - zeropoint \tag{4}
$$



> 上面的推导过程上半部分主要参考自[Distiller Docs](https://intellabs.github.io/distiller/algo_quantization.html)，最后一步对于zeropoint的处理来自作者本人，若可能存在错误，虚心接受读者评论区指正！

### 反量化

对应反量化公式


$$
x_f = scale \times (x_q + zeropoint) = scale \times x_q + bias
$$

---
**NOTE**：关于`zeropoint`

`zeropoint`浮点数0.0量化后对应的整型数。保证0.0在经过量化和反量化后，仍无误差地恢复为0.0：


$$
0.0\stackrel{\mbox{量化}}{\longrightarrow}-zeropoint\stackrel{\mbox{反量化}}{\longrightarrow}0.0
$$



- 量化时，$$x_f=0.0$$​将会量化到$$x_q = -zeropoint$$​ (式4)，量化过程无（取整操作带来的）误差；
- 反量化时，$$x_q = -zeropoint$$​​映射回$$x_f = 0.0$$​​，同样不存在误差。

它是必要的，因为0.0经常用来做一些padding的操作，如果量化+反量化后就变成了另外一个数，那结果就不可控了。

[参考-Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better](https://arxiv.org/abs/2106.08962)

---

**一些量化上的经验性总结**
[参考](https://pytorch.org/blog/quantization-in-practice/)

- 对权重，使用对称量化与`MinMax` observer；对激活，使用非对称量化与`MovingAverageMinMax` observer.




## 参考

- [Lei Mao - Quantization for Neural Networks](https://leimao.github.io/article/Neural-Networks-Quantization/)
- [量化-浅谈深度学习模型量化](https://zhuanlan.zhihu.com/p/349678095)
- [tinyML Talks: A Practical Guide to Neural Network Quantization](https://www.youtube.com/watch?v=KASuxB3XoYQ)

