[ { "title": "GPU通信元语", "url": "/posts/GPU%E9%80%9A%E4%BF%A1%E5%85%83%E8%AF%AD/", "categories": "专业积累, LLM推理", "tags": "LLM", "date": "2025-07-13 15:46:00 +0800", "snippet": "总览随着模型越来越大，单卡的算力和显存已经无法满足需求，多卡并行顺势而生。通信在多卡并行中扮演着重要角色，对性能也有影响。本文总结一些常见的通信元语，更多关注于推理（而不是训练）。多卡并行中常见的通信元语通信元语BroadcastScatterReduceAllReduce通信量：数学性质(参考)：AllReduce = reduceScatter + allGather典型使用场景：TP后的加和规约。GatherAllGatherReduceScatterAlltoAllhttps://images.nvidia.com/events/sc15/pdfs/NCCL-Woolley.pdfhttps://www.zhihu.com/search?type=content&amp;amp;q=moe%20alltoallhttps://blog.csdn.net/daydayup858/article/details/149742695从零实现一个MOE（专家混合模型） - KaiH的文章 - 知乎https://zhuanlan.zhihu.com/p/701777558DeepSeek模型MOE结构代码详解 - AI布道Mr.Jin的文章 - 知乎https://zhuanlan.zhihu.com/p/1896490149577200132MoE 训练到底是开 TP 还是 EP？ - xffxff的文章 - 知乎https://zhuanlan.zhihu.com/p/13997146226彻底搞懂MoE的EP并行（基于vLLM） - 九山海的文章 - 知乎https://zhuanlan.zhihu.com/p/1911059432953061899https://zhuanlan.zhihu.com/p/681692152https://www.zhihu.com/search?type=content&amp;amp;q=moe%20%E5%B9%B6%E8%A1%8Chttps://www.zhihu.com/search?type=content&amp;amp;q=moe%20%E5%B9%B6%E8%A1%8Chttps://mp.weixin.qq.com/s/9oPHFf519DLX1ictcnirXQhttps://apxml.com/courses/mixture-of-experts/chapter-4-scaling-moe-distributed-training/all-to-all-communication-moe这一段终于给我看懂了。至于EP和DP为什么习惯组合在一起用先不谈，当这二者在一起用时，是怎么个逻辑，为什么这个时候会用到AlltoAll，倒是终于抓到了点眉目。首先理解这个alltoall，先不要困在计算形式上，而是想它的计算本质：每个device都想将它的subset发到每一个其他device上（包括它自己），每个device都接收来自其他device的subset. 这里一个All是dp rank，一个all是ep rank （在这个语境下，强调i, j对应关系的，都是虚张声势了。）考虑dp_size = ep_size:alltoall dispatch: all (dp) to all (ep). dp rank N上的subset (token)，会分散到不同的ep rank device上。另一方面，每一个ep rank device, 将收到来自不同dp rank的subset。alltoall combine: all (ep) to all (dp). ep rank N上的subset (计算结果)，会汇总回token对应的dp rank上。另一方面，每一个dp rank，将收到来自不同ep rank的subset.完美。至于两个阶段里的分group，大概是实现方式了。分好group就可以复用已有的alltoall算子实现。" }, { "title": "DistServe阅读笔记", "url": "/posts/DistServe%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/", "categories": "专业积累, 编程积累", "tags": "LLM", "date": "2025-07-13 15:46:00 +0800", "snippet": "论文原文：https://arxiv.org/abs/2401.09670Why PD Disag make senseindependent tuing(KEY) How to get PD Disag profitLimitations/not suggested situations 优化throughut的场景：PD分离目标在于时延。换句话说，如果TTFT和TPOT优化得更好，那么在给定的TTFT和TPOT约束下，就可以达到更高的request rate。如果目标是最大化throughput（比如offline inference），那么不分离的chunked prefill更适合，因为可以把GPU打得更满。 GPU资源有限的场景：PD分离后的设计需要精心设计。GPU数量少，PD分离的设计空间（比如PD配比，并行方式）就会受限。（很）有可能分离后性能反而下降。 长context场景：长context对应更大的KV传输开销，有可能反而降低整体性能。Experiment result阿里云给了一些实验数据：https://help.aliyun.com/zh/pai/user-guide/enable-prefill-decode-disaggregation-deployment-of-llm-services" }, { "title": "gdb学习", "url": "/posts/gdb%E5%AD%A6%E4%B9%A0/", "categories": "实践, 硬技能", "tags": "GDB", "date": "2023-07-09 15:35:00 +0800", "snippet": "初步怎么运行gdb？写一个有bug的程序// crash.cpp#include &amp;lt;iostream&amp;gt;using namespace std; int divint(int, int);int main() { x = 3; y = 0; cout &amp;lt;&amp;lt; divint(x, y); return 0; } int divint(int a, int b) { return a / b; }使用g++编译程序，运行出错g++ crash.cpp -o crash./crash# Floating point exception (core dumped) 增加-g选项，编译debug版本g++ -g crash.cpp -o crash_gdb启动gdbgdb crash_gdb显示GNU gdb (Ubuntu 8.2-0ubuntu1~16.04.1) 8.2Copyright (C) 2018 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &amp;lt;http://gnu.org/licenses/gpl.html&amp;gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law.Type &quot;show copying&quot; and &quot;show warranty&quot; for details.This GDB was configured as &quot;x86_64-linux-gnu&quot;.Type &quot;show configuration&quot; for configuration details.For bug reporting instructions, please see:&amp;lt;http://www.gnu.org/software/gdb/bugs/&amp;gt;.Find the GDB manual and other documentation resources online at: &amp;lt;http://www.gnu.org/software/gdb/documentation/&amp;gt;.For help, type &quot;help&quot;.Type &quot;apropos word&quot; to search for commands related to &quot;word&quot;...Reading symbols from crash...done.(gdb) 最后两行显示Reading symbols from crash...done.(gdb) 表示载入debug版本的可执行程序成功，并进入gdb交互状态。输入r在debug环境中运行程序：(gdb) rStarting program: crash Program received signal SIGFPE, Arithmetic exception.0x00000000004006e1 in divint (a=3, b=0) at demo.cpp:1818 return a / b; 输入l (代表list)，打印异常处附近的代码。(gdb) l13 return 0; 14 } 1516 int divint(int a, int b) 17 { 18 return a / b; 19 }输入bt或where，打印函数的调用栈(gdb) bt#0 0x00000000004006e1 in divint (a=3, b=0) at demo.cpp:18#1 0x00000000004006c0 in main () at demo.cpp:11进阶打断点写一个demo程序// for_breakpoint.cpp#include &amp;lt;iostream&amp;gt;using namespace std; void my_func1(int i) { std::cout &amp;lt;&amp;lt; &quot;Now in func1, i = &quot; &amp;lt;&amp;lt; i &amp;lt;&amp;lt; &quot;\\n&quot;;}void my_func2(int i) { std::cout &amp;lt;&amp;lt; &quot;Now in func2, i = &quot; &amp;lt;&amp;lt; i &amp;lt;&amp;lt; &quot;\\n&quot;;}int main() { for (int i = 0; i &amp;lt; 3; ++i) { my_func1(i); my_func2(i); } return 0;}加上-g参数编译，并运行g++ -g for_breakpoint.cpp -o bt_demogdb ./demo2GNU gdb (Ubuntu 8.2-0ubuntu1~16.04.1) 8.2...Reading symbols from bt_demo...done.(gdb) rStarting program: /home/zyf301/NewDisk/zg/learn_gym/learn_gdb/bt_demo Now in func1, i = 0Now in func2, i = 0Now in func1, i = 1Now in func2, i = 1Now in func1, i = 2Now in func2, i = 2正常执行。在第14行增加断点（my_func1调用的位置）(gdb) b 14Breakpoint 1 at 0x40076b: file for_breakpoint.cpp, line 14.再执行一下看看，程序在断点处停下来了(gdb) rStarting program: bt_demo Breakpoint 1, main () at for_breakpoint.cpp:1414 my_func1(i);打印一下i的值(gdb) p i$1 = 0接下来步进。步进有两种模式，一种是不进入内层函数，对应n (next)执行；一种是进入内层函数，对应s (step)执行。分别试试(gdb) nNow in func1, i = 015 my_func2(i);(gdb) nNow in func2, i = 013 for (int i = 0; i &amp;lt; 3; ++i) {(gdb) nBreakpoint 1, main () at for_breakpoint.cpp:1414 my_func1(i);(gdb) smy_func1 (i=1) at for_breakpoint.cpp:55 std::cout &amp;lt;&amp;lt; &quot;Now in func1, i = &quot; &amp;lt;&amp;lt; i &amp;lt;&amp;lt; &quot;\\n&quot;;(gdb) sNow in func1, i = 16 }(gdb) smain () at for_breakpoint.cpp:1515 my_func2(i);(gdb) smy_func2 (i=1) at for_breakpoint.cpp:99 std::cout &amp;lt;&amp;lt; &quot;Now in func2, i = &quot; &amp;lt;&amp;lt; i &amp;lt;&amp;lt; &quot;\\n&quot;;(gdb) sNow in func2, i = 110 }(gdb) smain () at for_breakpoint.cpp:1313 for (int i = 0; i &amp;lt; 3; ++i) {(gdb) sBreakpoint 1, main () at for_breakpoint.cpp:1414 my_func1(i);(gdb) 可以发现，使用s，相比于使用n，在步进的时候进入了my_func1和my_func2的内部。或者直接执行，直到下一个断点/异常/执行结束：c(gdb) cContinuing.Now in func1, i = 0Now in func2, i = 0Breakpoint 1, main () at for_breakpoint.cpp:1414 my_func1(i);(gdb) cContinuing.Now in func1, i = 1Now in func2, i = 1Breakpoint 1, main () at for_breakpoint.cpp:1414 my_func1(i);(gdb) cContinuing.Now in func1, i = 2Now in func2, i = 2[Inferior 1 (process 11840) exited normally]显示当前所有断点：info break(gdb) b 15Breakpoint 2 at 0x400775: file for_breakpoint.cpp, line 15.(gdb) info breakNum Type Disp Enb Address What1 breakpoint keep y 0x000000000040076b in main() at for_breakpoint.cpp:142 breakpoint keep y 0x0000000000400775 in main() at for_breakpoint.cpp:15清除断点：d Num(gdb) d 1(gdb) info breakNum Type Disp Enb Address What2 breakpoint keep y 0x0000000000400775 in main() at for_breakpoint.cpp:15清除所有断点：d(gdb) dDelete all breakpoints? (y or n) y(gdb) info breakNo breakpoints or watchpoints.(gdb) 在指定文件的某一行打断点：b filepath:line(gdb) b for_breakpoint.cpp:14Breakpoint 3 at 0x40076b: file for_breakpoint.cpp, line 14.打印打印函数中变量i的当前值(gdb) p i$1 = 0打印某一个函数的执行结果(gdb) p my_func1(5)Now in func1, i = 5$2 = void退出gdb使用q或quit退出当前gdb(gdb) qA debugging session is active. Inferior 1 [process 11934] will be killed.Quit anyway? (y or n) y进阶使用根据pid attach程序(gdb) attach &amp;lt;pid&amp;gt;参考参考https://zhuanlan.zhihu.com/p/504186531https://www.tutorialspoint.com/gnu_debugger/gdb_commands.htm" }, { "title": "【WIP】推理图优化方法小结", "url": "/posts/%E6%8E%A8%E7%90%86%E5%9B%BE%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E5%B0%8F%E7%BB%93/", "categories": "实践", "tags": "推理加速", "date": "2023-02-13 20:31:00 +0800", "snippet": "在神经网络推理过程中，图优化可以达到降低运算量、减少算子invoke数目与降低缓存开销等效果，提升推理效率。本文对常见的推理图优化方法进行小结，并不断增补中。符号定义卷积、全连接层运算设卷积或全连接层$Layer(W, b)$的权重和偏置为\\(W\\)和\\(b\\)，输入和输出分别为\\(X\\)和\\(X^{\\prime}\\)，有\\(X^{\\prime} = X \\otimes W + b\\)其中$\\otimes$代表卷积或矩阵乘法操作。$X \\in \\mathbf{R}^{c\\times h \\times w}, \\ \\ X^{\\prime} \\in \\mathbf{R}^{c^{\\prime} \\times h^{\\prime} \\times w^{\\prime}}, \\ \\ b \\in \\mathbf{R}^{c^{\\prime}}.$当$Layer$为卷积层时，$W \\in \\mathbf{R}^{c^{\\prime} \\times c \\times k \\times k}$，当$Layer$为全连接层时，$W \\in \\mathbf{R}^{c^{\\prime} \\times c}.$BN运算设BN层($\\gamma, \\beta$)输出和输出分别为\\(X\\)和\\(X^{\\prime}\\)，有\\(X^{\\prime} = \\gamma \\frac{X - \\mu}{\\sqrt{\\delta^2 + \\epsilon}} + \\beta\\)其中${X, X^{\\prime}}\\in \\mathbf{R}^{c\\times h \\times w}$. $\\mu, \\sigma, \\gamma, \\beta$均为逐特征通道的变量，在推理时为固定值，${\\mu, \\sigma, \\gamma, \\beta}\\in \\mathbf{R}^c$.关于BN层的更多细节介绍，可参见关于Batch Normalization。网络层融合缩放/偏置算子融合BN + scale (逐通道) 对应ncnn中的NetOptimize::fuse_batchnorm_scale()融合前：X --&amp;gt; [BN] --&amp;gt; X&#39; --&amp;gt; [S] --&amp;gt; Y融合后：X --&amp;gt; [BN&#39;] --&amp;gt; YBN层后接一个沿特征通道维度的缩放算子$S\\in\\mathbf{R}^{c}$，将对应系数逐通道融合进BN层的系数$\\gamma, \\beta$中即可。Conv + scale (逐通道) 对应ncnn中的NetOptimize::NetOptimize::fuse_convolution_mul()融合前：X --&amp;gt; [Conv] --&amp;gt; X&#39; --&amp;gt; [S] --&amp;gt; Y融合后：X --&amp;gt; [Conv&#39;] --&amp;gt; Y卷积层后接一个沿特征通道维度的缩放算子$S\\in\\mathbf{R}^{c}$，将对应系数逐输出通道（乘积）融合进卷积层的系数$W, b$中即可。Conv + Add (逐通道） 对应ncnn中的NetOptimize::NetOptimize::fuse_convolution_add()融合前：X --&amp;gt; [Conv] --&amp;gt; X&#39; --&amp;gt; [Add] --&amp;gt; Y融合后：X --&amp;gt; [Conv&#39;] --&amp;gt; Y卷积层后接一个沿特征通道维度的偏置算子$A\\in\\mathbf{R}^{c}$，将对应系数逐输出通道（加和）融合进卷积层的系数$b$中即可。 对应ncnn中的NetOptimize::fuse_innerproduct_add()融合前：X --&amp;gt; [FC] --&amp;gt; X&#39; --&amp;gt; [Add] --&amp;gt; Y融合后：X --&amp;gt; [FC&#39;] --&amp;gt; YFC + Add 和 Conv + Add 原理非常相似。将对应系数逐输出通道（加和）融合进全连接层的系数$b$中即可。FC + Add (逐通道)FC + Act 和 Conv + Act 原理非常相似。此处不再赘述。BN层融合Conv + BN 对应ncnn中的NetOptimize::fuse_convolution_batchnorm()融合前：X --&amp;gt; [Conv] --&amp;gt; X&#39; --&amp;gt; [BN] --&amp;gt; Y融合后：X --&amp;gt; [Conv&#39;] --&amp;gt; Y参考 To execute neural network inference, kernels are invoked for neural network layers in order to compute the output tensors given the input tensors. Each kernel call will bring some overhead time….To achieve high throughput and low latency for neural network inference, the rule of thumb is to have fewer large kernel calls instead of many small kernel calls.卷积输出\\(X^{\\prime}\\)紧接进入BN层，设BN层的输出为\\(Y\\)，则有\\(\\begin{aligned}Y &amp;amp;= \\gamma \\frac{X^{\\prime} - \\mu}{\\sqrt{\\delta^2 + \\epsilon}} + \\beta= \\gamma \\frac{X * W + b - \\mu}{\\sqrt{\\delta^2 + \\epsilon}} + \\beta \\\\ &amp;amp;= X * (\\frac{\\gamma}{\\sqrt{\\delta^2 + \\epsilon}}W) + \\beta + \\frac{\\gamma}{\\sqrt{\\delta^2 + \\epsilon}}(b - \\mu) \\\\ &amp;amp;= X * W^{\\prime} + b^{\\prime}\\end{aligned}\\)即 conv + BN 等效于调用一个权重为\\(W^{\\prime} = \\frac{\\gamma}{\\sqrt{\\delta^2 + \\epsilon}}W\\)，偏置为\\(b^{\\prime} = \\beta + \\frac{\\gamma}{\\sqrt{\\delta^2 + \\epsilon}}(b - \\mu)\\)的新卷积。该步融合减少了kernel的invoke次数，降低了计算量，且省去了原Conv层与BN层之间的缓存开销。FC + BN 对应ncnn中的NetOptimize::fuse_innerproduct_batchnorm()融合前：X --&amp;gt; [FC] --&amp;gt; X&#39; --&amp;gt; [BN] --&amp;gt; Y融合后：X --&amp;gt; [FC&#39;] --&amp;gt; YFC + BN 和 Conv + BN 原理非常相似。将对应系数逐输出通道（乘积）融合进全连接层的系数$W, b$中即可。激活函数融合Conv + Act (逐元素) 对应ncnn中的NetOptimize::NetOptimize::fuse_convolution_activation()融合前：X --&amp;gt; [Conv] --&amp;gt; X&#39; --&amp;gt; [Act] --&amp;gt; Y融合后：X --&amp;gt; [Conv&#39;] --&amp;gt; X&#39;(inplace Act) -&amp;gt; Y卷积层紧跟逐元素的激活函数，可以直接将卷积层的输出的激活操作原地（inplace）进行，节约一次缓存开销。ncnn目前支持融合的激活函数有ReLU, Clip, Sigmoid, Mish, HardSwish.基于此，Conv + BN + ReLU可先将Conv和BN融合成Conv’，再将ReLU融合inplace操作，从而在省去BN层计算量的同时，减少两次访存。参考FC + Act (逐元素) 对应ncnn中的NetOptimize::fuse_innerproduct_activation()融合前：X --&amp;gt; [FC] --&amp;gt; X&#39; --&amp;gt; [Act] --&amp;gt; Y融合后：X --&amp;gt; [FC&#39;] --&amp;gt; X&#39;(inplace Act) -&amp;gt; Y冗余算子消除消除dropout算子 对应ncnn中的NetOptimize::eliminate_dropout()消除前：X --&amp;gt; [dropout] --&amp;gt; Y消除后：X --&amp;gt; Ydropout算子在推理时不作用，因此可消除。消除identity算子 对应ncnn中的NetOptimize::eliminate_noop()消除前：X --&amp;gt; [identity] --&amp;gt; Y消除后：X --&amp;gt; Y啥不做的算子为啥留着？消除它。消除只有一个输出分支的split算子 对应ncnn中的NetOptimize::eliminate_split()消除前：X --&amp;gt; [split] --&amp;gt; Y (split只有一个输出分支)消除后：X --&amp;gt; Y当split只有一个输出分支时，该split节点时是冗余的，可以消除。消除global pooling后的flatten算子 对应ncnn中的NetOptimize::eliminate_flatten_after_global_pooling()消除前：X --&amp;gt; [global pooling] --&amp;gt; [flatten] --&amp;gt; Y 消除后：X --&amp;gt; [global pooling] --&amp;gt; Y在ncnn中，global pooling后特征的形状是大小为$C$的一维向量，再做flatten操作无实质变化，因此flatten是冗余的，可以消除。常量折叠参考 ncnn/tools/optimize Graph Optimizations in ONNX Runtime" }, { "title": "Flask项目部署记录（uwsgi）", "url": "/posts/Flask%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2%E8%AE%B0%E5%BD%95-uwsgi/", "categories": "实践, 硬技能", "tags": "Flask, uwsgi", "date": "2022-11-14 21:17:00 +0800", "snippet": "Flask是一个使用Python编写的轻量级Web应用框架。基于它可以很方便地搭建起一个Web应用，但其内建服务器不适用于生产环境。所以当在本地完成了一个Flask应用，为了更高效、安全、稳定地把它展示给全世界，是时候部署它了！本页记录我把一个Flask应用部署到腾讯云服务器上的操作过程，用到了uwsgi。 再进一步地，为了支持高性能、高并发，还可以用到nginx。但这一步我暂时还没弄清楚，等后续有机会实操并成功后再做记录。服务器准备在腾讯云租用一个服务器，会分配一个公网ip地址。 （首次连接前，需要）配置服务器的SSH用户名和密码 我租用的是一台“轻量级服务器”，它的SSH配置指南见这里 :star:关于端口 创建实例时默认已开通22端口，可供SSH连接使用。 其他端口放行情况可见服务器详情的”防火墙“栏，我们后续搭建Flask应用会使用已默认放行的3389端口。 配置本地和服务器的SSH连接 配置方法可参见这里 ok，可以开始我们的开发了！Flask应用搭建和它的裸奔以一个最简单的Flask应用为例，新建main.py# main.pyfrom flask import Flaskapp = Flask(__name__)@app.route(&#39;/&#39;)def index(): return &quot;Hello World!&quot;if __name__ == &#39;__main__&#39;: app.run(host=&#39;0.0.0.0&#39;, port=3389)执行python main.py输出： * Serving Flask app &#39;main&#39; * Debug mode: offWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:3389 * Running on http://10.0.4.2:3389Press CTRL+C to quit这时，我们本地登录 http://服务器公网ip:3389，即可以看到如下界面Well done! 咱们的Flask应用可以访问了！但是可以看到终端输出的警告：WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.这是由于Flask内建服务器不够高效、安全、稳定，所以不适用于生产环境。按照提示，我们使用WSGI server来部署它。WSGI server部署Flask应用安装uwsgi 常规的方法是用pip安装 参考 pip install uwsgi 如果使用的是Conda虚拟环境，可以从conda forge安装 参考，如 conda install -c &quot;conda-forge/label/cf202003&quot; uwsgi 基于uwsgi运行flask应用参考新建uwsgi.ini[uwsgi]; 指定ip和端口，应保持和之前main.py中app的定义一致 (此时main.py中的ip/端口定义可以略去)http = 0.0.0.0:3389; 指定flask项目启动文件地址 chdir = path/to/your/flask_projectwsgi-file = main.py; 显式指明main.py中定义的callable &amp;lt;== app = Flask(__name__)callable = app ; master = true; 指定进程/线程数目processes = 4threads = 2; 指定虚拟环境地址virtualenv = /path/to/virtual/environment注： 如果uwsgi.ini和flask项目启动文件在同一目录下，chdir项可省略。 如果是在虚拟环境中运行uwsgi，则virtualenv项需要指定。参考 注意，当前运行所处的虚拟环境，应该与virtualenv项保持一致！ 终端使用 echo $CONDA_PREFIX 可以得到当前虚拟环境的本机路径。 master项的具体含义可以参考这里，暂时还不太理解。如果没有指定master = true的话，会有WARNING: you are running uWSGI without its master process manager执行uwsgi uwsgi.ini会有一堆输出。如果没有报错，则本地登录 http://服务器公网ip:3389，再次出现成功！" }, { "title": "格式化输出小节", "url": "/posts/%E6%A0%BC%E5%BC%8F%E5%8C%96%E8%BE%93%E5%87%BA%E5%B0%8F%E7%BB%93/", "categories": "实践, 硬技能", "tags": "format", "date": "2022-10-19 10:58:00 +0800", "snippet": "本页总结Python、C和C++的格式化输出。Python参考1 参考2使用placeholder_str.format(value1, value2...)进行字符串格式化。其中 placeholder_str是包含{}的字符串，{}可以是空，下标值或参数名； value1，value2...可以是值，也可以是键值对。示例print(&quot;My name is {}, I&#39;m {}&quot;.format(&quot;Zander&quot;, 25))print(&quot;My name is {0}, I&#39;m {1}&quot;.format(&quot;Zander&quot;, 25))print(&quot;My name is {name}, I&#39;m {age}&quot;.format(name=&quot;Zander&quot;, age=25))其中第三行还有一种等效写法为name = &quot;Zander&quot;age = 25print(f&quot;My name is {name}, I&#39;m {age}&quot;)format()还可用于数字的格式化（这里介绍了各种各样支持的format，以供查询），比如以下常用的：示例 使用:f将数字转化为定点数 # default with 6 decimals. print(&quot;The price is {:f} dollars.&quot;.format(45)) # The price is 45.000000 dollars. # use `:.&amp;lt;n&amp;gt;f` to specify the number of decimals print(&quot;The price is {:.2f} dollars.&quot;.format(45)) # The price is 45.000000 dollars. # use key=value and number format simultaneously print(&quot;The price is {price:.2f} dollars.&quot;.format(price=45)) # The price is 45.00 dollars. C使用printf()进行格式化输出。这里介绍了各种各样支持的format，以供查询。以下常用的：示例#include &amp;lt;stdio.h&amp;gt; int main() { // print signed decimal integer int x = 45; printf(&quot;%d\\n&quot;, x); // 45 // print float value float f = 3.1416; // the default number of digits to be printed after the decimal point is 6 printf(&quot;%f\\n&quot;, f); // 3.141600 // use `%.&amp;lt;n&amp;gt;f` to specify the number of digits to be printed after the decimal point printf(&quot;%.3f\\n&quot;, f); // 3.142 // print string of characters char str[] = &quot;Hello World&quot;; printf(&quot;%s\\n&quot;, str); // Hello World} 注：format和实际的数据类型务必对应，否则显示的数值妈都不认识。C++C++同样支持printf()。但值得注意一些地方： printf输出C++的string？ printf不支持直接输出string，需要用.c_str()进行转换 string a = &quot;c++ string\\n&quot;;printf(&quot;%s&quot;, a.c_str()); 另外C++还支持cout，使用起来比较简单，这里介绍其常见用法。以下是一些值得注意的地方： 设定输出的精度：setprecision(n) 参考 控制输出的浮点数字的有效数字位数 #include &amp;lt;iostream&amp;gt; // std::cout, std::fixed#include &amp;lt;iomanip&amp;gt; // std::setprecisionusing namespace std; int main(){ double number1 = 132.364, number2 = 26.91; double quotient = number1 / number2; // 4.91876625789669... cout &amp;lt;&amp;lt; quotient &amp;lt;&amp;lt; endl; // 4.91877, 默认显示6位有效数字 cout &amp;lt;&amp;lt; setprecision(5) &amp;lt;&amp;lt; quotient &amp;lt;&amp;lt; endl; // 4.9188, 设定显示5位有效数字 cout &amp;lt;&amp;lt; setprecision(4) &amp;lt;&amp;lt; quotient &amp;lt;&amp;lt; endl; // 4.919, 设定显示4位有效数字 return 0;} 如果一个数字的值可以由少于 setprecision 指定的精度位数来表示，则操作符将不起作用 double dollars = 24.51;cout &amp;lt;&amp;lt; dollars &amp;lt;&amp;lt; endl; // 24.51cout &amp;lt;&amp;lt; setprecision (5) &amp;lt;&amp;lt; dollars &amp;lt;&amp;lt; endl; // 24.51 配合fixed使用可实现控制小数点后n位 #include &amp;lt;iostream&amp;gt; // std::cout, std::fixed#include &amp;lt;iomanip&amp;gt; // std::setprecisionusing namespace std; int main () { double f = 3.1415926; cout &amp;lt;&amp;lt; setprecision(5) &amp;lt;&amp;lt; f &amp;lt;&amp;lt; &#39;\\n&#39;; // 3.1416 cout &amp;lt;&amp;lt; fixed &amp;lt;&amp;lt; setprecision(5) &amp;lt;&amp;lt; f &amp;lt;&amp;lt; &#39;\\n&#39;; // 3.14159 cout &amp;lt;&amp;lt; fixed &amp;lt;&amp;lt; setprecision(10) &amp;lt;&amp;lt; f &amp;lt;&amp;lt; &#39;\\n&#39;; // 3.1415926000 return 0;} " }, { "title": "CMake学习", "url": "/posts/CMake%E5%AD%A6%E4%B9%A0/", "categories": "实践, 框架学习", "tags": "CMake", "date": "2022-10-18 10:35:00 +0800", "snippet": "CMake是一个跨平台、开源的构建工具，在各种大型C/C++项目中被广泛使用。 CMake是一个&quot;generator&quot;，它不会直接编译目标，而是根据CMakeLists.txt生成当前平台编译所需要的文件（如makefile），然后由当前平台编译工具进行编译（如make）。参考 之所以说CMake是跨平台的，是因为CMakeList.txt是平台无关的，实现了“Write once, run everywhere”。参考 CMake根据CMakeLists.txt中的变量，来确定当前的构建环境和构建规则。构建规则包括：要需要包含哪些头文件、链接哪些库、语言的版本、编译器参数等等。参考CMake作为一项硬技能，要好好学一下。本页为我在学习CMake过程中的一些笔记。分为三个层级： 第一层：如何使用 第二层：每一步背后发生了什么 第三层：一些更细致全面的学习第一层：如何使用这一节首先用一个简单的样例，看看如何操作，来借助CMake完成编译。假如我们现在有一个源文件demo.cpp#include &amp;lt;iostream&amp;gt;using namespace std;int main() { cout &amp;lt;&amp;lt; &quot;Hello, World!&quot; &amp;lt;&amp;lt; endl; return 0;}编写CMakeLists.txt# cmake最低版本需求cmake_minimum_required(VERSION 3.13)# 指定project名称project(DemoProject)# 编译源码生成目标add_executable(demo demo.cpp)文件目录如下：|- demo.cpp|- CMakeList.txt依次执行mkdir buildcd buildcmake .. 这一步直接使用cmake .当然也是可以的。但是生成的中间文件会和源文件混杂在一起，发生凌乱。因此一般都是新建一个build文件夹，用来保存中间文件（即通常说的&quot;out-of-source build&quot;）再执行make即可完成编译，在build目录下生成可执行文件demo。执行# build目录下./demo# 输出： Hello, World!第二层：每一步发生了什么这一节看看上面的各个步骤分别发生了什么。 上面各步是一个典型的&quot;out-of-source build&quot;过程。具体而言，build目录是我们执行cmake并保存生成文件的地方；..是CMakeLists.txt所在的目录。 mkdir build; cd build 创建并进入build文件夹，编译过程生成的中间文件将保存在此，与源文件隔离开来。 cmake .. 这一句的原型为：cmake &amp;lt;path-to-source&amp;gt;。即到当前（/build）上一层目录中寻找CMakeLists.txt 这一步会生成以下文件（最关键的是Makefile），并保存在当前目录（/build）下 |- CMakeFiles |- ...|- cmake_install.cmake|- CMakeCache.txt|- Makefile 这些生成的文件分别代表什么？参考 我理解，除了Makefile，其他都是类似于”中间文件“的存在吧。有了它们之后，再次执行cmake会跳过很多操作。 参考: stackoverflow - What does cmake .. do? make 根据生成的Makefile执行make编译。 第三层：一些更全面的学习这一节记录一些CMake的基本语法与使用策略。基本语法一些常用的指令 CMakeLists.txt文件中指令不区分大小写，但一般来说同一个文件中指令应保持大小写风格一致。 cmake_minimum_required 一般这条指令出现在CMakeLists.txt的最开始，指明了对cmake的最低版本的要求。 参考 project 指定工程名称（和版本、描述等）。 工程名称给定后，一些内置变量，如PROJECT_NAME、CMAKE_PROJECT_NAME会被自动赋为该值。 参考 include_directories 添加头文件搜索目录。 目录可以使用绝对路径格式，也可使用（相对于当前CMakeLists.txt的）相对路径格式。 相当于g++中-I的作用。 参考，参考 The difference between include_directories and target_include_directories? link_directories 添加需要链接的库文件目录。 相当于g++中-L的作用。 何时使用：如果要链接的库，没有放在/lib和/usr/lib和/usr/local/lib这三个目录里，需要使用该指令指定库文件所在的目录。 target_link_libraries 添加需要链接的库名称（或路径等，参见官方文档）。 相当于g++中-l的作用。 add_library 把指定的源文件来打包成动态库或静态库。 问：源文件写.cpp还是.h? 似乎是要写cpp。写.h的话，会在cmake阶段报错CMake Error: Cannot determine link language for target add_executable 使用指定的源文件来生成目标可执行文件。 参考 add_definitions 向C/C++编译器添加 -DXXX 定义。如果代码中定义了#ifdef XXX #endif，这个代码块就会生效。 参考 aux_source_directory 使用方法为：aux_source_directory(&amp;lt;dir&amp;gt; &amp;lt;variable&amp;gt;)，即根据dir内的所有源代码文件名生成列表，保存在变量variable中。 参考 add_subdirectory 添加一个子目录并构建该子目录。 子目录下应该包含CMakeLists.txt文件和代码文件。 后文讲组织关系时会重点介绍该指令。 参考 file 专注于文件系统中的文件和路径的读、写、查等。 参考（cmake文档） 参考 message cmake中打印信息的语句，类比Python中的print() 参考（cmake文档） option 定义一个bool开关，可在终端赋值 语法为：option(&amp;lt;variable&amp;gt; &quot;&amp;lt;help_text&amp;gt;&quot; [value]) 参考 变量访问变量使用${}进行变量的访问，如${VAR}。定义变量显式定义使用set显式定义变量，基本的用法为set(&amp;lt;variable&amp;gt; &amp;lt;value&amp;gt;)示例set(VAR &quot;I am a variable&quot;)message(&quot;${VAR}&quot;) # I am a variable 注： 如果set语句的value不加双引号，则定义的变量则被解释成列表，如 set(VAR I am a variable)message(&quot;${VAR}&quot;) # I;am;a;variable 如果message语句中对变量的引用不加双引号，则输出中各元素中无空格，如 set(VAR &quot;I am a variable&quot;)message(${VAR}) # Iamavariable 参考-stackoverflow: When should I quote CMake variables?隐式定义举例使用project(PROJ_NAME)语句指定工程名称后，则&amp;lt;PROJ_NAME&amp;gt;_BINARY_DIR和&amp;lt;PROJ_NAME&amp;gt;_SOURCE_DIR则会被隐式地定义：project(ToyDemo)message(&quot;&amp;lt;PROJ_NAME&amp;gt;_BINARY_DIR: ${ToyDemo_BINARY_DIR}&quot;)# output: path/to/current_bin_dirmessage(&quot;&amp;lt;PROJ_NAME&amp;gt;_SOURCE_DIR: ${ToyDemo_SOURCE_DIR}&quot;)# output: path/to/current_source_dir内置变量以下记录下常见的内置变量 变量名 含义 PROJECT_NAME 通过project()指令定义的项目名称 PROJECT_SOURCE_DIR 源CMakeLists.txt所在的目录 PROJECT_BINARY_DIR 执行cmake命令所在的目录 控制指令写法分支控制if(&amp;lt;condition&amp;gt;) &amp;lt;commands&amp;gt;elseif(&amp;lt;condition&amp;gt;) &amp;lt;commands&amp;gt;else() # ()不能省 &amp;lt;commands&amp;gt;endif() # ()不能省 注： 需要注意的是，&amp;lt;condition&amp;gt;中如果访问变量，不用（不应）加${} 加不加${}的区别是什么？ :heavy_check_mark:if (VAR)：会以变量VAR的值为判断条件。 :x:if (${VAR})：${VAR}访问得到一个常量或者另一个变量。如果${VAR}是一个常量，那么if语句会寻找和该常量同名的变量作为判断条件，故可能会导致预料之外的结果。 这里有一个细致的解释。官方文档也提到这个问题。 **示例** 见wikibooks。完整语法规则见官方文档循环控制foreach参考foreach(&amp;lt;loop_var&amp;gt; &amp;lt;items&amp;gt;) &amp;lt;commands&amp;gt;endforeach()foreach(&amp;lt;loop_var&amp;gt; RANGE &amp;lt;stop&amp;gt;) # 注意右侧闭区间，会输出[0, stop]共stop + 1个数！foreach(&amp;lt;loop_var&amp;gt; RANGE &amp;lt;start&amp;gt; &amp;lt;stop&amp;gt; [&amp;lt;step&amp;gt;])其中items代指要遍历的列表，loop_var代指正在被迭代到的项。示例set(mylist &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot;)foreach(var ${mylist}) message(&quot;current value: ${var}&quot;)endforeach()foreach(var RANGE 3) message(&quot;current value: ${var}&quot;) # 输出 0 1 2 3 共【4】个数！endforeach()foreach(var RANGE 0 6 2) message(&quot;current value: ${var}&quot;) # 输出 0 2 4 6endforeach()whilewhile(&amp;lt;condition&amp;gt;) &amp;lt;commands&amp;gt;endwhile()如果需要用到数学计算，参照这里示例set(var 0)while(var LESS 3) message(&quot;cur var ${var}&quot;) math(EXPR var &quot;${var} + 1&quot;)endwhile()break和continue把break()或continue()插入到循环体中终止循环。常见的文件组织方式及应对策略含有多个子目录示例文件目录如下：.├── CMakeLists.txt├── main.cpp├── sub1│ ├── CMakeLists.txt│ ├── test1.cpp│ └── test1.h└── sub2 ├── CMakeLists.txt ├── test2.cpp └── test2.h主目录下的CMakeLists.txt# CMakeLists.txtcmake_minimum_required(VERSION 3.10.2)project(test)include_directories(sub1)include_directories(sub2)add_subdirectory(sub1)add_subdirectory(sub2)add_executable(test main.cpp)target_link_libraries(test sub1_lib sub2_lib)主目录下的main.cpp// main.cpp#include &quot;test1.h&quot;#include &quot;test2.h&quot;#include &amp;lt;iostream&amp;gt;int main(int argc, char** argv){ test1_func(); test2_func(); return 0;}sub1目录下的CMakeLists.txt# sub1/CMakeLists.txtcmake_minimum_required(VERSION 3.10.2)project(sub1)add_library(sub1_lib test1.cpp)sub1目录下的test1.h和test1.cpp// sub1/test1.hvoid test1_func();// sub1/test1.cpp#include &quot;test1.h&quot;void test1_func(){}sub2目录下的CMakeLists.txt# sub2/CMakeLists.txtcmake_minimum_required(VERSION 3.10.2)project(sub2)add_library(sub2_lib test2.cpp)sub2目录下的test2.h和test2.cpp// sub2/test2.hvoid test2_func();// sub2/test2.cpp#include &quot;test2.h&quot;void test2_func(){}之后在主目录下正常执行cmake即可。需要注意的问题 CMakeLists.txt文件名大小敏感，否则会报错&quot;CMake Error: does not appear to contain CMakeLists.txt&quot;参考 How to Build a CMake-Based Project Quick CMake tutorial CMake Tutorial Modern CMake CMake 教程 CMake 从入门到应用" }, { "title": "对Roofline模型的理解", "url": "/posts/%E5%AF%B9Roofline%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%90%86%E8%A7%A3/", "categories": "理论", "tags": "Roofline", "date": "2022-09-03 18:11:00 +0800", "snippet": "Roofline模型非常简洁，却对计算加速有很好的指导意义。它要解决的是“计算量为$N$且访存量为$V$的算法，在算力为$P_{peak}$且带宽为$B$的计算平台所能达到的理论性能上限是多少”这个问题。可以帮助分析当前算法的性能是受算法实现所限还是平台本身算力限制，进而更有针对性地改进算法性能。Roofline模型给定一个计算平台，它的峰值算力\\(P_{peak}\\)（单位flop/s），带宽\\(B\\)（单位byte/s）。给定一个算法，它的计算量为\\(N\\)（单位flop），访存量为\\(V\\)（单位byte），定义计算强度\\(I\\)（computational intensity，计算访存比，单位flop/byte）\\[I = \\frac{N}{V}\\]则该算法的理论性能\\(P\\)上限为\\[P = min(P_{peak}, IB)\\] 注：\\(P\\)代表算法每秒浮点运算次数，即使用的算力，不等同于算法执行时间。比如这里举的例子：单从\\(P\\)来说，VGG是MobileNet的3倍（且VGG处于Compute limited区域，MobileNet处于Memory limited区域），但是MobileNet的总计算量是VGG的三十分之一，因此MobileNet的计算时间是VGG的十分之一（理论值）。绘制算法性能\\(P\\)和算法计算强度\\(I\\)的曲线，如下图红色实线所示可以看到，在Roofline模型中，算法的性能由\\(P_{peak}\\)，\\(B\\)和\\(I\\)三个变量决定。前面二者是硬件本身的参数，因此编码时主要可以考虑从\\(I\\)出发，改善性能。计算强度\\(I\\)可以形象地体现编写的代码质量。 当\\(I \\ge \\frac{P_{peak}}{B}\\)时，处于compute limited区域，此时所使用的算力达到硬件峰值，算力充分利用； 要进一步提升性能，则需要提升硬件本身峰值算力。 当\\(I &amp;lt; \\frac{P_{peak}}{B}\\)时，处于Memory limited区域，算法执行受到访存限制（访存过多或利用率较低）。 此时需要提升计算强度，比如： 算子融合，则是在相同计算量的情况下，避免了中间的访存过程，从而降低了访存量，提升计算强度； 矩阵乘法中，通过合理的分块等方式，减低访存量，增大计算强度。 举例比如给定计算平台峰值算力\\(P_{peak} = 4\\ GFlop/s\\)，带宽\\(B = 10\\ GByte/s\\)下面一个小demofloat a[N], b[N], c[N];for (int i = 0; i &amp;lt; N; ++i) { c[i] = a[i] + b[i];}每个循环进行了2次浮点运算（2 flop），产生12 byte的访存（3 * 4 byte/float），故计算强度为\\(\\frac{1}{6}\\) flop/byte &amp;lt; \\(\\frac{P_{peak}}{B} = \\frac{2}{5}\\) flop/byte，故处于memory limited区域，需要进一步提升计算强度以提升性能。参考 A very short intro to the Roofline model Roofline Model与深度学习模型的性能分析 Roof-line Model性能分析模型简介" }, { "title": "Regex正则表达式学习与使用", "url": "/posts/Regex%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%BD%BF%E7%94%A8/", "categories": "实践, 硬技能", "tags": "regex", "date": "2022-08-18 10:54:00 +0800", "snippet": "感觉Regex正则表达式是这样一种东西： 没有需求时去学习它：天啊这一串符号都是什么东西，怎么学啊！ 当有需求用到它并且成功应用时：真是太方便了！真香，要学习！动机最近实习的时候，需要协助处理代码审查系统的告警，其中有大量（约1.6k）的命名风格的问题，需要对原有的变量命名进行批量替换。我用的编辑器是VS Code，它的朴素的字符串替换功能已经不能满足需求了，比如我需要做以下一些替换：src0_32f -&amp;gt; vSrc0dst0_32f -&amp;gt; vDst0...想到的是，用正则表达式匹配与替换，对于这个例子，所采用的方法是：搜索框填：([a-z])(\\w+)_32f替换框填：v\\U$1$2真是方便！这促使我开启regex正则表达式的学习和积累。基础 在处理字符串时，经常会有查找符合某些复杂规则的字符串的需要。正则表达式就是用于描述这些规则的工具。正则表达式的资料，网上已经有很多了，这里列举几个对我帮助比较大的： 正则表达式30分钟入门教程 中文，介绍正则表达式的一些基本概念。 Python RegEx :+1: 提供了各种regex的实例讲解。 本页以下内容主要基于此整理而成。 元字符（Metacharacters）Regex的元字符有：[] . ^ $ * + ? {} | () \\ []：定义一个字符集合，当某一个单个字符属于该集合时，则匹配 如规则[abc]作用于apple会发生一次匹配 可使用-表示一个区间，如[0-9]等价于[0123456789]，[a-z]等价于[abcd...xyz]，[A-Z]等价于[ABCD...XYZ] [0-39]等价于[01239] [a-z0-9]等价于[abcd...xyz0123456789] 使用^取集合的补集 如[^abc]代表所有不属于[abc]的字符集合 .：匹配除换行符\\n之外的所有单个字符 如规则.作用于abcd时会发生4次匹配（分别为a, b, c, d）；规则..作用于abcd时会发生2次匹配（分别为ab, cd）； ^：判断以后续字符/串为开头的字符/串 如规则^ab作用于abc会发生匹配，作用于acb则不会发生匹配 $：判断以前缀字符/串为结尾的字符/串 如规则a$作用于formula会发生匹配，作用于cab则不会发生匹配 计数元字符：*，+，?，{} *：其左方的单个字符重复大于等于0次 如ma*n可匹配mn，man，maaaaan +：其左方的单个字符重复大于等于1次 如ma+n可匹配man，maaaaan ?：其左方的单个字符重复0次或1次 如ma?n可匹配mn，man {} ：{m, n}表示重复大于等于m次，小于等于n次 比如[0-9]{2, 4}匹配长度在2~4之间的数字串 |：同or的含义，当某一个单个字符属于|连接的所有字符中的一个时，则发生匹配 如规则a|b作用于adb时，发生两次匹配（a和b） 感觉有点像[]啊 ()：分组 一个()扣起来的为一个组。当需要把匹配字符串中的某一小组提取出来，比较有用： 比如要把0000_32f替换为0000_32s，则可使用： 搜索框填([0-9]{4})_32f，替换框填$1_32s 其中$1就表示第一组的内容（regex从1开始计数） 比如要把0000_32f替换为0000_32F，则可使用： 搜索框填([0-9]{4})_32(\\w)，替换框填$1_32\\U$2 其中\\U表示将对应的字符转为大写 \\：常见的转移字符的含义特殊字符 \\d：等价于[0-9] \\D：等价于[^0-9] \\w：等价于[a-zA-Z0-9_]（注意还可匹配下划线） \\W：等价于[^a-zA-Z0-9_] \\s：匹配一个空白位置其他 在线测试正则表达式的工具：Wegester" }, { "title": "Python argparse使用方法小结", "url": "/posts/Python-argparse%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%E5%B0%8F%E7%BB%93/", "categories": "实践, 框架学习", "tags": "Python, argparse", "date": "2022-05-10 19:42:00 +0800", "snippet": "本页对Python常用的命令行解析模块argparse的常用方法做一个小结，为了清晰和简化，每个例程只涉及一种功能，便于日后参考。import argparsedef parse_args(): parser = argparse.ArgumentParser() parser.add_argument(&#39;name&#39;, help=&#39;test positional arg 1&#39;) parser.add_argument(&#39;age&#39;, help=&#39;test positional arg 2&#39;) ## ==&amp;gt; usage: python demo.py zg 25 parser.add_argument(&#39;-i&#39;, &#39;--input&#39;, help=&#39;test short and long arg&#39;) # optional, default is None parser.add_argument(&#39;--bool&#39;, action=&#39;store_true&#39;, help=&#39;test action#store_true&#39;) ## ==&amp;gt; usage: python demo.py --bool # out: Namespace(bool=True) parser.add_argument(&#39;-v&#39;, action=&#39;append&#39;, help=&quot;test action#append&quot;) ## ==&amp;gt; usage: python demo.py -v v1 -v v2 # out: Namespace(v=[&#39;v1&#39;, &#39;v2&#39;]) parser.add_argument(&#39;--required&#39;, required=True, help=&#39;test required&#39;) parser.add_argument(&#39;-d&#39;, dest=&#39;d_dest&#39;, help=&#39;test dest&#39;) ## ==&amp;gt; usage: python demo.py -d 3 ## out: Namespace(d_dest=&#39;3&#39;) # https://www.pynote.net/archives/2121 parser.add_argument(&#39;-i&#39;, help=&#39;test type&#39;) ## ==&amp;gt; usage: python demo.py -i 3 ## out: Namespace(i=&#39;3&#39;) parser.add_argument(&#39;-i&#39;, type=int, help=&#39;test type&#39;) ## ==&amp;gt; usage: python demo.py -i 3 ## out: Namespace(i=3) parser.add_argument(&#39;-i&#39;, default=3, help=&#39;test default&#39;) ## ==&amp;gt; usage: python demo.py ## out: Namespace(i=3) ## ==&amp;gt; usage: python demo.py -i 5 ## out: Namespace(i=&#39;5&#39;) # pay attention to the datatype here parser.add_argument(&#39;-v&#39;, metavar=&#39;--value&#39;, help=&quot;test metavar&quot;) ## ==&amp;gt; usage: python demo.py -h # It provides a different name for optional argument in [error and help messages] # https://docs.python.org/zh-cn/3/library/argparse.html#nargs parser.add_argument(&#39;-v&#39;, nargs=2, help=&quot;test nargs N&quot;) ## ==&amp;gt; usage: python demo.py -v 1 2 (only allow constant num) # out: Namespace(v=[&#39;1&#39;, &#39;2&#39;]) =&amp;gt; list ## ==&amp;gt; usage: python demo.py -v 1 # out: demo.py: error: argument -v: expected 2 arguments parser.add_argument(&#39;-v&#39;, nargs=&#39;*&#39;, help=&quot;test nargs *&quot;) # * nargs expects 0 or more arguments (allow varible num) ## ==&amp;gt; usage: python demo.py -v 1 2... (variable num) parser.add_argument(&#39;-v&#39;, nargs=&#39;+&#39;, help=&quot;test nargs *&quot;) # similar to &#39;+&#39; nargs parser.add_argument(&#39;--platform&#39;, &#39;-p&#39;, choices=[&#39;windows&#39;, &#39;linux&#39;], help=&quot;test choices&quot;) # choices limits argument values to the given list args = parser.parse_args() return argsif __name__ == &quot;__main__&quot;: args = parse_args() print(args) # a = args.XXX" }, { "title": "onnx-modifier ONNX可视化编辑!", "url": "/posts/ONNX%E5%8F%AF%E8%A7%86%E5%8C%96%E7%BC%96%E8%BE%91/", "categories": "专业积累, 工具开发", "tags": "onnx", "date": "2022-05-01 14:00:00 +0800", "snippet": " 本文可能未及时更新，请点击 onnx-modifier github仓库 查看最新版本和特性。ONNX(Open Neural Network Exchange) 是一种针对机器学习所设计的开放式的文件格式，经常作为不同框架模型转化的中间文件。有时我们拿到ONNX文件，想将它进行一些修改，比如： 删除部分节点。 比如，ONNX文件中一些前后处理的算子节点，以方便后续部署。 修改节点输入输出名。 比如修改某一节点的输入输出名称，更改模型拓扑结构。 编辑节点属性值。 增加新节点。目前常用的方法是，先可视化模型图结构，然后基于ONNX的Python API编写脚本，对模型图结构进行编辑。但这可能需要我们在可视化-脚本-可视化-…之间反复横跳。而且在一张庞大的图上搜集想要修改的节点，也比较繁琐耗时。👋能不能有一个工具，可以实时预览编辑后的可视化效果，从而更方便，快捷，直观地实现ONNX模型的编辑呢？:rocket: 这便是onnx-modifier (github)开发的动机。所有的编辑信息将最终汇总，统一送由ONNX Python API处理，得到编辑后的ONNX模型文件。目前已支持下列操作： 删除/恢复节点 删除单个节点 删除一个节点及所有以该节点为根节点的子节点 恢复误删的节点 修改节点输入输出名 编辑节点属性值 增加新节点（beta）onnx-modifier基于流行的模型可视化工具 Netron 和轻量级Web应用框架 flask 开发。希望它能给社区带来一些贡献~安装与运行目前支持两种方法运行onnx-modifier：源码+命令行启动 拉取onnx-modifier，安装所需要的Python库 git clone git@github.com:ZhangGe6/onnx-modifier.gitcd onnx-modifier pip install onnxpip install flask 运行 python app.py 从可执行文件启动 Windows: 下载可执行文件onnx-modifier.exe (27.6MB)，双击即可启动。 默认使用Edge浏览器作为运行环境。 生成可执行文件的步骤记录在app_desktop.py文件中。未来会为其他平台生成可执行文件。 点击输出中的url（如http://127.0.0.1:5000/），即可在浏览器中进入onnx-modifier界面。点击Open Model...，上传所需要编辑的模型文件，上传完毕后，网络可视化结构会自动显示。用法图结构层级的操作按钮放置在可视化页面的左上角，目前有四个：Refresh，Reset，Download和Add node. 它们的功能分别为： Refresh：刷新界面，预览当前编辑得到的模型图结构； 在当前版本里，模型图结构会在每次编辑操作后即时自动更新，无需手动刷新。所以该按钮用到的次数会比之前的版本里少上不少（解放双手:raised_hands:）。 Reset：重置模型图结构为导入时的初始状态； Download：保存编辑后的模型文件到本地。 Add node：向当前模型中，添加新节点。 节点层级的操作都在节点侧边栏里，点击某一节点后即可弹出。一起来详细康康。删除/恢复节点删除节点有两种模式：Delete With Children 和 Delete Single Node. 后者只删除当前单个节点；而前者还会自动删除以这个节点为根节点的所有子节点，当我们需要删除一长串节点时，这个功能会比较有用。 Delete With Children基于回溯算法实现。执行删除操作后，被删除的节点首先会变灰显示，以供预览。如果某一个节点被误删了，在该节点的侧边栏点击Recover Node即可以将其恢复到图中。预览确认删除操作无误后，点击Enter，图结构会刷新，显示节点删除后的状态。一个典型的删除操作如下图所示：修改节点输入输出名通过修改节点的输出输出名，我们可以对模型拓扑结构进行修改（如删除一些预处理/后处理节点）。该功能同样可以用在更改模型的输出的名称（即修改模型叶子节点的输出名）。那在onnx-modifer中要怎么做呢？很简单，找到节点侧边栏的输入输出对应的输入框，键入新的名称就可以啦。图结构会根据键入的名称即时自动刷新。举个栗子，在下图所示的模型中，我们想要删除预处理对应的节点（Sub-&amp;gt;Mul-&amp;gt;Sub-&amp;gt;Transpose），可以这样做： 点击第一个Conv节点，在弹出的属性栏中，将输入名称改为serving_default_input:0 (data_0节点的输出名)； 图结构自动刷新，可以发现，输入节点已经和第一个Conv节点直接相连，几个预处理节点也已经从前向图中分离出来，将它们删除； 完工（点击Download就可以获得编辑后的ONNX模型啦）。 如果我们希望通过修改，让节点$A$（比如上例中的data_0节点）连向节点$B$（比如上例中的第一个Conv节点），建议的方式是：将节点$B$的输入名称修改为节点$A$的输出名称，而不是把$A$的输出名称修改为节点$B$的输入名称。 因为节点$B$的输入名称可能同时为其他节点（比如上例中的Transpose节点）的输出名称，会导致一些预料外的结果。上例的修改过程如下图所示：编辑节点属性值在节点侧边栏对应的属性值输入框中，键入新的属性值即可。 点击属性值输入框右侧的+，可显示该属性的参考信息。增加新节点（beta）有时候我们希望向模型中添加新节点。onnx-modifier已开始支持该功能。在主页面的左上方，有一个Add node按钮和一个selector选择器，我们可以通过这二者的配合，完成节点的添加，只需3步： 在selector中选择要添加的节点类型，在点击Add node按钮后，一个对应类型的新节点将自动出现在图上。 selector中包含来自ai.onnx(171), ai.onnx.preview.training(4), ai.onnx.ml(18) 和 com.microsoft(1)的所有节点类型。 点击这个新节点，在弹出的侧边栏中进行节点的编辑： 节点属性：初始化为null （显示为undefined）。同上节，在对应的属性框中输入新值即可。 修改节点输入输出名。输入输出名决定了节点将插入在图结构中的位置。 完工（点击Download即可获得编辑后的ONNX模型）。以下是该功能的一些提醒和小tip： 在当前版本中，是不支持添加含有参数的节点的（比如Conv, BatchNormalization）。其他大多数节点，在我的测试中，可正确添加（比如Flatten, ArgMax, Concat）。 点击selector，输入要添加的节点的首字母（比如Flatten的f），可帮我们定位到以该字母开头的节点列表区域，加快检索速度。 点击节点侧边栏的NODE PROPERTIES的type框右侧的?，和节点属性框右侧的+，可以显示关于当前节点类型/属性值的参考信息。 为确保正确性，节点的各属性值建议全部填写（而不是留着undefined）。默认值在当前版本可能支持得还不够好。 如果一个属性值是列表类型，则各元素之间使用‘,’分隔。 在当前版本中，如果一个节点的输入/输出是一个列表类型（如Concat），限制最多显示8个。如果一个节点实际输入/输出小于8个，则填写对应数目的输入输出即可，多出来的应以list_custom开头，它们会在后续处理中自动被忽略。 这个功能还处在开发中，可能会不够鲁棒。所以如果大家在实际使用时碰到问题，非常欢迎提issue!onnx-modifer正在活跃地更新中:hammer_and_wrench:。 欢迎使用，提issue，如果有帮助的话，感谢给个:star:~示例模型文件为方便测试，以下提供一些典型的样例模型文件，主要来自于onnx model zoo squeezeNet 链接 (4.72MB) MobileNet 链接 (13.3MB) ResNet50-int8 链接 (24.6MB) movenet-lightning 链接 (9.01MB) 将Google提供的预训练tflite模型，使用tensorflow-onnx转换得到； 模型中包含前处理和一大块后处理节点。 参考资料 Netron flask ONNX IR Official doc ONNX Python API Official doc, Leimao’s Blog ONNX IO Stream Leimao’s Blog onnx-utils sweetalert" }, { "title": "部署经验总结", "url": "/posts/%E9%83%A8%E7%BD%B2%E7%BB%8F%E9%AA%8C%E7%A7%AF%E7%B4%AF/", "categories": "专业积累, 模型压缩", "tags": "实践", "date": "2022-04-15 12:31:00 +0800", "snippet": "预处理 ncnn的substract_mean_normalize​ 该函数输入像素范围在[0,255]，对于不同的框架可能需要进行转换，具体参见这里。安卓端部署 Android Studio 打包APK" }, { "title": "ACM格式输入输出总结", "url": "/posts/ACM%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E6%80%BB%E7%BB%93/", "categories": "编程积累", "tags": "", "date": "2022-04-15 12:06:00 +0800", "snippet": "Python读取单个元素t = input() # 字符t = int(input()) # 整数读取单行元素items = list(input().split(&quot; &quot;)) # 字符items = list(map(int, input().split(&quot; &quot;))) # 整数确定数目行的元素N = int(input())for i in range(N): items = list(map(int, input().split(&quot; &quot;))) # 整数为例不定数目行的元素while True: try: items = list(map(int, input().split(&quot; &quot;))) # 整数为例 except: breakC++读取单个元素int n;cin &amp;gt;&amp;gt; n;string s;cin &amp;gt;&amp;gt; s;读取单行元素// 固定个数（2个）int a, b;cin &amp;gt;&amp;gt; a &amp;gt;&amp;gt; b;// 每行第1个数字表示该行元素数目int N, val;cin &amp;gt;&amp;gt; N;for (int i = 0; i &amp;lt; N; ++i) { cin &amp;gt;&amp;gt; val;}// 单行不定个数int num;while (cin &amp;gt;&amp;gt; num) { // do sth. with `num`}// 读取并解析以逗号分隔的单行，如`a,b,c`string inp;cin &amp;gt;&amp;gt; inp; // 一整行字符串都已经读进来了vector&amp;lt;string&amp;gt; vec;string tmp_str;for (auto c : inp) { if (c == &#39;,&#39;) { vec.push_back(tmp_str); tmp_str.clear(); } else { tmp_str += c; }}vec.push_back(tmp_str);读取多行元素// 确定行数int N; // 行数cin &amp;gt;&amp;gt; N;for (int i = 0; i &amp;lt; N; ++i) { // 读取单行}// 不定行数，有文件结束标志行while (true) { // 读取单行 if (end_condition) break;}// 不定行数，每行第一个数是该行元素个数int n, val; while (cin &amp;gt;&amp;gt; n) { // 每行的第一个数 for (int i = 0; i &amp;lt; n; ++i) { cin &amp;gt;&amp;gt; val; }}// 不定行数，自己判定每一行的结束位置int num;while (cin &amp;gt;&amp;gt; num) { // do sth. with `num` if (cin.get() == &#39;\\n&#39;) { // 每行末尾的操作 }}一些需要注意的点 输入输出中，元素的间隔符不一定是空格，还可能是‘，’等，要视题目为准。参考 牛客网-OJ在线编程常见输入输出练习场 【ACM模式】牛客网OJ输入输出案例ACM机试模式Python&amp;amp;Java代码总结" }, { "title": "Python的继承", "url": "/posts/Python%E7%9A%84%E7%BB%A7%E6%89%BF/", "categories": "专业积累, 编程积累", "tags": "python", "date": "2021-12-15 16:06:00 +0800", "snippet": "面向对象的编程带来的主要好处之一是代码的重用。假如需要定义几个类，而类与类之间有一些公共的属性和方法，这时就可以把相同的属性和方法作为基类的成员，而特殊的方法及属性则在本类中定义。这样子类只需要继承基类（父类），子类就可以访问到基类（父类）的属性和方法了，从而提高代码的可扩展性和重用性。Python的继承是一个比较基础的内容，以往存在一些理解不明确的地方，因此做了一个简单学习整理，本页是我在这个过程的笔记。继承的原理框架参考class BaseClass: def __init__(self, base_param): # do some initialization self.base_param = base_paramclass SubClass(BaseClass): def __init__(self, base_param, sub_param): # this __init__ will overwrite the one of BaseClass # to inherit the attributes from base class, we should call __init__() of BaseClass explicitly, There are several ways: # way 1. This way is recommanded super().__init__(base_param) # way 2. in Python3, it is equivalent to way 1. But in Python2, we are stuck with this form super(SubClass, self).__init__(base_param) # way 3. BaseClass.__init__(self, base_param) # add new attrbites for sub class self.sub_param = sub_param一个democlass Animal: def __init__(self, name): self.name = name def talk(self): raise NotImplementedErrorclass Dog(Animal): def __init__(self, name): super().__init__(name) # super(Dog, self).__init__(name) # Animal.__init__(self, name) def talk(self): print(&quot;我叫 {}, 我会 {}&quot;.format(self.name, &quot;汪汪&quot;))class Cat(Animal): def __init__(self, name): super().__init__(name) def talk(self): print(&quot;我叫 {}, 我会 {}&quot;.format(self.name, &quot;喵喵&quot;)) dog = Dog(name=&quot;狗狗&quot;)cat = Cat(name=&quot;猫猫&quot;)dog.talk()cat.talk()输出我叫 狗狗, 我会 汪汪我叫 猫猫, 我会 喵喵同时继承多个类一个子类允许同时继承多个父类。如果子类调用的方法为两个父类公有的方法，则遵循“先来后到”原则，即调用排列在前面的父类对应的方法。参考其他关于isinstance()和type()的区别isinstance()会认为子类是一种父类类型，考虑继承关系；type()不会认为子类是一种父类类型，不考虑继承关系。参考假设已经按照上面的demo创建了dog和cat两个对象，则有print(isinstance(dog, Animal)) # Trueprint(type(dog) == Animal) # False" }, { "title": "tensorboard使用", "url": "/posts/tensorboard%E4%BD%BF%E7%94%A8/", "categories": "实践, 框架学习", "tags": "tensorboard", "date": "2021-11-16 19:45:00 +0800", "snippet": "关于本页tensorboard是一款经典的机器学习可视化工具。它原本是tensorflow的可视化工具，pytorch从1.2.0开始正式支持tensorboard。本页介绍我在使用过程中的一些操作和经验记录，便于日后参考复用，主要结合pytorch。 pytorch提供的tensorboard官方文档和示例：戳这里 一些已有的文章：1一些操作记录使用方法要运行tensorboad，需要在代码中加入from torch.utils.tensorboard import SummaryWriterwriter = SummaryWriter()writer.add_xxx()writer.close() # this can be ESSENTIAL, otherwise the last graph would not show!生成的日志文件默认存放在./runs下。 可以在定义writer时使用writer = SummeryWriter(log_dir=...)来指定日志保存位置。在终端中执行tensorboard --logdir=runs# for more usage, use `tensorboard -h` 这时默认在http://localhost:6006/ 打开tensorboard页面，显示可视化内容。常用操作绘制scalar曲线writer.add_scalar(tag, scalar_value, global_step)若要把多张图像绘制在一张图上，则使用：参考writer.add_scalars(main_tag, tag_scalar_dict, global_step)绘制直方图（如查看网络权重分布）for name, param in model.named_parameters(): writer.add_histogram(name, param)坑1：到HISTOGRAMS栏查看直方图（有一次我在DISRUBUTIONS栏看了半天愣是啥也看不到）坑2：记得最后调用writer.close()，否则最后一层的权重分布图（我这边）是看不到的显示matplotlib图像import matplotlib.pyplot as pltfig, ax = plt.subplots() ax.plot(A, B)ax.set_title(&#39;title&#39;)writer.add_figure( &#39;matplotlib demo&#39;, fig)使用过程中遇到的问题及解决tensorboard: command not found解决方法" }, { "title": "tqdm常用操作", "url": "/posts/tqdm%E4%BD%BF%E7%94%A8/", "categories": "实践, 框架学习", "tags": "tqdm", "date": "2021-10-10 21:39:00 +0800", "snippet": "关于本页tqdm及一个基于Python的开源包，可以很方便地为程序运行加上进度条，方便知道程序运行进度。本页结合官方文档，记录一些个人的常用操作。基本操作iterable-basedWrap tqdm() around any iterable:from tqdm import tqdmfor i in tqdm(range(10)): ...for i in tqdm([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;]): ...manual手动指定进度条总长度total与每次更新长度update()with tqdm(total=100) as pbar: for i in range(10): ... pbar.update(10)或pbar = tqdm(total=100)for i in range(10): ... pbar.update(10)pbar.close() # do not forget this其他让进度条显示额外信息对于from tqdm import tqdmfor i in tqdm(range(100)): loss = ...进度条默认显示为76%|████████████████████████ | 76/100 [00:33&amp;lt;00:10, 229.00it/s]但我们可以自定义输出额外的信息，如pbar = tqdm(total=len(train_dataloader))pbar.set_description(&#39;Epoch {}&#39;.format(epoch))for i, batch in enumerate(train_dataloader): loss1 = ... loss2 = ... pbar.set_postfix( loss1=&#39;{:4f}&#39;.format(loss1.item()), loss2=&#39;{:4f}&#39;.format(loss2.item()) ) pbar.update()pbar.close()此时显示为Epoch 0: 2%|██ | 88/5175 [00:02&amp;lt;02:21, 35.90it/s, loss1=2.894164, loss2=9.039881]" }, { "title": "Shell学习", "url": "/posts/Shell%E5%AD%A6%E4%B9%A0/", "categories": "实践, 硬技能", "tags": "shell, sh", "date": "2021-09-23 09:30:00 +0800", "snippet": "shell语法不多，不算难，这里作为一项硬技能，自己总结一下。主体内容来自于菜鸟教程。shell变量定义变量时，变量名和等号之间不能有空格your_name=&quot;runoob.com&quot;使用变量加美元符号，变量名外面的花括号从语法层面上是可选的，加不加都行。不过加上可以帮助解释器识别变量的边界。echo $your_name # &quot;runoob.com&quot;echo ${your_name} # &quot;runoob.com&quot;数组Bash Shell 只支持一维数组（不支持多维数组），数组元素的下标由 0 开始，Shell 数组用括号来表示，元素用”空格”符号分割开。定义数组：array=(value1 value2 ... valuen)# examplearray=(1 2 3 4)索引数组：${array[index]}获取数组的所有元素：${array[@]}# demoarray=(1 2 3 4)echo ${array} # 1echo ${array[@]} # 1 2 3 4获取数组的长度：${#array[@]}遍历数组：for v in ${array[@]}; do echo &quot;${v}&quot;;done字符串定义字符串可以用单引号，也可以用双引号。str1=&#39;string1&#39;str2=&quot;string2&quot;拼接字符串，写在一起即可str3=${str1}${str2}echo str3 # string1string2获取字符串长度echo ${#str1}提取子字符串，语法为${variable:offset:length}string=&quot;welcome&quot;echo ${string:1:4} # 从index为1的位置开始，读取4个元素。输出 elco字符串运算符a=&quot;abc&quot;b=&quot;efg&quot;[ $a = $b ] # 判断是否相等，返回 false。[ $a != $b ] # 判断是否不等，返回 true。# 更多运算符参见https://www.runoob.com/linux/linux-shell-basic-operators.html向shell脚本传入参数使用$获取传入的参数。$1表示第一个参数，$2表示第二个参数… （$0是执行的bash脚本路径）示例：建立test.sh如下#!/bin/bashecho &quot;脚本路径: $0&quot;echo &quot;第一个参数: $1&quot;echo &quot;第二个参数: $2&quot;执行./test.sh 1 2得到输出：脚本路径: path/to/test.sh第一个参数: 1第二个参数: 2获取更多传入参数的信息，参见菜鸟教程-Shell 传递参数基本运算符参见菜鸟教程-Shell 基本运算符算术运算符原生bash不支持简单的数学运算，可借助expr实现。a=2b=2echo &quot;两数之和为 : `expr $a + $b`&quot;echo &quot;两数之差为 : `expr $a - $b`&quot;echo &quot;两数之积为 : `expr $a \\* $b`&quot; # 注意，单纯的*会报语法错误，要用\\*echo &quot;两数之积为 : `expr $a / $b`&quot;需要注意： 完整的表达式要被 ` 包含，注意这个字符不是常用的单引号，是在 Esc 键下边的反引号。 表达式和运算符之间要有空格关系运算符参见菜鸟教程-Shell 基本运算符布尔运算符参见菜鸟教程-Shell 基本运算符let语句let 命令是 BASH 中用于计算的工具，用于执行一个或多个表达式，变量计算中不需要加上$ 来表示变量。菜鸟教程-Linux let 命令示例a=1b=2let c=a+b # 不用加$echo $c # 3shell流程控制分支ifif conditionthen command1 ... commandN fiif elseif conditionthen command1 ... commandNelse commandfiif else ifif condition1then command1elif condition2 then command2else commandNfi使用((...))作为判断语句。示例if (( $a == $b ))then echo &quot;a 等于 b&quot;fifor循环for var in item1 ... itemNdo command1 ... commandndone写成一行for var in item1 item2 ... itemN; do command1; ...; commandn; done;示例for loop in 1 2 3 4 5do echo &quot;The value is: $loop&quot;donewhile循环while conditiondo commanddone示例val=1while(( $val&amp;lt;=5 ))do echo $val let val++donebreak和continuebreak和continue作为一条command出现在bash脚本中。其含义与其他语言一致。shell函数基本格式funname(){ ... [return sth;]} 可以向函数传入参数，参见下面的示例 在函数体内部，通过 $n 的形式来获取传入的参数值。$1表示第一个参数，$2表示第二个参数… （$0是执行的bash脚本路径） 如果函数有返回值，则函数返回值在调用该函数后通过使用$?获得示例add(){ echo &quot;第一个参数为 $1&quot; echo &quot;第二个参数为 $2&quot; let val=$1+$2 return $val}add 1 2echo &quot;输入的两个数字之和为 $?&quot; # 3shell输出可用echo和printf方法。printf比较类似C语言中的printf，支持格式化输出。参见 菜鸟教程-Shell echo命令 和 菜鸟教程-Shell printf 命令。shell 输入输出重定向 大多数 UNIX 系统命令从你的终端接受输入并将所产生的输出发送回到您的终端。一个命令通常从一个叫标准输入的地方读取输入，默认情况下，这恰好是你的终端。同样，一个命令通常将其输出写入到标准输出，默认情况下，这也是你的终端。菜鸟教程-Shell 输入/输出重定向将输出重定向到 file （file已经存在的内容将被清空替代）command &amp;gt; file将输出重定向到 file （在file结尾追加）command &amp;gt;&amp;gt; file将输入重定向到 filecommand &amp;lt; file示例输入重定向：# in.txt12# demo.shadd(){ read a echo &quot;第一个参数为 $a&quot; read b echo &quot;第二个参数为 $b&quot; let val=$a+$b return $val}add echo &quot;输入的两个数字之和为 $?&quot;执行bash ./str_demo.sh &amp;lt; in.txt# 从in.txt读取输入，在终端输出结果输出重定向：# demo.shadd(){ echo &quot;第一个参数为 $1&quot; echo &quot;第二个参数为 $2&quot; let val=$1+$2 return $val}add 1 2echo &quot;输入的两个数字之和为 $?&quot; # 3执行bash ./str_demo.sh &amp;gt; out.txt# 输出将写入out.txt，而不显示在终端当然，也可以选择性地把部分语句重定向，如# demo.shadd(){ echo &quot;第一个参数为 $1&quot; echo &quot;第二个参数为 $2&quot; let val=$1+$2 return $val}add 1 2echo &quot;输入的两个数字之和为 $?&quot; &amp;gt; out.txt执行bash ./str_demo.sh# 只会把最后一句的输出重定向到out.txt其他Q1加入现在有一个demo.sh文件，使用 bash ./demo.sh sh ./demo.sh ./demo.sh这三者的区别？答：TODO。参考https://stackoverflow.com/a/20616103/10096987" }, { "title": "网络推理加速的一些基础操作", "url": "/posts/%E7%BD%91%E7%BB%9C%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E6%93%8D%E4%BD%9C/", "categories": "专业积累, 模型压缩", "tags": "", "date": "2021-08-17 17:03:00 +0800", "snippet": "本页介绍一些遇到的加速的基础操作。推理时BN层融合入卷积层参考 To execute neural network inference, kernels are invoked for neural network layers in order to compute the output tensors given the input tensors. Each kernel call will bring some overhead time….To achieve high throughput and low latency for neural network inference, the rule of thumb is to have fewer large kernel calls instead of many small kernel calls.设卷积核的权重和偏置为\\(W\\)和\\(b\\)，卷积层输出和输出分别为\\(X\\)和\\(X^{\\prime}\\)​，有\\[X^{\\prime} = X * W + b\\]卷积核输出\\(X^{\\prime}\\)紧接进入BN层，设BN层的输出为\\(Y\\)，则有\\[\\begin{aligned}Y &amp;amp;= \\gamma \\frac{X^{\\prime} - \\mu}{\\sqrt{\\delta^2 + \\epsilon}} + \\beta= \\gamma \\frac{X * W + b - \\mu}{\\sqrt{\\delta^2 + \\epsilon}} + \\beta \\\\ &amp;amp;= X * (\\frac{\\gamma}{\\sqrt{\\delta^2 + \\epsilon}}W) + \\beta + \\frac{\\gamma}{\\sqrt{\\delta^2 + \\epsilon}}(b - \\mu) \\\\ &amp;amp;= X * W^{\\prime} + b^{\\prime}\\end{aligned}\\]即 conv + BN 等效于调用一个权重为\\(W^{\\prime} = \\frac{\\gamma}{\\sqrt{\\delta^2 + \\epsilon}}W\\)，偏置为\\(b^{\\prime} = \\beta + \\frac{\\gamma}{\\sqrt{\\delta^2 + \\epsilon}}(b - \\mu)\\)的新卷积核。该步融合减少了kernel的invoke次数，达到加速的效果。以下是Pytorch源码中的BN fusion部分# torch/nn/utils/fusion.pydef fuse_conv_bn_weights(conv_w, conv_b, bn_rm, bn_rv, bn_eps, bn_w, bn_b): if conv_b is None: conv_b = bn_rm.new_zeros(bn_rm.shape) bn_var_rsqrt = torch.rsqrt(bn_rv + bn_eps) conv_w = conv_w * (bn_w * bn_var_rsqrt).reshape([-1] + [1] * (len(conv_w.shape) - 1)) conv_b = (conv_b - bn_rm) * bn_var_rsqrt * bn_w + bn_b return torch.nn.Parameter(conv_w), torch.nn.Parameter(conv_b)bias correction高通的一篇工作(DFQ)发现，对网络中的某一层进行权重量化（激活尚未量化）之后，得到的输出相比于量化之前是有偏的。具体来说，对于相同的输入\\(x\\)，设量化前权重和输出分别为\\(W\\)和\\(y\\)，量化后权重和输出分别为\\(\\widetilde{W}\\)和\\(\\widetilde{y}\\)，有\\[E[\\widetilde{y}] \\neq E[y]\\]其中\\(\\widetilde{y} = \\widetilde{W}x,\\quad y = Wx\\).这可能是量化过程中对权重进行clip导致的。而这种输出的分布偏移传到下一层，可能会影响效果，因此需要将这部分输出上的偏移抵消掉。有：\\[\\begin{align}E[y] &amp;amp;= E[Wx] \\\\ &amp;amp;= E[(\\widetilde{W} - \\epsilon)x] = E[\\widetilde{W}x - \\epsilon x] \\\\ &amp;amp;= E[\\widetilde{y}] - E[\\epsilon x] \\\\ &amp;amp;= E[\\widetilde{y}] - \\epsilon E[x]\\end{align}\\]其中\\(\\epsilon = \\widetilde{W} - W\\)为量化误差。 在训练数据（此时无需标签）可得的情况下，具体的做法为，先将一批数据输入全精度模型，得到每一层输出的均值\\(E[y_l], \\quad l=1, 2, \\cdots, L\\)，然后将同一批数据通入权重量化后模型，分别统计第\\(l\\)层的输出期望\\(E[\\widetilde{y_l}]\\)，将偏差\\(E[\\widetilde{y_l}] - E[y]\\)加到该层的bias参数中即可。 当训练数据不可得时，DFQ提出可以从该层前的BN层中获取\\(E[x]\\)，从而得到补偿值\\(\\epsilon E[x]\\)，有兴趣的读者可以阅读原文中详细的描述。 这里高通提出的bias correction是一种后训练（post-training）的方法。参考" }, { "title": "关于加速上的一些指标", "url": "/posts/%E5%85%B3%E4%BA%8E%E5%8A%A0%E9%80%9F%E4%B8%8A%E7%9A%84%E4%B8%80%E4%BA%9B%E6%8C%87%E6%A0%87/", "categories": "专业积累, 模型压缩", "tags": "", "date": "2021-08-15 12:41:00 +0800", "snippet": "本文总结记录一些衡量压缩算法速度/复杂度的一些指标。FLOPsFLOPs全称为floating point operations，意指浮点运算数，理解为计算量，可以用来衡量算法/模型的复杂度。 与FLOPS，flops or flop/s区分开来，它们是floating point operations per second的缩写，意指每秒（最多）浮点运算次数，理解为计算速度，常用来衡量硬件性能。wiki计算原理示例分别以卷积层和全连接层为例介绍FLOPs的计算原理 参考：卷积层设卷积核输出特征图尺寸为\\(C_o \\times H_o \\times W_o\\)​​，输入特征图通道数为\\(C_i\\)​，则对于输出特征图的每一个pixel，需要的乘法次数为\\(C_i \\times K^2\\)​，加法次数为\\(C_i \\times K^2 - 1\\)​，共为\\(C_i \\times K^2 + C_i \\times K^2 - 1 = 2 \\times C_i \\times K^2 - 1\\)​​.因此考虑输出特征图上的所有pixel，总的FLOPs为\\(FLOPs_{conv} = (2 \\times C_i \\times K^2 - 1) \\times H_o \\times W_o \\times C_o\\) 上述计算没有考虑bias，如果考虑bias，则对输出特征图的每一个pixel会增加一次加法，有\\(FLOPs= (2 \\times C_i \\times K^2) \\times H_o \\times W_o \\times C_o\\)​全连接层设全连接层输入个数为\\(I\\)，输出个数为\\(O\\)，则对每一个输出，都有\\(I\\)次乘法与\\(I - 1\\)次加法，因此考虑所有输出，总的FLOPs为\\(FLOPs_{fc} = (2 \\times I - 1) \\times O\\) 上述计算同样没有考虑bias，如果考虑bias，则每一个输出都会增加一次加法，有\\(FLOPs= (2 \\times I ) \\times O\\)注：上述计算均没有考虑输入的Batch size维度。自动计算的工具sovrasov/flops-counter.pytorch Flops counter for convolutional networks in pytorch framework使用示例 (经微调，以repo提供为准)import torchvision.models as modelsimport torchfrom ptflops import get_model_complexity_infonet = models.alexnet()macs, params = get_model_complexity_info(model=net, input_res=(3, 224, 224), as_strings=True, print_per_layer_stat=True, verbose=True)print(&#39;{:&amp;lt;30} {:&amp;lt;8}&#39;.format(&#39;Computational complexity: &#39;, macs))print(&#39;{:&amp;lt;30} {:&amp;lt;8}&#39;.format(&#39;Number of parameters: &#39;, params))将返回各计算单元的计算量及比例，如下：AlexNet( 61.101 M, 100.000% Params, 0.716 GMac, 100.000% MACs, (features): Sequential( 2.47 M, 4.042% Params, 0.657 GMac, 91.804% MACs, (0): Conv2d(0.023 M, 0.038% Params, 0.07 GMac, 9.848% MACs, 3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)) (1): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.027% MACs, inplace=True) (2): MaxPool2d(0.0 M, 0.000% Params, 0.0 GMac, 0.027% MACs, kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) ... (10): Conv2d(0.59 M, 0.966% Params, 0.1 GMac, 13.936% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.006% MACs, inplace=True) (12): MaxPool2d(0.0 M, 0.000% Params, 0.0 GMac, 0.006% MACs, kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) ) (avgpool): AdaptiveAvgPool2d(0.0 M, 0.000% Params, 0.0 GMac, 0.001% MACs, output_size=(6, 6)) (classifier): Sequential( 58.631 M, 95.958% Params, 0.059 GMac, 8.195% MACs, (0): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.5, inplace=False) ... (6): Linear(4.097 M, 6.705% Params, 0.004 GMac, 0.573% MACs, in_features=4096, out_features=1000, bias=True) ))Computational complexity: 0.72 GMacNumber of parameters: 61.1 M其他参考 神经网络运行效率" }, { "title": "优秀的教程/资料", "url": "/posts/%E4%BC%98%E7%A7%80%E7%9A%84%E8%B5%84%E6%96%99%E6%95%99%E7%A8%8B/", "categories": "有趣/有用的资料, 优秀资料，教程", "tags": "", "date": "2021-08-11 14:00:00 +0800", "snippet": "本页积累一些在学习过程中遇到的比较好的教程或资料，记录不迷路~kalman filter rlabbe/Kalman-and-Bayesian-Filters-in-Python Kalman Filter book using Jupyter Notebook 非常详细生动（可能有点啰嗦…），配合Jupyter Notebook，丰富可视化，卡尔曼滤波入门强推:+1: KalmanFilter.Nettransformer bilibili-台大李宏毅21年机器学习课程 self-attention和transformer 李宏毅老师永远的神！Transformer入门强推:+1: 这个视频更多是从seq2seq，机器翻译的角度来讲解transformer bilibili-李沐-transfomer论文逐段精读强化学习 datawhalechina/easy-rl正则表达式 RegEx 正则表达式30分钟入门教程 Programiz - Python RegEx 机器学习基础 CORNELL CS4780 “Machine Learning for Intelligent Systems” “This man is a gift” 推理加速 :+1:旷视天元Megengine博客 CPU矩阵乘法优化解析 CUDA 矩阵乘法终极优化指南 ARM 算子性能优化上手指南 (知乎) 开源 2 年半，除了性能优化我们啥也没做 ​" }, { "title": "matplotlib系统学习", "url": "/posts/matplotlib%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0/", "categories": "实践, 框架学习", "tags": "matplotlib", "date": "2021-08-10 21:47:00 +0800", "snippet": "关于本页matplotlib是一个常用的Python绘图包，本页介绍我在使用过程中的一些操作和经验记录，便于日后参考复用。图1 matplotlib绘图的关键元素基本使用导入包import matplotlib.pyplot as pltA quick and dirty way - 使用plt.xxxA = [1, 2, 3]B = [4 ,5, 6]plt.plot(A, B)plt.title(&#39;test title&#39;)plt.show()以上代码默认建立一张Figure，在这张Figure上隐式地建立一个axes，然后把内容绘制在这个axes上。matplotlib要画出一张图真是有N种方式。而plt.xxx尽管方便，但有的功能可能不支持，所以为了方便代码的复用，希望自己稳定地使用一种更加普适的方式。使用ax.xxxfig, ax = plt.subplots()print(type(fig)) # &amp;lt;class &#39;matplotlib.figure.Figure&#39;&amp;gt;print(type(ax)) # &amp;lt;class &#39;matplotlib.axes._subplots.AxesSubplot&#39;&amp;gt;plt.subplots()的默认值为ncols=nrows=1，故以上代码返回的ax为单一的一个AxesSubplot对象（不可索引），我们可以在这个ax上绘图：ax.plot(A, B)绘制多图n_rows, n_cols = 2, 2fig, axes = plt.subplots(n_rows, n_cols)print(type(axes))print(axes)输出为：&amp;lt;class &#39;numpy.ndarray&#39;&amp;gt;[[&amp;lt;AxesSubplot:&amp;gt; &amp;lt;AxesSubplot:&amp;gt;] [&amp;lt;AxesSubplot:&amp;gt; &amp;lt;AxesSubplot:&amp;gt;]]可以看到返回的axes是一个由AxesSubplot组成的numpy.ndarray对象，我们需要使用常规的索引得到对应的ax，然后在其上绘图：axes[0][0].plot(...)axes[0][1].plot(...)axes[1][0].plot(...)axes[1][1].plot(...)使用for循环：for i in range(n_rows): for j in range(n_cols): axes[i][j].plot(...) # plot anything you want on the coresponding ax plt.show()如果觉得二维的索引写起来有些麻烦，也可以for i in range(n_rows * n_cols): ax = plt.subplot(n_rows, n_cols, i+1) # because index in plt starts from 1, so we use `i+1` here ax.plot(...)plt.show()为图像增加属性有时需要为图像增加一些属性，如坐标轴label，图的title等。ax.set_xlabel(&#39;x_label&#39;) # x_labelax.set_ylabel(&#39;y_label&#39;) # y_labelax.set_title(&#39;ax title&#39;) # ax titlefig.suptitle(&#39;fig title&#39;) # fig title绘制曲线为曲线增加属性markerax.plot(A, B, marker=&#39;o&#39;)colorax.plot(A, B, color=&#39;b&#39;) matplotlib也支持根据关键字指定颜色，如color=#C6FDFA 配色方案参考：配色卡 一个ax上绘制多条曲线并添加图例ax.plot(A1, B1, label=&#39;plot_1&#39;)ax.plot(A2, B2, label=&#39;plot_2&#39;)ax.legend()水平/竖直线参考 水平线：ax.axhline(y=5, xmin=0.1, xmax=0.9, linestyle=&quot;--&quot;) 竖直线：ax.axvline(x=5, ymin=0.1, ymax=0.9, linestyle=&quot;--&quot;)绘制柱状图参考给柱状图增加annotation的方案： 参考绘制直方图参考 参考ax.hist(dist, bins=n_bins)绘制带colorbar的(二维特征)图参考 参考import matplotlib.pyplot as pltimport numpy as npplt.imshow(np.random.random((100, 100)))plt.colorbar()# plt.show()plt.savefig(&quot;colorbar.png&quot;)绘制箱型图参考参考其他一些操作 保存图像：plt.savefig(PATH) 把图例放在图像外：参考，调节legend()参数，一个示例如下： # plt.figure(figsize=(8, 4.8)) # 可能需要调节画布大小，防止图像本身被图例空间过度压缩 # 把图例的左上角对齐到图的右上角plt.legend(loc=&#39;upper left&#39;, bbox_to_anchor=(1., 1.))plt.tight_layout() # 没有这一行，图例可能被裁切掉 关闭坐标轴：plt.axis(&#39;off&#39;) 解决中文无法显示：参考 import matplotlib.pyplot as plt plt.rcParams[&#39;font.sans-serif&#39;] = [&#39;SimHei&#39;] # 步骤一（替换sans-serif字体）plt.rcParams[&#39;axes.unicode_minus&#39;] = False # 步骤二（解决坐标轴负数的负号显示问题） 图像属性设置 设置Figure大小：plt.figure(figsize=(width, height)) 默认是(6.4, 4.8) 参考 设定图像分辨率：plt.figure(dpi=1200) 设置字体大小：plt.rcParams.update({&#39;font.size&#39;: 22}) 默认是10，参考 参考 Medium - What Are the “plt” and “ax” in Matplotlib Exactly? matplotlib：先搞明白plt. /ax./ fig再画 matplotlib gallery geeksforgeeks - Matplotlib Tutorial" }, { "title": "Latex cheat sheet", "url": "/posts/Latex-cheat-sheet/", "categories": "有趣/有用的资料, cheat sheets", "tags": "cheat sheets", "date": "2021-05-08 10:46:00 +0800", "snippet": "各种符号可参考这篇博客。另外总结一些平时用到的符号： 组合数 \\(\\tbinom{n}{r}\\)形式：\\tbinom{n}{r} \\(C_n^r\\)形式：C_n^r 分段函数 f(x)= \\begin{cases} 0 &amp;amp; condition1 \\\\ 1 &amp;amp; condition2 \\end{cases} \\[f(x)= \\begin{cases} 0 &amp;amp; condition1 \\\\ 1 &amp;amp; condition2 \\end{cases}\\] 换行对齐 \\begin{aligned} f(x) &amp;amp;= ax + b \\\\ g(x) &amp;amp;= cx + d\\end{aligned} \\[\\begin{aligned} f(x) &amp;amp;= ax + b \\\\ g(x) &amp;amp;= cx + d\\end{aligned}\\] 一些数学符号： 范数：\\Vert x \\Vert _p \\(\\rightarrow\\) $\\Vert x \\Vert _p$ 连乘：\\prod \\(\\rightarrow\\) $\\prod$ 积分：\\int \\(\\rightarrow\\) $\\int$ 正比于：\\propto \\(\\rightarrow\\) $\\propto$ 一些数学字体 空心字体：$\\mathbb{R}$ \\(\\rightarrow\\) \\(\\mathbb{R}\\) 加粗字体：$\\mathbf{R}$ \\(\\rightarrow\\) \\(\\mathbf{R}\\) 花体：$\\mathcal{R}$ \\(\\rightarrow\\) $\\mathcal{R}$ 一些上标： $\\hat{x}$ \\(\\rightarrow\\) $\\hat{x}$ $\\tilde{x}$ \\(\\rightarrow\\) $\\tilde{x}$ $\\overline{x}$ \\(\\rightarrow\\) $\\overline{x}$ $\\dot{x}$ \\(\\rightarrow\\) $\\dot{x}$ 图片在document开始前，定义定义图片文件目录与扩展名% 定义图片文件目录与扩展名\\graphicspath{ {figures/} }\\DeclareGraphicsExtensions{.pdf,.eps,.png,.jpg,.jpeg}\\begin{document}......\\end{document}单张图片\\begin{figure}[!htp] \\centering \\includegraphics[width=0.3\\linewidth]{fig_path} % [height=2cm] \\caption{fig_caption} \\label{fig:}\\end{figure}两（多）图并排放置（不共用图编号）% 两图并排放置（不共用图编号）\\begin{figure}[!htp] \\begin{minipage}{0.48\\textwidth} \\centering \\includegraphics[height=3cm]{fig_path} \\caption{验电器示意} \\label{fig:parallel1} \\end{minipage}\\hfill \\begin{minipage}{0.48\\textwidth} \\centering \\includegraphics[height=3cm]{fig_path} \\caption{场景} \\label{fig:parallel2}\\end{minipage}\\end{figure}两图并排放置（共用图编号）\\usepackage{subcaption} % package `subcaption` is needed% 两图并排放置（共用图编号）\\begin{figure}[!htp] \\centering \\begin{subfigure}{0.4\\textwidth} \\centering \\includegraphics[height=2cm]{sjtu-badge.pdf} % width=0.3\\linewidth \\caption{子图1名} \\end{subfigure} % \\hspace{1cm} \\begin{subfigure}{0.4\\textwidth} \\centering \\includegraphics[height=2cm]{sjtu-name.pdf} % width=0.3\\linewidth \\caption{子图2名} \\end{subfigure} \\caption{图名} \\label{fig:subfigure}\\end{figure}表格参考\\begin{table}[!htp] \\centering \\label{tab:labelOfTable} \\caption{captionOfTable} \\begin{tabular}{|c|c|c|c|} \\hline &amp;amp; &amp;amp; &amp;amp; \\\\ \\hline &amp;amp; &amp;amp; &amp;amp; \\\\ \\hline \\end{tabular}\\end{table}合并单元格：参考双栏模板下，要插入跨栏图表将\\begin{figure}...\\end{figure}替换为\\begin{figure*}...\\end{figure*}即可。表格table同理。代码和算法代码段\\begin{lstlisting}[language=Python]\\end{lstlisting} 伪代码参考引用BibTeX参考% main.tex\\begin{document}Here~\\cite{zhang2022integer} is a sample citation.\\bibliographystyle{plain}\\bibliography{citations} \\end{document}% citations.bib@inproceedings{zhang2022integer, title={...}, author={...}}...其他 文字居中：\\centerline{} 文字下划线：\\uline{} 调节行距：\\linespread{} ：\\linespread{1.3} \\(\\rightarrow\\) 1.3倍行距 参考 另起一页：\\clearpage 公示中插入中文\\mbox{}：\\sin(\\mbox{狗}) \\(\\rightarrow\\) \\(\\sin(\\mbox{狗})\\) 创建新的格式（如加框文字） % 在\\begin{document}之前\\usepackage{environ}\\usepackage{tikz}\\NewEnviron{elaboration}{\\par\\begin{tikzpicture}\\node[rectangle,minimum width=0.9\\textwidth] (m) {\\begin{minipage}{0.85\\textwidth}\\BODY\\end{minipage}};\\draw[dashed] (m.south west) rectangle (m.north east);\\end{tikzpicture}} % 在正文中\\begin{elaboration}what you write...\\end{elaboration} 添加附录 \\usepackage[title]{appendix} \\begin{document}... \\begin{appendices}\\section{Some Notation}\\end{appendices} \\end{document} 手动缩小间距 \\vspace{-0.8cm} 参考文献中使用first author et al.格式（缩减参考文献篇幅） 参考 author={Alpher, R. and others} 参考文献显示问号的解决 参考 以矢量图形式插入PPT中绘制的图 PPT中选中图形，右键 $\\rightarrow$ 另存为图片 $\\rightarrow$ 保存为.svg格式 将.svg转化为.pdf：INKSCAPE 打开.svg文件 另存为.pdf文件 在Latex中插入.pdf文件 " }, { "title": "Follow!", "url": "/posts/Follow!/", "categories": "有趣/有用的资料, 资料杂记", "tags": "follow", "date": "2021-04-29 19:25:00 +0800", "snippet": "业界大拿们 Song Han 主页 Google scholar Cewu Lu 主页 Google scholar Rongrong Ji 主页 Google scholar Yunhe Wang 主页 Google scholar 知乎 Kurt Keutzer 主页 Google scholar Kilian Q. Weinberger 主页 Google scholar一些棒棒哒博主们 Oldpan的个人博客 &amp;amp; 知乎 写了非常不错的部署相关的博客（像TensorRT详细入门指北） GiantPandaCV 苏剑林-BoJone - 科学空间 Jonathan Hui polariszhao 来呀，快活呀~ YaHei 从零开始的BLOG “世事难料，保持低调”的CSDN博客 Machine Learning@Berkeley 黎明灰烬 海滨的Blog 学习资源 Valse - Bilibili 上海交通大学IPADS - Bilibili " }, { "title": "再遇caffe", "url": "/posts/%E5%86%8D%E9%81%87caffe/", "categories": "实践, 框架学习", "tags": "caffe", "date": "2021-04-19 13:13:00 +0800", "snippet": "为啥叫再遇caffe，是因为近一年前因为实验室项目需要，第一次使用到了caffe。当时caffe的环境配置和不够直观易用简直让我对caffe印象很坏…现在因为涉及部署相关的工作，再次接触到caffe。现在再看，倒觉得caffe像是深度学习框架特别朴素纯真的样子，也学到了一些新的caffe的使用技巧，以后可能还会用到，做个记录。Python环境下直接使用opencv的dnn模块运行caffe模型偶然发现opencv的dnn模块直接加载和使用caffe模型，以后遇到仅有caffe预训练模型的情况，可以不必配置caffe环境，直接使用opencv运行使用啦！（opencv永远的神！）一个样例import cv2protoFile = &#39;path/to/xx.prototxt&#39;weightsFile = &#39;path/to/xx.caffemodel&#39;net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)# === CPU === # # net.setPreferableBackend(cv2.dnn.DNN_TARGET_CPU)# print(&quot;Using CPU device&quot;)# === GPU(not tested) === # # net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)# net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)# print(&quot;Using GPU device&quot;)img = cv2.imread(&#39;path/to/img&#39;)inpBlob = cv2.dnn.blobFromImage(img, 1.0 / 255, (inWidth, inHeight), (0, 0, 0), swapRB=False, crop=False)net.setInput(inpBlob, &#39;image&#39;) # bind the input and input layeroutput = net.forward(&#39;net_output&#39;) # set to the layer of which you want the feature相关函数 Deep Neural Network module blobFromImage readNetFromCaffe setPreferableBackend cv::dnn::Net Class Reference setInput forward caffe模型转pytorch一次需要把caffe模型转到Pytorch形式，记录一下具体转化的过程。已知：caffe模型定义文件model_deploy.prototxt，caffe权重文件model_weight.caffemodel;目标：Pytorch模型定义文件（model.py）与pytorch权重文件model.pt流程： 权重转化：使用caffemodel2pytorch工具，将caffe权重文件转化位pytorch权重文件。 python caffemodel2pytorch.py model_weight.caffemodel -o model_weight.pt 得到pytorch权重文件model_weight.pt。 得到Pytorch模型结构定义。 使用netron可视化caffe模型结构; 结合可视化的网络结构，手写pytorch模型定义文件(我真的是手写，不知道是否有更方便的办法？)。 输入同一样本，检查caffe和Pytorch两种模型的输出是否相同。相同便转化成功，如不相同则比对网络内部对应输出，逐点排查。 img0 = cv2.imread(&#39;path/to/img&#39;)# get caffe outputnet = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)inpBlob = cv2.dnn.blobFromImage(img0, 1.0 / 255, (inWidth, inHeight), (0, 0, 0), swapRB=False, crop=False)net.setInput(inpBlob, &#39;image&#39;) # bind the input and input layercaffe_out = net.forward(&#39;net_output&#39;) # set to the layer of which you want the feature# get pytorch outputmodel = Model()model.load_state_dict(torch.load(&#39;path/to/model.pt&#39;))img = cv2.resize(img0, ((inWidth, inHeight)))img = np.transpose(img, (2, 0, 1)).astype(np.float32) # HWC -&amp;gt; CWHimg = torch.from_numpy(img)img = img / 255img = torch.unsqueeze(img, 0)pytorch_out = model(img)一个供参考的pytorch模型定义文件框架# model.pyimport torchimport torch.nn as nnfrom collections import OrderedDictclass Pose25Model(nn.Module): def __init__(self): super(Pose25Model, self).__init__() self.get_module_dict() self.get_torch_module() # 配置网络层名称及其参数 def get_module_dict(self): ... for stage_id in range(0, 4): self.blocks[&#39;Mconv1_stage%d&#39; % stage_id] = OrderedDict([ # (&#39;conv_name&#39;: [in_channels, inner_channel, kernel_size, stride(default=1), padding(default=0)]) (&#39;Mconv1_stage%d&#39; % stage_id, [input_channel, inner_channel, 3, 1, 1]), ... ]) # 为网络层分配实际的pytorch网络module def get_torch_module(self): for key in self.blocks.keys(): block_info = self.blocks[key] for layer_name, v in block_info.items(): # print(layer_name, v) if &#39;pool&#39; in layer_name: layer = nn.MaxPool2d(kernel_size=v[0], stride=v[1], padding=v[2], ceil_mode=True) self.add_module(layer_name, layer) else: layer = nn.Conv2d(in_channels=v[0], out_channels=v[1], kernel_size=v[2], stride=v[3], padding=v[4]) self.add_module(layer_name, layer) ReLU_name = &#39;relu&#39; + layer_name.split(&#39;conv&#39;)[1] self.add_module(ReLU_name, torch.nn.ReLU(inplace=True)) def forward(self, x): x = ... ... return x大小坑们Q：caffe和pytorch模型得到的输出大小不一致？A：看看Pytorch中，maxpooling层的定义class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)Pytorch中ceil_mode参数默认为False，在计算特征图时，会向下取整；而caffe是默认向上取整的。因此若使用Pytorch的默认参数，会使caffe得到的特征图尺寸比pytorch的要大。在Pytorch中修改ceil_mode=True就可以啦。参考Q：转化得到的pt文件是正确的（可以通过将对应层的参数打印出来比对下），Pytorch的模型定义似乎也没有问题，但是为什么输出结果就不一样呢？A：这个原因可能有很多…我的原因是cat的拼接顺序。比如对特征沿channel维进行拼接时，不同的拼接顺序，得到的结果肯定也是不一样的。所以遇到cat操作要小心，要看准确的拼接顺序，需要到.prototxt中去看，并据此写pytorch的cat语句，netron作图是不会反映这个顺序的，不可想当然就直接按照它给的顺序从左向右依次拼接了。" }, { "title": "TensorRT使用", "url": "/posts/TensorRT%E4%BD%BF%E7%94%A8/", "categories": "实践, 框架学习", "tags": "tensorrt", "date": "2021-04-16 09:43:00 +0800", "snippet": "TensorRT安装按照官方文档来，一般没啥问题。我使用的是tar格式安装。 由于TensorRT 7对CUDA版本的要求，我原先服务器的CUDA版本为10.1，因此新安装了CUDA11.0并做了切换（可以参考这里） TensorRT需要安装CuDNN（同样参照CuDNN官方文档）。 20210416：需要提醒的是，文档上给了tar文件安装指引，但是CuDNN下载页只提供了后缀为.solitairetheme8和.deb的文件下载。如果使用tar文件方式，需要下载.solitairetheme8格式的那个，下载后将其后缀改为.tgz后按tar文件安装方式来即可。 遇到的问题Q：ImportError: libnvinfer.so.7: cannot open shared object file: No such file or directoryA： 参考，这是由于tensorrt的路径没有添加到自己的环境变量。解决方法为：# 打开~/.bashrcvim ~/.bashrc# 点击insert进入编辑模式，将下面一行加入（注意替换自己的tensorRT路径）export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:path/to/your_tensorRT/lib# 编辑完成后，依次键入Esc, :wq退出# 刷新source ~/.bashrc # 这一步可能要等较长时间另，关于Linux的环境变量，可参考这篇Pytorch 模型向TensorRT部署一个很好的TensorRT入门资料目前已经尝试了两种可行的方案： 使用NVIDIA-AI-IOT/torch2trt pytorch \\(\\rightarrow\\) onnx \\(\\rightarrow\\) TensorRT使用torch2trtgithubdocument假设现在已经有了模型的定义文件model.py和pytorch权重文件weight.pt基础操作torch2trt基础操作使用起来非常简单import torchfrom torch2trt import torch2trtfrom torchvision.models.alexnet import alexnet# create some regular pytorch modelmodel = alexnet(pretrained=True).eval().cuda()# create a dummy input datax = torch.ones((1, 3, 224, 224)).cuda()# convert pytorch model to trt model (fp32) model_trt = torch2trt(model, [x])# excute and check the output of the converted trt_modely = model(x)y_trt = model_trt(x)print(torch.max(torch.abs(y - y_trt)))# save the trt model as a state_dict.torch.save(model_trt.state_dict(), &#39;alexnet_trt.pth&#39;)# load the saved model into a TRTModulefrom torch2trt import TRTModulemodel_trt = TRTModule()model_trt.load_state_dict(torch.load(&#39;alexnet_trt.pth&#39;))一些更高级一些的convert操作参照&amp;lt;torch2trt_root&amp;gt;/torch2trt/torch2trt.py#L482def torch2trt(module, inputs, input_names=None, output_names=None, log_level=trt.Logger.ERROR, max_batch_size=1, fp16_mode=False, max_workspace_size=1&amp;lt;&amp;lt;25, strict_type_constraints=False, keep_network=True, int8_mode=False, int8_calib_dataset=None, int8_calib_algorithm=DEFAULT_CALIBRATION_ALGORITHM, int8_calib_batch_size=1, use_onnx=False, **kwargs): ...设定max_batch_size# convert pytorch model to trt model (fp32) Max_batch_size = 8model_trt = torch2trt(model, [x], max_batch_size=Max_batch_size)convert to fp16 trt model# convert pytorch model to trt model (fp16) model_trt = torch2trt(model, [x], fp16_mode=True)convert to int8 model转化为int8模型需要构建calibrate数据集进行校正，按照这里使用Pytorch的dataset类构建一个cali_dataset传入即可。cali_dataset = Your_defined_cali_dataset# convert pytorch model to trt model (int8)model_trt = torch2trt(model, [x], int8_mode=True, int8_int8_calib_dataset=cali_dataset)注：官方的__getitem__(self, index)实现为def __getitem__(self, idx): image, _ = self.dataset[idx] image = image[None, ...] # add batch dimension return [image]但我运行起来会有维度不匹配的问题，我在实际使用时使用了如下定义def __getitem__(self, idx): image, _ = self.dataset[idx] if self.transform is not None: image = self.transform(image) return [image]扩展converter目前torch2trt支持的所有converter可以看这里，但并没有涵盖所有TensorRT已经支持的所有算子。当所转化的模型中包含目前torch2trt的converter没有实现的算子，则会报错并模型转化失败，比如对torch.nn.MaxPool3d会出现warning: encountered known unsupported method torch.max_pool3dwarning: encountered known unsupported method torch.nn.fucntional.max_pool3d[TensorRT] ERROR: INVALID_ARGUMENT: Cannot find binding of given name: input_0理论上，只要是TensorRT已经支持的算子，即使是torch2trt没有定义对应的converter，我们参照torch2trt已经实现的converer，自己也可以很方便地将其实现，官方给了一个ReLU算子的converter例子。自定义一个converter的流程为： 在&amp;lt;torch2trt_root&amp;gt;/torch2trt/conveters/下新建一个&amp;lt;your_custom_converter&amp;gt;.py 在&amp;lt;your_custom_converter&amp;gt;.py中实现converter 在&amp;lt;torch2trt_root&amp;gt;/torch2trt/conveters/__init__.py中import新定义的converter 重新安装torch2trt：python setup.py install以下记录下MaxPool3d的converter实现流程，非常方便，很大程度上参照了avg_pool.py中avg_pool3d的converter的实现。 在&amp;lt;torch2trt_root&amp;gt;/torch2trt/conveters/下新建一个max_pool3d.py 在max_pool3d.py实现torch.nn.MaxPool3d的converter # A CUSTOM ONE #from torch2trt.torch2trt import *from torch2trt.module_test import add_module_test @tensorrt_converter(&#39;torch.nn.functional.max_pool3d&#39;, enabled=trt_version() &amp;gt;= &#39;7.0&#39;)def convert_max_pool_trt7(ctx): # print(ctx.method_args) # parse args # refer to https://pytorch.org/docs/stable/generated/torch.nn.MaxPool3d.html input = get_arg(ctx, &#39;input&#39;, pos=0, default=None) kernel_size = get_arg(ctx, &#39;kernel_size&#39;, pos=1, default=None) stride = get_arg(ctx, &#39;stride&#39;, pos=2, default=None) padding = get_arg(ctx, &#39;padding&#39;, pos=3, default=0) dilation = get_arg(ctx, &#39;dilation&#39;, pos=4, default=False) return_indices = get_arg(ctx, &#39;return_indices&#39;, pos=5, default=True) ceil_mode = get_arg(ctx, &#39;ceil_mode&#39;, pos=6, default=True) # print(kernel_size, stride, padding, ceil_mode, count_include_pad) # get input trt tensor (or create constant if it doesn&#39;t exist) input_trt = add_missing_trt_tensors(ctx.network, [input])[0] output = ctx.method_return input_dim = input.dim() - 2 # get kernel size if not isinstance(kernel_size, tuple): kernel_size = (kernel_size,) * input_dim # get stride if not isinstance(stride, tuple): stride = (stride,) * input_dim # get padding if not isinstance(padding, tuple): padding = (padding,) * input_dim # For type, check the TensorRT Python API document layer = ctx.network.add_pooling_nd( input=input_trt, type=trt.PoolingType.MAX, window_size=kernel_size) layer.stride_nd = stride layer.padding_nd = padding if ceil_mode: layer.padding_mode = trt.PaddingMode.EXPLICIT_ROUND_UP output._trt = layer.get_output(0) 看起来很简单的对不？需要提醒的是最开始获取当前layer的参数 input = get_arg(ctx, &#39;input&#39;, pos=0, default=None)...ceil_mode = get_arg(ctx, &#39;ceil_mode&#39;, pos=6, default=True) 从torch2trt.torch2trt.py看到get_arg的定义 def get_arg(ctx, name, pos, default): if name in ctx.method_kwargs: return ctx.method_kwargs[name] elif len(ctx.method_args) &amp;gt; pos: return ctx.method_args[pos] else: return default 这里的method_kwargs和Pytorch对应的算子输入参数是一一对应的！（从这个函数的pos参数来看，除了第0位挤入了input，其他参数都是依次向后排开的）所以要转化某一个算子，需要到Pytorch官网去看看这个算子的参数表，比如torch.nn.MaxPool3d 在&amp;lt;torch2trt_root&amp;gt;/torch2trt/conveters/__init__.py中import新定义的converter from .max_pool3d import * 重新安装torch2trt：python setup.py install 再次尝试转化含torch.nn.MaxPool3d的模型，报错消失，转化后的模型运行也正常，问题解决~！Pytorch \\(\\rightarrow\\) ONNX \\(\\rightarrow\\) TensorRT其他 竟然还可以用pip安装tensorRT?! here" }, { "title": "pybind11使用", "url": "/posts/pybind11%E4%BD%BF%E7%94%A8/", "categories": "实践, 框架学习", "tags": "pybind", "date": "2021-04-13 19:58:00 +0800", "snippet": "pybind11实现python和C++之间的数据通信。这是文档。Pytorch的文档CUSTOM C++ AND CUDA EXTENSIONS也提到了如何进行绑定，结合起来一个简单的绑定例子是文件结构extend_demo/ setup.py op.cpp op_test.py# setup.pyfrom setuptools import setup, Extensionfrom torch.utils import cpp_extensionsetup(name=&#39;simpleOp&#39;, ext_modules=[cpp_extension.CppExtension(&#39;simpleOp&#39;, [&#39;op.cpp&#39;])], cmdclass={&#39;build_ext&#39;: cpp_extension.BuildExtension})// op.cpp#include &amp;lt;torch/extension.h&amp;gt;#include &amp;lt;iostream&amp;gt;int add(int x, int y){ return x + y;}PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) { m.def(&quot;add&quot;, &amp;amp;add, &quot;add two number&quot;);}执行python setup.py install，开始编译。编译成功后，看看conda list里有simpleOp 0.0.0 &amp;lt;pip&amp;gt;然后就可以像导入一个普通的包来调用我们自定义的模块啦：# op_test.pyimport torch # torch should be imported first !import simpleOpa = 1b = 1c = simpleOp.add(a, b)print(a, b, c) # 1 1 2参考Pytorch学习 (二十一) ——自定义C++/ATen扩展" }, { "title": "C++积累", "url": "/posts/C++%E7%A7%AF%E7%B4%AF/", "categories": "专业积累, 编程积累", "tags": "c++", "date": "2021-04-10 10:16:00 +0800", "snippet": "C++计时参考#include &amp;lt;iostream&amp;gt;#include &amp;lt;chrono&amp;gt; long fibonacci(unsigned n){ if (n &amp;lt; 2) return n; return fibonacci(n-1) + fibonacci(n-2);} int main(){ auto start = std::chrono::steady_clock::now(); std::cout &amp;lt;&amp;lt; &quot;f(42) = &quot; &amp;lt;&amp;lt; fibonacci(42) &amp;lt;&amp;lt; &#39;\\n&#39;; auto end = std::chrono::steady_clock::now(); std::chrono::duration&amp;lt;double&amp;gt; elapsed_seconds = end-start; std::cout &amp;lt;&amp;lt; &quot;elapsed time: &quot; &amp;lt;&amp;lt; elapsed_seconds.count() &amp;lt;&amp;lt; &quot;s\\n&quot;;}// output// f(42) = 267914296// elapsed time: 1.88232sC++计时也可使用clock_t，但是当使用多核多线程时可能会计时有较大偏差。这里有一篇看起来比较好的C++日期和时间总结。C/C++多文件共享全局变量时，使用extern的规范参考extern的使用规范，内容如下：1、在定义文件中定义全局变量, 比如A.cpp中定义全局变量 int a;2、在对应的头文件A.h中声明外部变量extern int a;3、在引用a变量的文件中# include &quot;A.h&quot;;lambda表达式知乎-Lambda 表达式有何用处？如何使用？C++ 11 Lambda表达式C++ lambda表达式与函数对象C++类型转换：string -&amp;gt; charstd::string str(&quot;abc&quot;);fprintf(f, &quot;%s\\n&quot;, str.c_str());C++ 头文件理解理解 C++ 中的头文件和源文件的作用C++ printf函数格式化输出符号说明参考" }, { "title": "OpenCV常用代码段-Python", "url": "/posts/OpenCV%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81%E6%AE%B5-Python/", "categories": "专业积累, 杂记", "tags": "python, opencv", "date": "2021-03-23 10:35:00 +0800", "snippet": " NOTE：opencv中的位置索引都是先x方向后y方向的，比如绘制函数时点的位置表示为（x, y）, resize中新尺寸大小指定格式为（width, height）等等。这个和numpy中先y方向后x方向不同，要注意区分。图像读取与保存参考：GeeksforGeeks, CSDNimport cv2# 读取img_path = &#39;&#39;img = cv2.imread(img_path) # BGR (default)# 获取尺寸信息height, width, channel = img.shape# 保存cv2.imwrite(&#39;save_name.png&#39;, img)值得注意的是cv2.imread(path, flag)函数在一些场景下可能要指定flag，可选的flag有 cv2.IMREAD_COLOR：当加载彩色图片时。这是默认值，等价的操作为直接传入1； cv2.IMREAD_GRAYSCALE：当加载灰度图时。等价的操作为直接传入0； 读取灰度图若不指定flag，则读取的灰度图也会默认转化为3通道！ cv2.IMREAD_UNCHANGED：按照原图的格式读取。等价的操作为直接传入-1. 比如要读取16位的.tiff数据时。 图像变形参考1 参考2 内插方式有三种：cv2.INTER_AREA, cv2.INTER_CUBIC和cv2.INTER_LINEAR(default)。 缩小图片时，内插方式一般使用cv2.INTER_AREA会最好；放大图片时，使用cv2.INTER_CUBIC会效果最好（但是慢），使用cv2.INTER_LINEAR会较快，并且看起来还OK.指定尺寸resized = cv2.resize(img, dsize=(width, height), interpolation)指定缩放比例half = cv2.resize(image, (0, 0), fx=0.5, fy=0.5)图像绘制参考画框img = cv2.rectangle(img, (x1, y1), (x2, y2), color=(0, 0, 255), thickness=2)# img = cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), color=(0, 0, 255), thickness=2)# img = cv2.rectangle(img, (int(x), int(y)), (int(x+w), int(y+h)), color=(0, 0, 255), thickness=2)画圆img = cv2.circle(img, center, radius, color, thickness)画线img = cv2.line(img, start_point, end_point, color, thickness)写文字img = cv2.putText(img, &#39;text&#39;, (50,150), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 1)各参数依次是：照片/添加的文字/左上角坐标/字体/字体大小/颜色/字体粗细其他BGR =&amp;gt; RGBopencv读取的图片是BGR格式的，如果需要RGB格式（如numpy可视化）的话，要做一下转换# way1img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)# way2image = image[:, :, ::-1]opencv调用摄像头参考上demo:import cv2# camera_addr = 0 # 本机摄像头camera_addr = &#39;rtsp://username:password@ip&#39; # 网络摄像头cap = cv2.VideoCapture(camera_addr)assert cap.isOpened(), &#39;Failed to open %s&#39; % camera_addrw = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))fps = cap.get(cv2.CAP_PROP_FPS)print(&#39;success (%gx%g at %.2f fps).&#39; % (w, h, fps))while True: ret, frame = cap.read() cv2.imshow(&#39;Video&#39;, frame) cv2.waitKey(1)这里有一个opencv的参数列表，获取一段视频的总帧数为frames_num=cap.get(cv2.CAP_PROP_FRAME_COUNT)" }, { "title": "Python常用代码段", "url": "/posts/Python%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81%E6%AE%B5/", "categories": "专业积累, 杂记", "tags": "python", "date": "2021-03-23 10:34:00 +0800", "snippet": "在程序中输出临时文件大小def print_size_of_model(model): torch.save(model.state_dict(), &quot;temp.p&quot;) print(&#39;Size (MB):&#39;, os.path.getsize(&quot;temp.p&quot;)/2**20) os.remove(&#39;temp.p&#39;)print_size_of_model(model)" }, { "title": "模型压缩实战", "url": "/posts/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E5%AE%9E%E6%88%98/", "categories": "专业积累, 模型压缩", "tags": "实践", "date": "2021-03-16 09:08:00 +0800", "snippet": "666DZY666/micronet github repo Pytorch official Quantization intro 知乎 - Gemfield - PyTorch的量化 A developer-friendly guide to model quantization with PyTorch FBGEMM - Blog PruningTensorflow official腾讯 PocketFlow基于Tensorflow开发，集成了当前主流与腾讯AI Lab自研的多个模型压缩与训练算法，方便快速地部署到移动端产品上。 github repo 如何看待腾讯 AI Lab 开源的 PocketFlow？ 代码风格，文档，算法数目。看知乎的评论好像现在还不是很好用。 Intel Distiller基于 PyTorch 的开源神经网络压缩框架，Distiller 是由 Intel AI Lab 维护的基于 PyTorch 的开源神经网络压缩框架。主要包括： 用于集成剪枝，正则化和量化算法的框架。 一套用于分析和评估压缩性能的工具。 现有技术压缩算法的示例实现。 github repo Distiller 模型剪枝教程 CSDN-Distiller:神经网络压缩研究框架百度PaddleSlimPaddlePaddle实现了目前主流的网络量化、剪枝、蒸馏三种压缩策略，并可快速配置多种压缩策略组合使用。针对体积已经很小的 MobileNet 模型，在模型效果不损失的前提下实现 70% 以上的体积压缩。 github repo 精度无损，体积压缩70%以上，百度PaddleSlim为你的模型瘦身 深度学习-模型压缩-PaddleSlim 微软NNIgithub repo自动机器学习(AutoML)工具和框架，其中集成了多种模型压缩算法，并支持PyTorch，TensorFlow，MXNet，Caffe2等多个开源框架。不过重点不是模型压缩。上新了，NNI！微软开源自动机器学习工具NNI概览及新功能详解TensorRT（推理）一个高性能的深度学习推理（Inference）优化器，可以为深度学习应用提供低延迟、高吞吐率的部署推理。 TensorRT(1)-介绍-使用-安装 如何使用TensorRT对训练好的PyTorch模型进行加速? - 伯恩legacy的文章 - 知乎腾讯NCNN（推理）纯 C++ 实现，跨平台，支持 android iosncnn 是一个为手机端极致优化的高性能神经网络前向计算框架。ncnn 从设计之初深刻考虑手机端的部署和使用。手机端 cpu 的速度快于目前所有已知的开源框架。注：只包含前向计算，因此无法进行训练，需要导入其他框架训练好的模型参数。 github repo 移动端机器学习框架ncnn简介与实践 pytorch模型的部署（系列一）–ncnn的编译和使用 - limzero的文章 - 知乎 https://zhuanlan.zhihu.com/p/137458205 nihui-视频-谈谈ncnn的设计理念和软件工程 https://github.com/zchrissirhcz/awesome-ncnn 😎 A Collection of Awesome NCNN-based Projects 知乎专栏-ncnn初探 关注nihui大佬知乎不迷路 :star: 如何阅读一个前向推理框架？以 NCNN 为例。腾讯TNN github repo" }, { "title": "ResNet源码阅读", "url": "/posts/ResNet%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/", "categories": "专业积累, 杂记", "tags": "源码", "date": "2021-03-11 21:42:00 +0800", "snippet": "本文对ResNet源码(Pytorch)进行解读。许多网络结构的改进是在其基础上做的，因此有必要仔细理解下。 注：为了增强代码的易读性，本文所展示的代码基于2021年3月11日的torchvision代码删减得到的，因此和源码并不完全一致，需要的读者可以前往查看torchvision的ResNet源码。首先定义了预训练模型下载地址，封装了3x3和1x1的卷积核model_urls = { &#39;resnet18&#39;: &#39;https://download.pytorch.org/models/resnet18-5c106cde.pth&#39;, ... &#39;resnext50_32x4d&#39;: &#39;https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth&#39;, ... &#39;wide_resnet50_2&#39;: &#39;https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth&#39;, ...}# basic 3x3 conv wrapperdef conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -&amp;gt; nn.Conv2d: &quot;&quot;&quot;3x3 convolution with padding&quot;&quot;&quot; return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation, groups=groups, bias=False, dilation=dilation)# basic 1x1 conv wrapperdef conv1x1(in_planes: int, out_planes: int, stride: int = 1) -&amp;gt; nn.Conv2d: &quot;&quot;&quot;1x1 convolution&quot;&quot;&quot; return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)接下来正式开始网络的定义。两种基本模块ResNet主要是由两种基本模块堆叠而成的：BasicBlock和BottleNeck，模块结构定义如下# Naive residual block for Renset18/34class BasicBlock(nn.Module): expansion = 1 def __init__( self, inplanes, planes, stride=1, downsample=None, ): super(BasicBlock, self).__init__() # Both self.conv1 and self.downsample layers downsample the input when stride != 1 self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = nn.BatchNorm2d(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = nn.BatchNorm2d(planes) self.downsample = downsample self.stride = stride def forward(self, x): identity = x # Conv + BatchNorm + RelU out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return out# BottleNeck Residual block for Renset50/101/152class Bottleneck(nn.Module): # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2) # while original implementation places the stride at the first 1x1 convolution(self.conv1) # according to &quot;Deep residual learning for image recognition&quot;https://arxiv.org/abs/1512.03385. # This variant is also known as ResNet V1.5 and improves accuracy according to # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch. expansion = 4 def __init__( self, inplanes, planes, stride=1, downsample=None ): super(Bottleneck, self).__init__() # Both self.conv2 and self.downsample layers downsample the input when stride != 1 self.conv1 = conv1x1(inplanes, planes) self.bn1 = nn.BatchNorm2d(planes) self.conv2 = conv3x3(planes, planes, stride) self.bn2 = nn.BatchNorm2d(planes) self.conv3 = conv1x1(planes, planes * self.expansion) self.bn3 = nn.BatchNorm2d(planes * self.expansion) self.relu = nn.ReLU(inplace=True) self.downsample = downsample self.stride = stride def forward(self, x): identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return out 两个模块的实现均比较直白，画出示意图如下图所示。图1 左：BasicBlock, 右：BottleNeck. 途中标黄代表特征图维度数可以总结为，两个block类型都是输入一个inplanes维的特征图，输出一个planes*block.expansion维的特征图（注意而不是planes。BasicBlock的expansion=1）。downsample操作是为了对shortcut支路进行大小或维度上的调整，以使得该路输出与residual路支路输出保持维度一致，以执行相加操作。downsample的具体定义会在下文构建整个ResNet网络时提到。ResNet网络整体结构class ResNet(nn.Module): def __init__( self, block, layers, num_classes=1000, zero_init_residual=False ): super(ResNet, self).__init__() self.inplanes = 64 self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False) self.bn1 = nn.BatchNorm2d(self.inplanes) self.relu = nn.ReLU(inplace=True) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) self.layer1 = self._make_layer(block, 64, layers[0]) self.layer2 = self._make_layer(block, 128, layers[1], stride=2) self.layer3 = self._make_layer(block, 256, layers[2], stride=2) self.layer4 = self._make_layer(block, 512, layers[3], stride=2) self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) self.fc = nn.Linear(512 * block.expansion, num_classes) for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode=&#39;fan_out&#39;, nonlinearity=&#39;relu&#39;) elif isinstance(m, (nn.BatchNorm2d)): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) # Zero-initialize the last BN in each residual branch, # so that the residual branch starts with zeros, and each residual block behaves like an identity. # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677 if zero_init_residual: for m in self.modules(): if isinstance(m, Bottleneck): nn.init.constant_(m.bn3.weight, 0) # type: ignore[arg-type] elif isinstance(m, BasicBlock): nn.init.constant_(m.bn2.weight, 0) # type: ignore[arg-type] # construct layer/stage conv2_x,conv3_x,conv4_x,conv5_x def _make_layer(self, block, planes, blocks, stride=1): downsample = None # when need to downsample if stride != 1 or self.inplanes != planes * block.expansion: downsample = nn.Sequential( conv1x1(self.inplanes, planes * block.expansion, stride), nn.BatchNorm2d(planes * block.expansion), ) layers = [] layers.append(block(self.inplanes, planes, stride, downsample)) # inplanes are expanded / self.inplanes is freshed for next block self.inplanes = planes * block.expansion for _ in range(1, blocks): layers.append(block(self.inplanes, planes)) return nn.Sequential(*layers) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) x = self.avgpool(x) # For `torch.flatten`, this blog may help: https://blog.csdn.net/GhostintheCode/article/details/102530451 # so `x = torch.flatten(x, 1)` is equivalent to `x = x.view(x.size(0), -1)` in the previous versions, where `x.size(0)` is the batch_size x = torch.flatten(x, 1) x = self.fc(x) return x可以看到，首先一个7 x 7的卷积作用在输入的3维图片上，并输入一个64维的特征图（64也就为self.inplanes的初始值），通过BatchNorm层，ReLU层，MaxPool层；然后经过4层layer，这4层layer通过_make_layer()函数构建，是上述两种模块的堆叠，下文将详细介绍；最后经过一个AveragePooling层，再经过一个fc层得到分类输出。另外在网络搭建起来后，还对模型的参数(Conv2d、BatchNorm2d、last BN)进行了初始化。_make_layer()函数是理解网络结构的关键。单独拎出来看一下：def _make_layer(self, block, planes, blocks, stride=1): downsample = None if stride != 1 or self.inplanes != planes * block.expansion: downsample = nn.Sequential( conv1x1(self.inplanes, planes * block.expansion, stride), nn.BatchNorm2d(planes * block.expansion), ) layers = [] layers.append(block(self.inplanes, planes, stride, downsample)) self.inplanes = planes * block.expansion for _ in range(1, blocks): layers.append(block(self.inplanes, planes)) return nn.Sequential(*layers)一个_make_layer()构建一个layer层，每一个layer层是上述两种模块的堆叠。输入参数中block代表该layer堆叠模块的类型，可选BasicBlock或者BottleNeck；blocks代表该layer中堆叠的block的数目；planes与该layer最终输出的维度数有关，注意最终输出的维度数=planes * block.expansion.首先判断是否需要downsample操作。如前所述，downsample操作是为了对shortcut支路进行大小或维度上的调整，以使得该路输出与residual支路输出保持一致，保证可以执行相加操作。因此有两种情况需要对block的shortcut支路执行downsample操作： stride != 1：需要对shortcut支路特征图进行大小上的调整。在整个代结构中，卷积的stride值都是默认为1的，在同样默认为1的padding作用下，得到的特征图大小将保持不变。但可以看到，layer2，layer3，layer4在构建时，指定了stride为2（只对该layer的第一个block生效），这将使得residual支路的输出特征图在大小上小于shortcut支路，因此需要对shortcut支路增加一个downsample支路，也做一下stride=2的卷积，保证两支路输出大小一致，即 conv1x1(self.inplanes, planes * block.expansion, [stride=2]) self.inplanes != planes * block.expansion：需要对shortcut支路特征图进行维度数目上的调整。shortcut支路的输出维度为即为输入维度self.inplanes，而residual支路的输出维度为plane * block.expansion，二者可能不一致，也就是self.inplanes != planes * block.expansion，当该情况发生，对shortcut支路输出进行维度上的调整，从self.inplanes调整至planes * block.expansion，即 conv1x1(self.inplanes, [planes * block.expansion], stride) 当一个layer包含多个block时，是通过向layers列表中依次加入每个block，来实现block的堆叠的。第一个block需要特殊处理，该block依据传入的self.inplanes, planes以及stride判断，可能含有downsample支路；这个block的输出维度是planes*block.expansion。紧接着便把self.inplanes更新为此值作为后续block的输入维度。后面的block的stride为默认值1，同时，由于输入为self.inplanes，输出为planes*block.expansion，而self.inplanes = planes * block.expansion，因此不会出现特征图大小或者尺寸不一致的情况，不可能出现downsample操作，就可以放心for循环加进列表啦。ResNet实例化def resnet18(pretrained=False, **kwargs): &quot;&quot;&quot;Constructs a ResNet-18 model. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet &quot;&quot;&quot; model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls[&#39;resnet18&#39;])) return modeldef resnet34(pretrained=False, **kwargs): &quot;&quot;&quot;Constructs a ResNet-34 model. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet &quot;&quot;&quot; model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls[&#39;resnet34&#39;])) return modeldef resnet50(pretrained=False, **kwargs): &quot;&quot;&quot;Constructs a ResNet-50 model. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet &quot;&quot;&quot; model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls[&#39;resnet50&#39;])) return modeldef resnet101(pretrained=False, **kwargs): &quot;&quot;&quot;Constructs a ResNet-101 model. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet &quot;&quot;&quot; model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls[&#39;resnet101&#39;])) return modeldef resnet152(pretrained=False, **kwargs): &quot;&quot;&quot;Constructs a ResNet-152 model. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet &quot;&quot;&quot; model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls[&#39;resnet152&#39;])) return model通过指定block的类别(如BasicBlock)，四个layer依次堆叠的block数目(如[2, 2, 2, 2])，以及一些字典类型参数**kwargs，就可以实例化出一个ResNet模型啦。完整代码 需要再次强调的是，本文所展示的代码基于2021年3月11日的torchvision代码删减得到的，因此和源码并不完全一致，需要的读者可以前往查看torchvision的ResNet源码。import torchimport torch.nn as nnimport torch.utils.model_zoo as model_zoo__all__ = [&#39;ResNet&#39;, &#39;resnet18&#39;, &#39;resnet34&#39;, &#39;resnet50&#39;, &#39;resnet101&#39;, &#39;resnet152&#39;, &#39;resnext50_32x4d&#39;, &#39;resnext101_32x8d&#39;, &#39;wide_resnet50_2&#39;, &#39;wide_resnet101_2&#39;]model_urls = { &#39;resnet18&#39;: &#39;https://download.pytorch.org/models/resnet18-5c106cde.pth&#39;, &#39;resnet34&#39;: &#39;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#39;, &#39;resnet50&#39;: &#39;https://download.pytorch.org/models/resnet50-19c8e357.pth&#39;, &#39;resnet101&#39;: &#39;https://download.pytorch.org/models/resnet101-5d3b4d8f.pth&#39;, &#39;resnet152&#39;: &#39;https://download.pytorch.org/models/resnet152-b121ed2d.pth&#39;, &#39;resnext50_32x4d&#39;: &#39;https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth&#39;, &#39;resnext101_32x8d&#39;: &#39;https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth&#39;, &#39;wide_resnet50_2&#39;: &#39;https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth&#39;, &#39;wide_resnet101_2&#39;: &#39;https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth&#39;,}def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -&amp;gt; nn.Conv2d: &quot;&quot;&quot;3x3 convolution with padding&quot;&quot;&quot; return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation, groups=groups, bias=False, dilation=dilation)def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -&amp;gt; nn.Conv2d: &quot;&quot;&quot;1x1 convolution&quot;&quot;&quot; return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)class BasicBlock(nn.Module): expansion = 1 def __init__( self, inplanes, planes, stride=1, downsample=None, ): super(BasicBlock, self).__init__() # Both self.conv1 and self.downsample layers downsample the input when stride != 1 self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = nn.BatchNorm2d(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = nn.BatchNorm2d(planes) self.downsample = downsample self.stride = stride def forward(self, x): identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return outclass Bottleneck(nn.Module): # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2) # while original implementation places the stride at the first 1x1 convolution(self.conv1) # according to &quot;Deep residual learning for image recognition&quot;https://arxiv.org/abs/1512.03385. # This variant is also known as ResNet V1.5 and improves accuracy according to # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch. expansion = 4 def __init__( self, inplanes, planes, stride=1, downsample=None ): super(Bottleneck, self).__init__() # Both self.conv2 and self.downsample layers downsample the input when stride != 1 self.conv1 = conv1x1(inplanes, planes) self.bn1 = nn.BatchNorm2d(planes) self.conv2 = conv3x3(planes, planes, stride) self.bn2 = nn.BatchNorm2d(planes) self.conv3 = conv1x1(planes, planes * self.expansion) self.bn3 = nn.BatchNorm2d(planes * self.expansion) self.relu = nn.ReLU(inplace=True) self.downsample = downsample self.stride = stride def forward(self, x): identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return outclass ResNet(nn.Module): def __init__( self, block, layers, num_classes=1000, zero_init_residual: bool = False ): super(ResNet, self).__init__() self.inplanes = 64 self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False) self.bn1 = nn.BatchNorm2d(self.inplanes) self.relu = nn.ReLU(inplace=True) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) self.layer1 = self._make_layer(block, 64, layers[0]) self.layer2 = self._make_layer(block, 128, layers[1], stride=2) self.layer3 = self._make_layer(block, 256, layers[2], stride=2) self.layer4 = self._make_layer(block, 512, layers[3], stride=2) self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) self.fc = nn.Linear(512 * block.expansion, num_classes) for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode=&#39;fan_out&#39;, nonlinearity=&#39;relu&#39;) elif isinstance(m, (nn.BatchNorm2d)): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) # Zero-initialize the last BN in each residual branch, # so that the residual branch starts with zeros, and each residual block behaves like an identity. # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677 if zero_init_residual: for m in self.modules(): if isinstance(m, Bottleneck): nn.init.constant_(m.bn3.weight, 0) # type: ignore[arg-type] elif isinstance(m, BasicBlock): nn.init.constant_(m.bn2.weight, 0) # type: ignore[arg-type] def _make_layer(self, block, planes, blocks, stride=1): downsample = None if stride != 1 or self.inplanes != planes * block.expansion: downsample = nn.Sequential( conv1x1(self.inplanes, planes * block.expansion, stride), nn.BatchNorm2d(planes * block.expansion), ) layers = [] layers.append(block(self.inplanes, planes, stride, downsample)) self.inplanes = planes * block.expansion for _ in range(1, blocks): layers.append(block(self.inplanes, planes)) return nn.Sequential(*layers) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) x = self.avgpool(x) # For `torch.flatten`, this blog may help: https://blog.csdn.net/GhostintheCode/article/details/102530451 # so `x = torch.flatten(x, 1)` is equivalent to `x = x.view(x.size(0), -1)` in the previous versions, where `x.size(0)` is the batch_size x = torch.flatten(x, 1) x = self.fc(x) return xdef resnet18(pretrained=False, **kwargs): &quot;&quot;&quot;Constructs a ResNet-18 model. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet &quot;&quot;&quot; model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls[&#39;resnet18&#39;])) return modeldef resnet34(pretrained=False, **kwargs): &quot;&quot;&quot;Constructs a ResNet-34 model. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet &quot;&quot;&quot; model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls[&#39;resnet34&#39;])) return modeldef resnet50(pretrained=False, **kwargs): &quot;&quot;&quot;Constructs a ResNet-50 model. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet &quot;&quot;&quot; model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls[&#39;resnet50&#39;])) return modeldef resnet101(pretrained=False, **kwargs): &quot;&quot;&quot;Constructs a ResNet-101 model. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet &quot;&quot;&quot; model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls[&#39;resnet101&#39;])) return modeldef resnet152(pretrained=False, **kwargs): &quot;&quot;&quot;Constructs a ResNet-152 model. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet &quot;&quot;&quot; model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls[&#39;resnet152&#39;])) return modelif __name__ == &#39;__main__&#39;: model = resnet18() print(model)参考ResNet Pytorch官方实现源码解析" }, { "title": "visdom使用", "url": "/posts/Visdom%E4%BD%BF%E7%94%A8/", "categories": "实践, 框架学习", "tags": "pytorch, visdom", "date": "2021-03-06 11:42:00 +0800", "snippet": "官方github repo首先在服务器端安装visdompip install visdom使用流程简述 服务器端启动visdom（建议在screen下进行） visdom # equivalent to running python -m visdom.server. 当使用VS code时，在终端窗口port栏即可看到8097的port，点击Open in Browser即可打开浏览器显示visdom界面。 如果不是使用VS code或者登入http://localhost:8097/没法显示到visdom界面，可尝试做好端口映射。在本机执行 ssh -C -f -N -g -L 8097:localhost:8097 -p &amp;lt;your_server_port&amp;gt; &amp;lt;user_name&amp;gt;@&amp;lt;ip&amp;gt; 默认端口为8097，若需要指定端口，可执行visdom -port &amp;lt;new_port&amp;gt; 服务器端运行代码 # filename: vis_exp.py# This is a toy example import visdomviz = visdom.Visdom(env=&#39;&amp;lt;your_env_name&amp;gt;&#39;) iter_set = []loss_set = [] for iter in total_iter: loss = ... iter_set.append(iter) loss_set.append(loss) viz.line(X=iter_set, Y=loss_set, win=&#39;&amp;lt;your_win_name&amp;gt;&#39;) 在服务器端已启动visdom后（第1步），运行该程序： python vis_exp.py 本机浏览器登入localhost:8097，即可观察到上述定义的曲线。 Demos官方github给了一些demo，可参照，另可参考知乎。以下纪录一些常用操作，方便直接使用。 指定win为某一名称，则可在所在窗口进行更新，而不是新建一个窗口作图，参见这里绘制曲线绘制单条曲线viz.line(X=x_vec, Y=y_vec, win=&#39;win_name&#39;, # 在所在窗口进行更新，而不是新建一个窗口作图 opts=dict(xlabel=&#39;xlabel&#39;, ylabel=&#39;ylabel&#39;,title=&#39;title&#39;))绘制多条曲线并加上图例import numpy as npviz.line(X=np.column_stack((X_tensor_set_1, X_tensor_set_2)), Y=np.column_stack((Y_tensor_set_1, Y_tensor_set_2)), win=&#39;win_name&#39;, opts=dict(xlabel=&#39;xlabel&#39;, ylabel=&#39;ylabel&#39;,title=&#39;title&#39; legend=[&#39;legend1&#39;, &#39;legend2&#39;]) )文本框# append mannertextwindow = viz.text(&#39;Hello World! More text should be here&#39;)viz.text(&#39;And here it is&#39;, win=textwindow, append=True)# not append mannerviz.text(&amp;lt;your_text&amp;gt;, win=&#39;&amp;lt;your_win_name&amp;gt;&#39;)图片# 注意这里传入的图片应为CHW格式的# 单张viz.image( np.random.rand(3, 512, 256), opts=dict(title=&#39;Random!&#39;, caption=&#39;How random.&#39;),)# 多张viz.images( np.random.randn(20, 3, 64, 64), opts=dict(title=&#39;Random images&#39;, caption=&#39;How random.&#39;))其他关闭窗口参考# 通过指定win来关闭对应窗口。当win为None时，关闭当前环境下所有窗口vis.close(win=None)保存和本地加载visdom实验结果This github repo theevann/visdom-save helps." }, { "title": "模型压缩自己的总结", "url": "/posts/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E8%87%AA%E5%B7%B1%E7%9A%84%E6%80%BB%E7%BB%93/", "categories": "专业积累, 模型压缩", "tags": "", "date": "2021-03-03 08:45:00 +0800", "snippet": "关于STE(straight through estimator)以二值网络为例，对网络进行训练时，需要对参数做\\(sign\\)操作后参与运算，有\\[q = sign(r)\\]但由于\\(sign\\)函数在\\(r=0\\)处不可导，在\\(r \\neq 0\\)处导数为\\(0\\)，因此反向传播在这一步是失效的，不能有效实现参数的更新。STE本身的模样straight through estimator，直通估计。非常直白，顾名思义就是在反向传播的过程中，当梯度传递遇到\\(sign\\)函数时，直接跳过\\(sign\\)函数，把二值参数的梯度作为对应的浮点型参数的梯度来进行参数更新，即\\[g_r = g_q\\]直通估计(STE)示意改良——“饱和STE”在BinaryNet中作者发现，对activation进行二值化时（可视为激活函数），当\\(sign\\)函数的输入的绝对值大于1的时候，将梯度置0，可以得到更好的实验结果，即\\[g_r = g_q \\mathbf{1}_{|r \\le 1|}\\]暂且可以叫做“饱和STE”，这也是后面不少工作对activation沿用的处理方法。关于早期量化文章的迷思主要是BinaryConnect和BinaryNet这两篇。 BinaryConnect只对weight使用进行量化。在反向传播过程中，使用最原始的STE。另外，对实值的weights单独加了一个截断函数clip(x,-1,1)。 BinaryNet同时对weight和activation进行量化。在反向传播过程中，对activation使用饱和STE。对实值的weights同样加了一个截断函数clip(x,-1,1)。 QA【Q】：对weights施加截断函数clip(x,-1,1)的目的？【A】：如果实值weights是没有设置边界的，这样它就有可能会一直累加到特别大的值，从而与二值化的weights之间的量化误差越来越大，积重难返，所以作者对实值的weights单独加了一个截断函数clip(x,-1,1)，将其限制在-1和+1之间，这样使得实值weights和二值化weights的距离更近。参考参考二值网络，围绕STE的那些事儿二值化神经网络(BNN)综述" }, { "title": "模型压缩资料积累", "url": "/posts/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E8%B5%84%E6%96%99%E7%A7%AF%E7%B4%AF/", "categories": "专业积累, 模型压缩", "tags": "", "date": "2021-02-27 22:41:00 +0800", "snippet": "深度学习的模型压缩可能就是自己硕士阶段的研究课题啦，这篇博客记录在学习过程中遇到的比较好的资料，作为积累和备忘。github awesome collections htqin/awesome-model-quantization A list of papers, docs, codes about model quantization. This repo is aimed to provide the info for model quantization research. csyhhu/Awesome-Deep-Neural-Network-Compression Summary, Code for Deep Neural Network Quantization chester256/Model-Compression-Papers Papers for deep neural network compression and acceleration juliagusak/model-compression-and-acceleration-progress he-y/Awesome-Pruning A curated list of neural network pruning resources. FLHonker/Awesome-Knowledge-Distillation Awesome Knowledge-Distillation. 分类整理的知识蒸馏paper(2014-2021)。 github project repo IntelLabs/distiller Neural Network Distiller by Intel AI Lab: a Python package for neural network compression research. j-marple-dev/model_compression lucamocerino/Binary-Neural-Networks-PyTorch-1.0 BNNs (XNOR, BNN and DoReFa) implementation for PyTorch 1.0+ jiecaoyu/XNOR-Net-PyTorch 666DZY666/micronet 可供模型压缩加速入门参考 JDAI-CV/dabnn dabnn is an accelerated binary neural networks inference framework for mobile platform flame/how-to-optimize-gemm A step-by-step gemm optimization tutorial Attractive blogs量化 :star: 二值化神经网络(BNN)综述 - Ironboy的文章 - 知乎 神经网络模型量化方法简介 模型压缩算法汇总 Making Neural Nets Work With Low Precision Compression and Acceleration of High-dimensional Neural Networks distiller - 非对称量化和对称量化的理论推导 NVIDIA Deep Learning Performance Documentation ——英伟达yyds! 闲话模型压缩之量化（Quantization）篇 Anatomy of a High-Speed Convolution剪枝 论文总结 - 模型剪枝 Model Pruning 闲话模型压缩之网络剪枝（Network Pruning）篇 https://www.yuque.com/yahei/hey-yahei/opsummary.mxnet) https://segmentfault.com/a/1190000020993594) BiliBili量化 神经网络量化 BiliBili code Google Developer Days - 模型优化与量化 :star: 哈佛大学在读博士董鑫：模型量化—更小更快更强 " }, { "title": "资料杂记", "url": "/posts/%E8%B5%84%E6%96%99%E6%9D%82%E8%AE%B0/", "categories": "有趣/有用的资料, 资料杂记", "tags": "github", "date": "2021-02-20 14:31:00 +0800", "snippet": "github repo lufficc/SSD 一个SSD算法的实现 jbhuang0604/awesome-computer-vision A curated list of awesome computer vision resources AgaMiko/data-augmentation-review List of useful data augmentation resources. You will find here some not common techniques, libraries, links to github repos, papers and others. aleju/imgaug Image augmentation for machine learning experiments. Paperspace/DataAugmentationForObjectDetection Data Augmentation For Object Detection https://github.com/amusi Mnist数据集链接可以用这里 替代已经失效的lecun网址 ysh329/deep-learning-model-convertor The convertor/conversion of deep learning models for different deep learning frameworks/softwares. Mathematics for Machine Learning We wrote a book on Mathematics for Machine Learning that motivates people to learn mathematical concepts. The book is not intended to cover advanced machine learning techniques because there are already plenty of books doing this. Instead, we aim to provide the necessary mathematical skills to read those other books. FLHonker/ZAQ-code CVPR 2021 : Zero-shot Adversarial Quantization (ZAQ)，包含一些量化的自主实现，可参考理解原理。 Openpose作者本者开的Openpose repo 相比于CMU-Lab的这个相比更偏Python jbhuang0604/awesome-tips A curated list of tips on various topics. Website stanford CS231n Convolutional Neural Networks for Visual Recognition ConvNetJS - Deep Learning in your browser Web入门 英伟达显卡计算能力(Compute Capability)查询与支持的运算类型查询 CVonline: Image Databases a collated list of image and video databases that people have found useful for computer vision research and algorithm evaluation. 如何解决神经网络训练时loss不下降的问题 当出现loss不下降时，可以对照一下 The Missing Semester of Your CS Education知乎 如何确定自己的科研课题？ - 王树义的回答 神经网络训练中的梯度消失与梯度爆炸视频3Blue1Brown series 反向传播演算（第 3 季 • 第 4 集）Blog Graph Convolutional Networks (GCN) &amp;amp; Pooling Object Detection Paper collection Self-Supervised Representation Learning 台湾清华彭明辉教授的研究生手册(简体完全版) A Beginner’s Guide To Understanding Convolutional Neural Networks(这个博主写的文章挺不错的) 一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉 The Illustrated Transformer Do We Really Need Model Compression? Object Detection Series of Lil’Log: part1 part2 part3 part4 为什么你应该（从现在开始就）写博客 debug DNN model Checklist for debugging neural networks Debugging Neural Networks with PyTorch and W&amp;amp;B Using Gradients and Visualizations 一篇历时五年的心得体会：关于AI部署、关于选择方向、关于就业TEDs Julian Treasure: How to speak so that people want to listen Kai-Fu Lee: How AI can save our humanity Projects mediapipe：这也太牛:cow:了吧！环境配置 Windows下C++编译器下载与配置：MinGW C++ Download and Installation 更换pip源 基于Chrome获取Andriod端浏览器log：How to get the web console log in Chrome for Android" }, { "title": "关于Python装饰器", "url": "/posts/%E5%85%B3%E4%BA%8EPython%E8%A3%85%E9%A5%B0%E5%99%A8/", "categories": "专业积累, 编程积累", "tags": "python, 转载", "date": "2021-02-15 21:01:00 +0800", "snippet": " 本文的内容主要转载自runoob-Python 函数装饰器…的评论区笔记，原始出处不明，但写得确实非常好，因此摘录于此。引入讲Python装饰器前，我想先举个例子，虽有点污，但跟装饰器这个话题很贴切。每个人都有的内裤主要功能是用来遮羞，但是到了冬天它没法为我们防风御寒，咋办？我们想到的一个办法就是把内裤改造一下，让它变得更厚更长，这样一来，它不仅有遮羞功能，还能提供保暖，不过有个问题，这个内裤被我们改造成了长裤后，虽然还有遮羞功能，但本质上它不再是一条真正的内裤了。于是聪明的人们发明长裤，在不影响内裤的前提下，直接把长裤套在了内裤外面，这样内裤还是内裤，有了长裤后宝宝再也不冷了。装饰器就像我们这里说的长裤，在不影响内裤作用的前提下，给我们的身子提供了保暖的功效。谈装饰器前，还要先要明白一件事，Python中的函数和 Java、C++不太一样，Python中的函数可以像普通变量一样当做参数传递给另外一个函数，例如：def hello_world(): print(&quot;hello world!&quot;) hello_world() # output: &quot;hello world!&quot; # 我们可以将一个函数赋值给一个变量，比如greet = hello_world# 我们这里没有在使用小括号，因为我们并不是在调用hi函数, 而是在将它放在greet变量里头。我们尝试运行下这个greet() # output: &quot;hello world!&quot;# 再试试把函数作为参数传递给另一个函数def hi(func): func() hi(hello_world) # output: &quot;hello world!&quot;正式回到我们的主题。装饰器本质上是一个 Python 函数或类，它可以让其他函数或类在不需要做任何代码修改的前提下增加额外功能，装饰器的返回值也是一个函数/类对象。它经常用于有切面需求的场景，比如：插入日志、性能测试、事务处理、缓存、权限校验等场景，装饰器是解决这类问题的绝佳设计。有了装饰器，我们就可以抽离出大量与函数功能本身无关的雷同代码到装饰器中并继续重用。概括的讲，装饰器的作用就是为已经存在的对象添加额外的功能。先来看一个简单例子，虽然实际代码可能比这复杂很多：def foo(): print(&#39;i am foo&#39;)现在有一个新的需求，希望可以记录下函数的执行日志，于是在代码中添加日志代码：def foo(): print(&#39;i am foo&#39;) print(&quot;foo is running&quot;)如果函数 bar()、bar2() 也有类似的需求，怎么做？再写一个 logging 在 bar 函数里？这样就造成大量雷同的代码，为了减少重复写代码，我们可以这样做，重新定义一个新的函数：专门处理日志 ，日志处理完之后再执行真正的业务代码def use_logging(func): print(&quot;%s is running&quot; % func.__name__) func()def foo(): print(&#39;i am foo&#39;)use_logging(foo)这样做逻辑上是没问题的，功能是实现了，但是我们调用的时候不再是调用真正的业务逻辑 foo 函数，而是换成了 use_logging 函数，这就破坏了原有的代码结构， 现在我们不得不每次都要把原来的那个 foo 函数作为参数传递给 use_logging 函数，那么有没有更好的方式的呢？当然有，答案就是装饰器。简单装饰器def use_logging(func): def wrapper(): print(&quot;%s is running&quot; % func.__name__) return func() # 把 foo 当做参数传递进来时，执行func()就相当于执行foo() return wrapperdef foo(): print(&#39;i am foo&#39;)foo = use_logging(foo) # 因为装饰器 use_logging(foo) 返回的时函数对象 wrapper，这条语句相当于 foo = wrapperfoo() # 执行foo()就相当于执行 wrapper()use_logging 就是一个装饰器，它一个普通的函数，它把执行真正业务逻辑的函数 func 包裹在其中，看起来像 foo 被 use_logging 装饰了一样，use_logging 返回的也是一个函数，这个函数的名字叫 wrapper。@ 语法糖如果你接触 Python 有一段时间了的话，想必你对 @ 符号一定不陌生了。没错 ，@ 符号就是装饰器的语法糖，它放在函数开始定义的地方，这样就可以省略最后一步再次赋值的操作。def use_logging(func): def wrapper(): print(&quot;%s is running&quot; % func.__name__) return func() return wrapper@use_loggingdef foo(): print(&quot;i am foo&quot;)foo()# 等同于之前的`foo = use_logging(foo)`如上所示，有了 @ ，我们就可以省去foo = use_logging(foo)这一句了，直接调用 foo() 即可得到想要的结果。你们看到了没有，foo() 函数不需要做任何修改，只需在定义的地方加上装饰器，调用的时候还是和以前一样，如果我们有其他的类似函数，我们可以继续调用装饰器来修饰函数，而不用重复修改函数或者增加新的封装。这样，我们就提高了程序的可重复利用性，并增加了程序的可读性。装饰器在 Python 使用如此方便都要归因于 Python 的函数能像普通的对象一样能作为参数传递给其他函数，可以被赋值给其他变量，可以作为返回值，可以被定义在另外一个函数内。*args、**kwargs可能有人问，如果我的业务逻辑函数 foo 需要参数怎么办？比如：def foo(name): print(&quot;i am %s&quot; % name)我们可以在定义 wrapper 函数的时候指定参数：def wrapper(name): print(&quot;%s is running&quot; % func.__name__) return func(name) return wrapper这样 foo 函数定义的参数就可以定义在 wrapper 函数中。这时，又有人要问了，如果 foo 函数接收两个参数呢？三个参数呢？更有甚者，我可能传很多个。当装饰器不知道 foo 到底有多少个参数时，我们可以用 *args 来代替：def wrapper(*args): print(&quot;%s is running&quot; % func.__name__) return func(*args) return wrapper如此一来，甭管 foo 定义了多少个参数，我都可以完整地传递到 func 中去。这样就不影响 foo 的业务逻辑了。这时还有读者会问，如果 foo 函数还定义了一些关键字参数呢？比如：def foo(name, age=None, height=None): print(&quot;I am %s, age %s, height %s&quot; % (name, age, height))这时，你就可以把 wrapper 函数指定关键字函数：def wrapper(*args, **kwargs): # args是一个元组，kwargs一个字典 print(&quot;%s is running&quot; % func.__name__) return func(*args, **kwargs) return wrapper *args负责接收不定数目的参数，**kwargs负责接收关键字类型的参数。这时整体的函数就成了def use_logging(func): def wrapper(*args, **kwargs): # args是一个元组，kwargs一个字典 print(&quot;%s is running&quot; % func.__name__) return func(*args, **kwargs) return wrapper # equal to foo = use_logging(foo)@use_loggingdef foo(*args, **kwargs): print(&quot;i am foo&quot;) print(args, kwargs)带参数的装饰器装饰器还有更大的灵活性，例如带参数的装饰器，在上面的装饰器调用中，该装饰器接收唯一的参数就是执行业务的函数 foo 。装饰器的语法允许我们在调用时，提供其它参数，比如@decorator(a)。这样，就为装饰器的编写和使用提供了更大的灵活性。比如，我们可以在装饰器中指定日志的等级，因为不同业务函数可能需要的日志级别是不一样的。def use_logging(level): def decorator(func): def wrapper(*args, **kwargs): if level == &quot;warn&quot;: print(&quot;%s is running&quot; % func.__name__) elif level == &quot;info&quot;: print(&quot;%s is running&quot; % func.__name__) return func(*args) return wrapper return decorator@use_logging(level=&quot;warn&quot;)def foo(name=&#39;foo&#39;): print(&quot;i am %s&quot; % name)foo()上面的 use_logging 是允许带参数的装饰器。它实际上是对原有装饰器的一个函数封装，并返回一个装饰器。我们可以将它理解为一个含有参数的闭包。当我 们使用@use_logging(level=&quot;warn&quot;)调用的时候，Python 能够发现这一层的封装，并把参数传递到装饰器的环境中。@use_logging(level=”warn”) 等价于 @decorator类装饰器没错，装饰器不仅可以是函数，还可以是类，相比函数装饰器，类装饰器具有灵活度大、高内聚、封装性等优点。使用类装饰器主要依靠类的__call__方法，当使用 @ 形式将装饰器附加到函数上时，就会调用此方法。class Foo(object): def __init__(self, func): self._func = func def __call__(self): print (&#39;class decorator runing&#39;) self._func() print (&#39;class decorator ending&#39;)@Foodef bar(): print (&#39;bar&#39;)bar() __call__()使得类实例可当做函数调用。如Foo(bar)()将会触发__call__()函数（Foo(bar)实例化Foo对象，Foo(bar)()将该对象做函数调用）。" }, { "title": "频域角度理解深度学习 F-principle", "url": "/posts/%E9%A2%91%E5%9F%9F%E8%A7%92%E5%BA%A6%E7%90%86%E8%A7%A3%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-F-principle/", "categories": "专业积累, 基础知识", "tags": "", "date": "2021-02-13 22:00:00 +0800", "snippet": "\\(F-Principle\\)定理：DNN倾向于按从低频到高频的顺序来拟合训练数据。实验Spatial DomainRed: the target function;Blue: DNN output.Abscissa: input;Ordinate: output.Fourier DomainRed: FFT of the target function;Blue: FFT of DNN output.Abscissa: frequency;Ordinate: amplitude.从上述实验(图源)可以看出，模型的拟合是有顺序的，首先从低频开始，逐渐转移至更高频率的拟合。启发从\\(F-principle\\)角度来理解过拟合现象：神经网络的泛化性能来源于它在训练过程，会更多关注低频分量。随着训练的进行，模型对训练集的拟合逐渐转化至高频成分，即对高频成分的拟合越来越好。但高频成分往往是噪声信号，因此导致了模型的泛化能力减弱。因此，提前停止训练（early-stopping）就能在实践中提高 DNN 的泛化能力。early stopping的必要性参考知乎-如何从频域的角度解释 CNN（卷积神经网络）？Bilibli-许志钦-数学学院本科课程：统计计算与机器学习3 Frequency PrincipleF-principle主页" }, { "title": "关于Batch Normalization", "url": "/posts/%E5%85%B3%E4%BA%8EBatch-Normalization/", "categories": "专业积累, 基础知识", "tags": "", "date": "2021-02-09 14:09:00 +0800", "snippet": "Batch-Normalization (BN) 通过使用当前batch数据的均值mean和方差variance对它们进行标准化处理，可以使得模型的训练更加快速和稳定。实践中会把BN一般放在非线性激活函数的上一层或者下一层。原理训练时BN之所以称之为batch normalization，就是因为normalization是沿batch_size维度进行的。设某一个神经元对一个batch内的\\(n\\)个样本的输出为分别为\\(Z^{(i)}, i=1, \\cdots, n\\)，BN层通过计算\\[\\mu = \\frac{1}{n}\\sum_i Z^{(i)}, \\ \\ \\ \\delta = \\frac{1}{n}\\sum_i (Z^{(i)} - \\mu) \\tag{1, 2}\\]得到它们的均值 \\(\\mu\\) 和方差 \\(\\delta\\). 然后通过\\[Z_{norm}^{(i)} = \\frac{Z^{(i)} - \\mu}{\\sqrt{\\delta^2 - \\epsilon}} \\tag{3}\\]将原来该神经元的输出（可能是任意分布）转化到均值为0，方差为1的标准正态分布上。如图3所示。图3 batch_size为b, 3个神经元输出的示例。对于每个神经元，在使用BN前输出(沿batch_size维度)分布各异，经过BN操作后均服从相同的分布——正态分布最后，再对\\(Z_{norm}^{(i)}\\)做一个线性变换\\[\\hat{Z} = \\gamma \\times Z_{norm}^{(i)} + \\beta \\tag{4}\\]\\(\\gamma\\) 和 \\(\\beta\\) 是可学习的参数，可以在正态分布的基础上调整，选择其最优的正态分布。上面的解释是基于神经元neuron的输出展开的，但是对于2D CNN中卷积得到的特征图，batch normalization是如何作用的呢？图4 2D CNN下的BN操作方式示意如图4，就是将某一个channel对应的batchsize个维度为(H, W)的tensor求解出一对均值方差，对这batchsize个(H, W)的tensor，其每一个pixel value减去该均值并除以该方差。参考 Q: bn 不是对batch维度做归一化嘛，为什么h,w维度也要做？ A: 因为CNN中卷积核是共享的，所以一张特征图应该被看作一个”神经元”的输出，因此归一化的时候N,H,W应该放在一起归一化。参考(评论区)注意，BN层操作是channel-wise的：一个channel对应一对均值和方差，一对\\(\\gamma\\) 和 \\(\\beta\\) 。这也是在Pytorch中定义BatchNorm2d层时，需要传入的是channel_num的缘故。参考和参考测试时训练时，BN层可以为每一个batch输入数据计算均值和方差，但是测试时，BN层使用的均值和方差不是根据输入样本计算，而是使用训练过程中计算得到的值，直接从(3)式开始计算。可以参考这个demo code.效果图5 x n代表以之前最优学习率的n倍进行训练可以看到，使用BN可以容忍更高的学习率，更快更好地收敛。:raising_hand_woman:QAQ：经常见到的mean = [0.485, 0.456, 0=406] ， std = [0.229, 0.224, 0.225]是什么？A：前面的(0.485,0.456,0.406)表示均值，分别对应的是RGB三个通道；后面的(0.229,0.224,0.225)则表示的是标准差，这些值是在ImageNet数据集计算出来的，所以很多人都使用它们。不过在有了BN之后，它们已经不必要了。Q：对于RGB图而言，均值不应该是接近于128吗？为什么会是接近于0.5的数呢？A： 这是因为使用了pytorch的transforms.ToTensor()对图片进行了归一化处理（另外对于PIL Image or numpy.ndarray类型的输入，还将尺寸由 (H x W x C) 转化为了 (C x H x W)）。如transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])Q：上面说BN层在训练和测试时使用方法不同，那么模型如何区分自己是在训练还是在测试呢？A：使用model.train()或者model.train(True)将model设为训练状态；使用model.eval()或者model.train(False)将model设为测试状态。当状态变化时，目前只有dropout和batchnorm两种层会受到影响(参考)。batchnorm层在不同状态下的表现如前所述；对于dropout层，当处在测试状态时，该层输出值等于输入值(identity map)，相当于disabled。 Tips: dropout(p)在训练时有一个scaled by factor\\(\\frac{1}{1-p}\\)操作，目的是使得某一神经元输出在测试时的期望=训练时的期望，更进一步解释可参考这里Q：Batch Normalization的一些局限？A：因为测试时BN层使用的均值和方差是训练过程中计算得到的值，因此当测试集和训练集数据分布差别较大时，会得到不好的测试效果。Q：BN层应放在非线性激活函数前面还是后面？A：BN作者本意是为了通过BN层，使得任何的参数值，都可以用所期望的分布(应该就是指正态分布)来生成激活值，因此将BN层放在了非线性激活函数前面一层。后来很多网络结构，比如ResNet，mobilenet-v2也都沿用了这一准则。不过也有工作表示将BN层放在了非线性激活函数后面一层会取得更好的效果，但是没有给出有力的解释。reddit上对于这一问题有个激烈的讨论。Q：Group Normlization?A：参考Batch normalization in 3 levels of understandingpytorch 计算图像数据集的均值和标准差" }, { "title": "Pytorch系统学习", "url": "/posts/Pytorch%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0/", "categories": "实践, 框架学习", "tags": "pytorch", "date": "2021-02-08 17:06:00 +0800", "snippet": "读取数据参考 官方示例 和 这个github repo (recommanded)Dataset类需要继承torch.utils.data.Dataset 且重写__len__和__getitem__。__len__返回数据集的大小，__getitem__根据索引值\\(i\\)从数据集中返回第\\(i​\\)个样本。一个简单的示例import osfrom torch.utils.data.dataset import Datasetclass CustomDataset(Dataset): def __init__(self, root_dir): self.data_list = [os.path.join(root_dir, data_name) for data_name in os.listdir(root_dir)] def __getitem__(self, index): data_path = self.data_list[index] data = READ_OP(data_path) # data reading return data def __len__(self): return len(self.data_list)可以用data = CustomDataset.__getitem__(99) 返回第99个样本。更进一步的，CustomDataset可以这样来读取样本for i in range(len(CustomDataset)): sample = CustomDataset[i]使用transform示例import osfrom torch.utils.data.dataset import Datasetfrom torchvision import transforms class CustomDataset(Dataset): def __init__(self, root_dir, transform): self.data_list = [os.path.join(root_dir, data_name) for data_name in os.listdir(root_path)] self.transform = transform def __getitem__(self, index): data_path = self.data_list[index] data = READ_OP(data_path) # data reading # apply transform if self.transform is not None: data = self.transform(data) return data def __len__(self): return len(self.data_list) if __name__ == &#39;__main__&#39;: transform = transforms.Compose([ transforms.ToPILImage(), # 使用RandomCrop等函数，要求变量为PIL transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)) ]) # apply the transformations to the dataset dataset = CustomDataset(root_dir, transform)使用DataLoaderCustomDataset可以按索引index读取样本，但是不便于 批量读取数据(Batching the data) 打乱数据(Shuffling the data) 多线程读取数据(Loading the data in parallel using multiprocessing workers)使用DataLoader对其进一步封装即可赋予这些能力from torch.utils.data import DataLoaderdataloader = DataLoader(transformedDataset, batch_size=4, shuffle=True, num_workers=4)for data in dataloader: ...搭建网络继承nn.Module，参考官方示例示例import torchimport torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Module): def __init__(self): super(Net, self).__init__() # 1 input image channel, 6 output channels, 5x5 square convolution # kernel self.conv1 = nn.Conv2d(1, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) # an affine operation: y = Wx + b self.fc1 = nn.Linear(16 * 5 * 5, 120) # 5*5 from image dimension self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # Max pooling over a (2, 2) window x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # If the size is a square, you can specify with a single number x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return xnet = Net()print(net):warning:: torch.nn只支持batch数据，不支持单帧数据。比如 nn.Conv2d 的输入应为一个4维Tensor: Batch_size x Channels x Height x Width. 如果要输入单帧的数据，应该使用input.unsqueeze(0)来转化成一个“伪”batch数据。模型训练计算loss，反向传播，参数更新。参考官方示例import torch.optim as optimimport torch.nn as nn# create your optimizeroptimizer = optim.SGD(net.parameters(), lr=0.01)criterion = nn.CrossEntropyLoss()# in your training loop:optimizer.zero_grad() # zero the gradient buffersoutput = model(input)loss = criterion(output, target)loss.backward()optimizer.step() # Do the update优化器使用参考官方示例import torch.optimoptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)optimizer = optim.Adam([var1, var2], lr=0.0001)固定部分参数model.fc.requires_grad = Falseoptimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=0.1)如果后续需要再对这些层进行训练，则可以通过model.fc.requires_grad = Trueoptimizer.add_param_group({&#39;params&#39;: net.fc2.parameters()})为不同参数设定不同的学习规则为优化器传入一个元素类型为字典的迭代器。每个字典元素必须包含params键值以指定待优化的参数。optim.SGD([ {&#39;params&#39;: model.base.parameters()}, {&#39;params&#39;: model.classifier.parameters(), &#39;lr&#39;: 1e-3} ], lr=1e-2, momentum=0.9)这代表model.base参数使用1e-2学习率，model.classifier参数的学习率由于重写，会使用1e-3；momentum = 0.9为二者公用。那么问题来了，怎么知道我们的模型有哪些模块，对应哪些参数呢？可以到模型的定义处看一下，或者也可以print(model)输出看一下。下面以torchvision自带的ResNet18为例，展示一下如何让使得最后的分类fc层和网络前端参数使用不同的学习率。参考CSDN-Pytorch中，动态调整学习率、不同层设置不同学习率和固定某些层训练的方法from torch import nn, optimimport torchvisiondef group_params_v1(model): cls_params = [] for name, mod in model.named_modules(): if type(mod) == nn.Linear: cls_params += mod.parameters() # else: x ！注意model.modules()是递归遍历的，故不能直接使用else分支来获取非fc层的参数 # filter(function, iterable): 返回由&amp;lt;iterable&amp;gt;中符合条件&amp;lt;function&amp;gt;的元素组成的新列表 # list(map(id, cls_params)): 返回存放cls_params参数的内存地址 base_params = filter(lambda x:id(x) not in list(map(id, cls_params)), model.parameters()) return base_params, cls_paramsdef group_params_v2(model): params_dict = dict(model.named_parameters()) cls_param_names = { name for name, param in model.named_parameters() if &quot;fc&quot; in name and param.requires_grad } base_param_names = params_dict.keys() - cls_param_names # print(cls_param_names, base_param_names) base_params = (params_dict[n] for n in sorted(base_param_names)) cls_params = (params_dict[n] for n in sorted(cls_param_names)) return base_params, cls_paramsmodel = torchvision.models.resnet18(pretrained=True)print(model) # 可以看到最后一层为fc层: (fc): Linear(in_features=512, out_features=1000, bias=True)# base_params, cls_params = group_params_v1(model)base_params, cls_params = group_params_v2(model)optimizer = optim.SGD( [ {&#39;params&#39;: base_params, &#39;lr&#39;: 0.1}, {&#39;params&#39;: cls_params, &#39;lr&#39;: 0.01} ], momentum=0.9)使用scheduler以常用的StepLR为例，其定义及使用方法为from torch.optim import lr_scheduler# StepLR: https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.StepLRscheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)# MultiStepLR: https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.MultiStepLRscheduler = lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1)for epoch in range(100): train(...) validate(...) scheduler.step() # Learning rate scheduling should be applied after optimizer’s update获取优化器信息参考# === 总览 === #print(optimizer)# === output === #SGD (Parameter Group 0 dampening: 0 lr: 0.01 momentum: 0.9 nesterov: False weight_decay: 0)# 获取学习率optimizer.param_groups[0][&#39;lr&#39;]使用GPU训练模型使用单GPU训练模型参考官方示例device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)# print(device)model.to(device) # or model.cuda()input = input.to(device) # or input = input.cuda()使用多GPU分布式训练参考官方示例if torch.cuda.device_count() &amp;gt; 1: print(&quot;Let&#39;s use&quot;, torch.cuda.device_count(), &quot;GPUs!&quot;) # dim = 0 [30, xxx] -&amp;gt; [10, ...], [10, ...], [10, ...] on 3 GPUs model = nn.DataParallel(model)model.to(device)for data in data_loader: input = data.to(device) output = model(input) print(&quot;Outside: input size&quot;, input.size(), &quot;output_size&quot;, output.size())当分布式训练时，DataParallel自动将输入数据分配给多个GPU，每个GPU计算完成后再进行汇总输出。有多块卡，仅指定使用其中部分卡（可见），可通过：CUDA_VISIBLE_DEVICES=0,1 python train.py使用visdom查看数据可参考visdom使用使用tensorboard查看数据可参考tensorboard使用模型保存与加载参考官方示例torch.save()and torch.load() use Python’s pickle by default.保存和加载 tensors&amp;gt;&amp;gt;&amp;gt; t = torch.tensor([1., 2.])&amp;gt;&amp;gt;&amp;gt; torch.save(t, &#39;tensor.pt&#39;) # save with a ‘.pt’ or ‘.pth’ extension&amp;gt;&amp;gt;&amp;gt; torch.load(&#39;tensor.pt&#39;)tensor([1., 2.])保存和加载多个tensors&amp;gt;&amp;gt;&amp;gt; d = {&#39;a&#39;: torch.tensor([1., 2.]), &#39;b&#39;: torch.tensor([3., 4.])}&amp;gt;&amp;gt;&amp;gt; torch.save(d, &#39;tensor_dict.pt&#39;)&amp;gt;&amp;gt;&amp;gt; torch.load(&#39;tensor_dict.pt&#39;){&#39;a&#39;: tensor([1., 2.]), &#39;b&#39;: tensor([3., 4.])}保存和加载模型参数一个module的state dict包含着 all of its parameters和persistent buffersbn = torch.nn.BatchNorm1d(3, track_running_stats=True)&amp;gt;&amp;gt;&amp;gt; bn.state_dict()OrderedDict([(&#39;weight&#39;, tensor([1., 1., 1.])), (&#39;bias&#39;, tensor([0., 0., 0.])), (&#39;running_mean&#39;, tensor([0., 0., 0.])), (&#39;running_var&#39;, tensor([1., 1., 1.])), (&#39;num_batches_tracked&#39;, tensor(0))])简单示例&amp;gt;&amp;gt;&amp;gt; torch.save(bn.state_dict(), &#39;bn.pt&#39;)&amp;gt;&amp;gt;&amp;gt; bn_state_dict = torch.load(&#39;bn.pt&#39;) # load state dict from its file&amp;gt;&amp;gt;&amp;gt; new_bn = torch.nn.BatchNorm1d(3, track_running_stats=True)&amp;gt;&amp;gt;&amp;gt; new_bn.load_state_dict(bn_state_dict) # restore the state dict to a new module &amp;lt;All keys matched successfully&amp;gt;custom modules# A module with two linear layers&amp;gt;&amp;gt;&amp;gt; class MyModule(torch.nn.Module): def __init__(self): super(MyModule, self).__init__() self.l0 = torch.nn.Linear(4, 2) self.l1 = torch.nn.Linear(2, 1) def forward(self, input): out0 = self.l0(input) out0_relu = torch.nn.functional.relu(out0) return self.l1(out0_relu)&amp;gt;&amp;gt;&amp;gt; m = MyModule()&amp;gt;&amp;gt;&amp;gt; m.state_dict()OrderedDict([(&#39;l0.weight&#39;, tensor([[ 0.1400, 0.4563, -0.0271, -0.4406], [-0.3289, 0.2827, 0.4588, 0.2031]])), (&#39;l0.bias&#39;, tensor([ 0.0300, -0.1316])), (&#39;l1.weight&#39;, tensor([[0.6533, 0.3413]])), (&#39;l1.bias&#39;, tensor([-0.1112]))])&amp;gt;&amp;gt;&amp;gt; torch.save(m.state_dict(), &#39;mymodule.pt&#39;)&amp;gt;&amp;gt;&amp;gt; m_state_dict = torch.load(&#39;mymodule.pt&#39;)&amp;gt;&amp;gt;&amp;gt; new_m = MyModule()&amp;gt;&amp;gt;&amp;gt; new_m.load_state_dict(m_state_dict)&amp;lt;All keys matched successfully&amp;gt;保存和加载一个通用的checkpoint参考知乎# savetorch.save({ &#39;epoch&#39;: epoch, &#39;model_state_dict&#39;: model.state_dict(), &#39;optimizer_state_dict&#39;: optimizer.state_dict(), &#39;loss&#39;: loss, ... }, PATH)# loadcheckpoint = torch.load(PATH)model.load_state_dict(checkpoint[&#39;model_state_dict&#39;])optimizer.load_state_dict(checkpoint[&#39;optimizer_state_dict&#39;])epoch = checkpoint[&#39;epoch&#39;]loss = checkpoint[&#39;loss&#39;]何时需要自己重载load_state_dict()?TODOPytorch碎碎念Pytorch的TensorTensor初始化data = [[1, 2],[3, 4]]x_data = torch.tensor(data)shape = (2, 3)rand_tensor = torch.rand(shape)ones_tensor = torch.ones(shape)zeros_tensor = torch.zeros(shape)Tensor和numpy array相互转化# from numpy array to tensortensor = torch.from_numpy(np_array)# from tensor to numpy arraynp_array = tensor.numpy()Pytorch使用GPU时正确测试时间当使用GPU运算时，在计时时须使用torch.cuda.synchronize()start = time.time()result = model(input)torch.cuda.synchronize()end = time.time()参考1, 参考2module.named_x()参考named_modules()，named_children()与named_parmameters()三者均返回一个literator。上demo：import torchimport torch.nn as nn class ToyModel(nn.Module): def __init__(self): super().__init__() self.fc1 = nn.Linear(3, 4) self.sequence = nn.Sequential( nn.Linear(5, 4), nn.ReLU(), )model = ToyModel() named_modules()递归返回所有的module：for name, mod in model.named_modules(): print(name, mod)&#39;&#39;&#39;out ToyModel( (fc1): Linear(in_features=3, out_features=4, bias=True) (sequence): Sequential( (0): Linear(in_features=5, out_features=4, bias=True) (1): ReLU() ))fc1 Linear(in_features=3, out_features=4, bias=True)sequence Sequential( (0): Linear(in_features=5, out_features=4, bias=True) (1): ReLU())sequence.0 Linear(in_features=5, out_features=4, bias=True)sequence.1 ReLU()&#39;&#39;&#39; named_children()返回第一代module (immediate children module)：for name, child in model.named_children(): print(name, child)&#39;&#39;&#39;outfc1 Linear(in_features=3, out_features=4, bias=True)sequence Sequential( (0): Linear(in_features=5, out_features=4, bias=True) (1): ReLU())&#39;&#39;&#39; named_parameters()返回所有module的parameter：for name, param in model.named_parameters(): print(name, param.shape)&#39;&#39;&#39;outfc1.weight torch.Size([4, 3])fc1.bias torch.Size([4])sequence.0.weight torch.Size([4, 5])sequence.0.bias torch.Size([4])&#39;&#39;&#39;常用代码段参考PyTorch常用代码段其他 从torch tensor中取值：torch_tensor.item() 注意.item()只能从单个的torch tensor中取值，如需要从torch tensor list中取值，可使用 [tensor.item() for tensor in tensor_list] DEFINING NEW AUTOGRAD FUNCTIONS 定义torch.autograd.Function的子类，自己定义某些操作，且定义反向求导函数 EXTENDING TORCHSCRIPT WITH CUSTOM C++ OPERATORS 获取数据类型详细信息 参考 浮点类型：torch.finfo(dtype)，如 print(torch.finfo(torch.float32))# finfo(resolution=1e-06, min=-3.40282e+38, max=3.40282e+38, eps=1.19209e-07, tiny=1.17549e-38, dtype=float32) 整型：torch.iinfo(dtype)，如 print(torch.iinfo(torch.int8))# iinfo(min=-128, max=127, dtype=int8) bit shift 负值移位是一种未定义的行为。在原生python中，会ValueError: negative shift count，但在Pytorch中无报错信息，但会产生预期之外的结果，当涉及移位运算时需要注意。如 a = torch.tensor(8) # dtype = torch.int64 (default) print(a &amp;gt;&amp;gt; 2) # 2 print(a &amp;lt;&amp;lt; -2) # 0 !!! 获取模型所在的device model = Model() # device = model.device() # WRONG. ==&amp;gt; &#39;Model&#39; object has no attribute &#39;device&#39;device = next(model.parameters()).device # &#39;cpu&#39; or &#39;cuda:0&#39; 踩坑 ValueError: can’t optimize a non-leaf Tensor 使用下面的codedevice = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;scales = nn.Parameter(torch.ones(in_channels), requires_grad=True)scales = scales.to(device)optimizer = torch.optim.SGD([scales], lr=0.01, momentum=0.9)会报错：ValueError: can&#39;t optimize a non-leaf Tensor正确的用法是：device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;equalize_scales = nn.Parameter(torch.ones(in_channels), requires_grad=True)scale_optimizer = torch.optim.SGD([equalize_scales], lr=0.01, momentum=0.9)scales = scales.to(device)" }, { "title": "三阶魔方复原方法", "url": "/posts/%E4%B8%89%E9%98%B6%E9%AD%94%E6%96%B9%E5%A4%8D%E5%8E%9F%E6%96%B9%E6%B3%95/", "categories": "有趣/有用的资料, 休闲", "tags": "休闲, 转载", "date": "2021-02-07 21:31:00 +0800", "snippet": " 以下内容来自知乎用户@小狼啊小狼的魔方那点事专栏。依次做完下面这八步，魔方就能复原啦🥳，要想熟练得需要多多练习。:one:【初级篇】（一）最简单的三阶魔方入门教程——魔方概述:two:【初级篇】（二）最简单的三阶魔方入门教程——底面十字:three:【初级篇】（三）最简单的三阶魔方入门教程——底层还原 左右手公式初遇：右手公式：URU&#39;R&#39;，左手公式：U&#39;L&#39;UL(右手公式的镜像公式):four:【初级篇】（四）最简单的三阶魔方入门教程——中层还原 左右手公式变形1：（左手公式+逆时针旋转90°+右手公式） or （右手公式+顺时针旋转90°+左手公式）:five:【初级篇】（五）最简单的三阶魔方入门教程——顶面十字 左右手公式变形2：F + 右手公式 + F’ 小黄点 -&amp;gt; 一字马 -&amp;gt; 小拐角 -&amp;gt; 十字架 :six:【初级篇】（六）最简单的三阶魔方入门教程——顶面还原 小鱼公式初相遇：右手小鱼公式：RUR&#39;URUUR&#39; 如果是基本小鱼情况，则直接运用左右手小鱼公式即可还原； 如果是非基本小鱼情况，则运用“2前4左”摆放规则将魔方摆到正确的朝向，然后使用右手小鱼公式转化为基本小鱼情况，即可使用左右手小鱼公式还原。 :seven:【初级篇】（七）最简单的三阶魔方入门教程——顶角还原 左右手公式变形3： U (U) R U’ (U’) R’ (F2) + U’ (U’) L’ U (U) L (F’2) 公式较长，可看作是左右手公式的基础上，顶层每次转两下，最后又多了转两次“F”的步骤（加粗的字体即为左右手公式） :eight:【初级篇】（八）最简单的三阶魔方入门教程——顶棱还原 小鱼公式变形2： 顺时针：右手小鱼+顺时针旋转90°+左手小鱼；逆时针：左手小鱼+逆时针旋转90°+右手小鱼" }, { "title": "markdown cheat sheet", "url": "/posts/markdown-cheat-sheet/", "categories": "有趣/有用的资料, cheat sheets", "tags": "cheat sheets, 转载", "date": "2021-02-05 22:31:00 +0800", "snippet": "镇楼：没有什么比Markdown官方教程更懂Markdown.Markdown emoji:point_right:See this github repo: ikatyang/emoji-cheat-sheetand this：rxaviers/7360908 可以先到这里搜关键词，找到图标，然后到这里直接搜图标得到markdown代码。 有的emoji因为各种原因在目前的markdown中不支持，可以在这里搜索，跳过markdown编码，直接复制表情。如这个:partying_face: 🥳其他经验积累为公式添加编号在公式末尾加入\\tag{id}，如 $$ x+y = z \\tag{1} $$可渲染为\\[x + y = z \\tag{1}\\]显示github项目star数目参考类似于显示一张图片（或者本质就是），方法是：![GitHub stars](https://img.shields.io/github/stars/&amp;lt;usr_name&amp;gt;/&amp;lt;repo_name&amp;gt;.svg?style=flat&amp;amp;label=Star)比如我的一个star少得可怜的github repo" }, { "title": "github无法加载头像和图片", "url": "/posts/%E8%A7%A3%E5%86%B3github%E6%97%A0%E6%B3%95%E5%8A%A0%E8%BD%BD%E5%A4%B4%E5%83%8F%E5%92%8C%E5%9B%BE%E7%89%87/", "categories": "实践, 实践杂记", "tags": "github, troubleshooting", "date": "2021-02-04 09:38:00 +0800", "snippet": "参考这一篇，2.3 修改 hosts。测试有效。 注：不保证长期有效，如果失效，请以原博客更新为准。Windows系统的hosts文件在C:\\Windows\\System32\\drivers\\etc\\hosts，打开后在文件末尾添加：# GitHub Start 140.82.113.3 github.com140.82.114.20 gist.github.com151.101.184.133 assets-cdn.github.com151.101.184.133 raw.githubusercontent.com199.232.28.133 raw.githubusercontent.com 151.101.184.133 gist.githubusercontent.com151.101.184.133 cloud.githubusercontent.com151.101.184.133 camo.githubusercontent.com199.232.96.133 avatars.githubusercontent.com151.101.184.133 avatars0.githubusercontent.com199.232.68.133 avatars0.githubusercontent.com199.232.28.133 avatars0.githubusercontent.com 199.232.28.133 avatars1.githubusercontent.com151.101.184.133 avatars1.githubusercontent.com151.101.108.133 avatars1.githubusercontent.com151.101.184.133 avatars2.githubusercontent.com199.232.28.133 avatars2.githubusercontent.com151.101.184.133 avatars3.githubusercontent.com199.232.68.133 avatars3.githubusercontent.com151.101.184.133 avatars4.githubusercontent.com199.232.68.133 avatars4.githubusercontent.com151.101.184.133 avatars5.githubusercontent.com199.232.68.133 avatars5.githubusercontent.com151.101.184.133 avatars6.githubusercontent.com199.232.68.133 avatars6.githubusercontent.com151.101.184.133 avatars7.githubusercontent.com199.232.68.133 avatars7.githubusercontent.com151.101.184.133 avatars8.githubusercontent.com199.232.68.133 avatars8.githubusercontent.com199.232.96.133 avatars9.githubusercontent.com# GitHub End如果因没有修改权限而无法保存，可以将hosts文件复制一份到桌面，修改之后，复制回去覆盖原文件。" }, { "title": "Jekyll博客搭建流程记录", "url": "/posts/jekell%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/", "categories": "Blog_tutorial", "tags": "getting started", "date": "2021-02-03 20:55:00 +0800", "snippet": "环境配置安装Ruby, Bundler.（看这里，更详细的看这里）挑选jekyll模板来这里挑选中意的模板。模板一般会有配置的说明，可以按照模板说明一步步来，把模板用起来。模板配置(以chirpy为例)chirpy模板页，点击homepage跳转到其对应的github页，按照readme配置即可。以下简要记录下实际配置chirpy的过程。 点击use the starter template 后新建一个&amp;lt;user_name&amp;gt;.github.io的仓库; clone下来; 修改_config.yml 在项目根目录下执行：bundle； push到远程： git add .git commit -m &#39;first commit&#39;git push origin main # main是当前分支名称 push后会在github的action看到进程，action进程成功完成后会在github分支中看到gh-pages分支； 注意这一步，到repo的setting里看看action权限是否够 参考 更改publishing source 进入 https://&amp;lt;user_name&amp;gt;.github.io，此时主页已初始配置完成了。模板使用添加blog 在项目根目录下增加_posts文件夹，在文件夹中按照添加markdown文件。 建议此处添加模板本身提供的几个示例markdown，它们也是日后使用该模板的重要参考。 push到远程 git add .git commit -m &#39;add blogs&#39;git push origin main 等待github action（action有时会有延迟，不会立即出现） 刷新https://&amp;lt;user_name&amp;gt;.github.io，查看网页端是否发生对应更改（可能存在延迟，或缓存问题，按ctrl + f5清空缓存。） 本地调试在项目根目录下执行jekyll s修改对应文件后，刷新页面可即时看到更改。太方便啦。 若本地调试，则需要保证路径名中无中文！否则本地预览会报错（尽管远程后可正常显示） 文件名中含有中文，则可能无法正常访问，需要做以下修改: 参考在blogs中添加图片在项目根目录下新建assets/img文件夹，在文件夹中放置图片文件（或包含图片文件的文件夹），在blog中即通过图片文件路径索引图片，如![](/assets/img/20210209/unBN.jpeg)" }, { "title": "Ubuntu重装系统后配置流程记录", "url": "/posts/ubuntu%E9%87%8D%E8%A3%85%E7%B3%BB%E7%BB%9F%E8%AE%B0%E5%BD%95/", "categories": "实践, 环境配置", "tags": "ubuntu", "date": "2021-01-28 14:53:00 +0800", "snippet": "下载安装包以Ubuntu 18.04为例，进入清华镜像源，选择：ubuntu-18.04.5-desktop-amd64.iso下载。制作系统盘与重启安装参考制作系统盘使用UltraISO。 打开UltraISO后，点击上边栏文件-打开，选择上面下载好的Ubuntu安装镜像文件。 点击上边栏启动-写入硬盘镜像，在跳出的窗口确认写入的硬盘驱动器（如要写入的U盘），点击写入后等待完成即可。重启安装重启，选择“启动入口”。具体操作为在开机时按下(参考)： Asus(华硕):【F8】鍵 Acer(宏碁):【F12】鍵 DELL(戴尔):【F12】鍵 GigaByte(技嘉):【F12】鍵 HP(惠普):【F9】鍵 MSI(微星):【F11】鍵在弹出的界面选择USB UEFI后进入安装界面后，根据提示完成安装即可。重要环境配置配置系统可远程登陆服务端(配置ssh)注意：按下面这个操作配置后，客户端可能仍连不上，可尝试重启一下服务端。ssh、xrdp远程连接Ubuntu服务器设置客户端 最简洁办法：使用MobaXterm 日常使用：VS code VS code使用中的 VS code Remote ssh配置 安装英伟达驱动与cudaUbuntu下nvidia driver和cuda版本管理安装与配置Anaconda可参考Anaconda使用其他右键新增新建文档 18.04初始右键的新建文档么有了，可以在主目录下的模板文件夹下放一个文本文件，这样右键才可以新建文档。参考安装搜狗输入法默认系统不支持中文输入法，需要自行安装新的输入法。此处以搜狗输入法为例。 官网下载安装包。 按照指南安装。 从系统输入法切换至搜狗输入法：Ctrl + space输入法中英文切换：shift 安装Chrome谷歌访问助手参考（本页面提供的下载链接已失效）参考简书-Win10和Ubuntu16.04双系统安装详解" }, { "title": "一个论文笔记框架（Markdown格式）", "url": "/posts/%E4%B8%80%E4%B8%AA%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E6%A1%86%E6%9E%B6-Markdown%E6%A0%BC%E5%BC%8F/", "categories": "专业积累, 杂记", "tags": "paper reading", "date": "2021-01-25 14:32:00 +0800", "snippet": "# Title## Summary写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段。注：写文章summary切记需要通过自己的思考，用自己的语言描述。忌讳直接Ctrl + c原文。## Research Objective(s)作者的研究目标是什么？## Background / Problem Statement研究的背景以及问题陈述：作者需要解决的问题是什么？## Method(s)作者解决问题的方法/算法是什么？是否基于前人的方法？基于了哪些？## Evaluation作者如何评估自己的方法？实验的setup是什么样的？感兴趣实验数据和结果有哪些？有没有问题或者可以借鉴的地方？## Conclusion作者给出了哪些结论？哪些是strong conclusions, 哪些又是weak的conclusions（即作者并没有通过实验提供evidence，只在discussion中提到；或实验的数据并没有给出充分的evidence）?## Notes(optional) 不在以上列表中，但需要特别记录的笔记。## References(optional) 列出相关性高的文献，以便之后可以继续track下去。参考：知乎”有好用的文献笔记工具吗？” 戴晓天的回答" }, { "title": "Docker学习", "url": "/posts/Docker%E5%AD%A6%E4%B9%A0/", "categories": "实践, 硬技能", "tags": "docker", "date": "2021-01-20 21:55:00 +0800", "snippet": "docker原理docker原理框架生成镜像的两种方式 通过Dockerfile构建。Dockerfile是一个用来构建镜像的文本文件，包含了一条条构建镜像所需的指令和说明，可参考Dockerfile文件详解。通过docker build命令来从Dockerfile构建镜像，可参照Docker build 命令。 docker build [OPTIONS] PATH | URL | - 其中OPTIONS可选，PATH/URL/-选其一，指定源。示例 $ docker build . # 在当前路径下寻找Dockerfile来构建$ docker build -t name:tag . # 在当前路径下寻找Dockerfile来构建，并为构建的镜像设定名字name及标签tag 从Docker Hub上直接拉取 在Docker Hub上查找已有的镜像，然后拉取到本地：docker pull &amp;lt;用户名/镜像名&amp;gt; 配置docker 安装docker：官方文档 将仓库网址改为国内的镜像站 打开/etc/default/docker文件（需要sudo权限），在文件的底部加上一行 DOCKER_OPTS=&quot;--registry-mirror=https://registry.docker-cn.com&quot; 然后重启Docker服务 $ sudo service docker restart 避免每次命令都输入sudo(官方文档): # 1. Create the docker group. sudo groupadd docker # 2. Add your user to the docker group. sudo usermod -aG docker $USER # 3. Log out and log back in so that your group membership is re-evaluated. 安装nvidia-docker：官方文档管理镜像与容器镜像(image) 列出本机的所有 image 文件：docker image ls 删除 image 文件：docker rmi &amp;lt;imageName/ID&amp;gt;容器(container) 列出本机正在运行的容器：docker ps 列出本机所有容器，包括终止运行的容器： docker ps -a 终止容器运行：docker container stop &amp;lt;containerID&amp;gt; 删除容器文件：docker rm &amp;lt;containerName/ID&amp;gt; 导出容器：docker export &amp;lt;OPTIONS&amp;gt; CONTAINER 导入容器：docker import &amp;lt;OPTIONS&amp;gt; file|URL|- [REPOSITORY[:TAG]]运行镜像 对于需要使用GPU的程序，需要使用nvidia-docker run通过docker run命令来运行镜像，可参考docker rundocker run [OPTIONS] IMAGE [COMMAND] [ARG...]示例1 - 基础docker run --name &amp;lt;container_name&amp;gt; -it &amp;lt;image_name&amp;gt; # 使用&amp;lt;image_name&amp;gt;镜像来生成一个容器，并为其命名为&amp;lt;image_name&amp;gt;# -i, or --interactive: Keep STDIN open even if not attached# -t, or --tty: Allocate a pseudo-TTY# The -it instructs Docker to allocate a pseudo-TTY connected to the container’s stdin; creating an interactive bash shell in the container示例2 - Mount volumedocker run --name &amp;lt;container_name&amp;gt; -v host_dir:docker_dir -it &amp;lt;image_name&amp;gt; # 将本地文件夹host_dir&quot;挂载到&quot;容器的docker_dir下比如docker run --name &amp;lt;container_name&amp;gt; -v ${PWD}:/work -it &amp;lt;image_name&amp;gt; # 将当前路径挂载在容器的/work目录下# 进入生成的container后ls # 即可在container中看到${PWD}中文件其他 修改 Docker 的默认存储路径参考网上真是好多docker的优秀教程啊。Docker 入门教程 - 阮一峰的网络日志Docker+VSCode配置属于自己的炼丹炉Docker Volume入门用法详解Docker卷(volume)的使用Docker 实践简明指南Docker入门-简书" }, { "title": "Ubuntu文件系统_硬盘挂载与卸载", "url": "/posts/Ubuntu%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F_%E7%A1%AC%E7%9B%98%E6%8C%82%E8%BD%BD%E4%B8%8E%E5%8D%B8%E8%BD%BD/", "categories": "实践, 环境配置", "tags": "ubuntu, 文件系统", "date": "2021-01-16 16:11:00 +0800", "snippet": "动机把文件系统的挂载相关知识学习下。几个有用的工具命令 列出系统中所有磁盘列表（包括未挂载的）： lsblk -f lsblk助记：老师不离开 示例 $ lsblk -f NAME FSTYPE LABEL UUID MOUNTPOINTsda ├─sda1 ntfs 系统保留 585A8E625A8E3D2E └─sda2 vfat wdisk 9111-7321 /media/lab301/wdisk... 其中NAME：设备文件名（省略\\dev等先导目录），FSTYPE：文件系统类型；UUID：全局唯一标识符(Universally uniue indentifier)，Linux会为系统内所有的设备都给予一个独一无二的标识符，这个标识符可以用来挂载或是使用这个设备；MOUNTPOINT：挂载点。 lsblk也可单独使用，列出其他一些信息。 示例 $ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 232.9G 0 disk ├─sda1 8:1 0 100M 0 part └─sda2 8:2 0 232.8G 0 part /media/lab301/wdisk... 其中RM：是否为可卸载设备，SIZE：容量，RO：是否为只读设备，TYPE：是磁盘disk或分区part等， 其他可查看磁盘信息的命令：sudo fdisk -l，df -h 查看某个文件夹的实际所在设备与挂载点：df &amp;lt;path/to/dir&amp;gt;文件系统的挂载与卸载挂载文件系统要链接到目录树上才能被我们使用。将文件系统与目录树结合的操作称为挂载。挂载点一定是目录，该目录为进入该文件系统（磁盘分区）的入口。挂载前有几点需要明确： 单一文件系统不应该被重复挂载在不同的挂载点（目录）中； 单一目录不应该挂载多个系统； 要作为挂载点的目录，理论上应该是空目录才对。否则会导致该目录下其他文件暂时性地消失（卸载后会重新出现）。挂载文件系统使用mount命令。它的一些简单操作示例如下：$ mount -a # 依照配置文件etc/fstab的数据将所有未挂载的磁盘都挂载上来$ mount # 当前挂载信息$ mount &amp;lt;NAME&amp;gt; or &amp;lt;UUID&amp;gt; or &amp;lt;LABEl&amp;gt; &amp;lt;MOUNTPOINT&amp;gt; # 通过&amp;lt;NAME&amp;gt; or &amp;lt;UUID&amp;gt; or &amp;lt;LABEl&amp;gt;索引到某一文件系统，将其挂载到MOUNTPOINT示例STEP1：创建挂载点 就是新建一个空目录mkdir ~/dirSTEP2：挂载 将sda2挂载到~/wdir。注意前面要加上/devmount /dev/sdb2 ~/dirSTEP3：查看是否挂载成功df # 或者使用lsblk 、mount命令查看当前磁盘挂载情况显示文件系统 1K-块 已用 可用 已用% 挂载点/dev/sdb2 2930134012 1450305892 1479828120 50% /home/lab301/dir表示挂载成功。卸载卸载文件系统使用umount命令。umount &amp;lt;NAME&amp;gt; or &amp;lt;MOUNTPOINT&amp;gt; # 将&amp;lt;NAME&amp;gt;或者&amp;lt;MOUNTPOINT&amp;gt;对应的文件系统卸载设置启动时自动挂载手动挂载的话，每次重新启动后需要重复操作。可通过修改/etc/fstab实现启动时自动挂载。/etc/fstab是启动时的配置文件。首先为该文件增加修改权限：sudo chmod 777 /etc/fstab打开该文件vi /etc/fstab在最后添加一行，每一行的格式为[设备NAME/UUID等] [挂载点] [文件系统] [文件系统参数] [dump] [fsck]其中文件系统可通过lsblk -f(手动)查看，一般情况下文件系统参数、dump、fsck依次填写defaults 0 0 就可以了（更详细的可以参考鸟哥的Linux私房菜，手动敲了半天才发现原来有电子版…）示例：要将 /dev/vda4设置为启动时自动挂载在主文件夹的dir目录下STEP1：修改配置文件。在 /etc/fstab中按上述添加一行/dev/vda4 /home/user/dir ntfs defaults 0 0 注意这里挂载点需要显式地写起，比如这里/home/user/是我的主文件目录，写~/,/都是不对的（后面的测试过程也会提示错误。）STEP2：测试配置修改是否正确。以下两步为测试上述文件配置是否正确的步骤。测试很重要！因为这个文件如果写错了， 则你的 Linux 很可能将无法顺利开机完成！为了便于测试，首先查看该硬盘是否已经挂载，如果挂载了，则先将其卸载$ dfFilesystem 1K-blocks Used Available Use% Mounted on/dev/vda4 1038336 32864 1005472 4% /data/xfs# 竟然不知道何时被挂载了？赶紧给他卸载先！# **因为，如果要被挂载的文件系统已经被挂载了（无论挂载在哪个目录），那测试就不会进行喔！**$ umount /dev/vda4STEP3：挂载测试# 依照配置文件etc/fstab的数据将所有未挂载的磁盘都挂载上来$ mount -a # 如果报错，则根据报错信息对/etc/fstab进行修改$ lsblk -f # 查看/dev/vda4是否已经成功挂载，如果成功挂载，则配置成功致谢以上内容根据《鸟哥的Linux私房菜》整理而来。感谢鸟哥的工作！这本书也有Web版，有需要的读者可以点击链接阅读学习。" }, { "title": "一些基本概念", "url": "/posts/%E4%B8%80%E4%BA%9B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/", "categories": "专业积累, 基础知识", "tags": "", "date": "2021-01-14 09:54:00 +0800", "snippet": "动机一些以往不清楚的基本概念。学习后做下记录。名词 网络的depth、width、cardinality depth：整个网络的层数 width：每一层的通道(channel)数 cardinality:首次出现在ResNeXt中， 原文解释为‘the size of the set of transformations’，可理解为同一层网络中的支路数。 The architecture of neural networks often specified by the width and the depth of the networks. The depth of a network is defined as its number of layers (including output layer but excluding input layer); while the width of a network is defined to be the maximal number of nodes in a layer. The number of input nodes, i.e. the input dimension, is denoted as $n$. 参考-The Expressive Power of Neural Networks - NIPS Proceedings FLOPs floating point operations，意指浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度。 与FLOPS（注意全大写）区分开来，FLOPS是floating point operations per second的缩写，意指每秒浮点运算次数，理解为计算速度。是一个衡量硬件性能的指标。 正则项-regularizer 正则化，更形象的名字叫规则化，就是向你的模型加入某些规则，加入先验，缩小解空间，减小求出错误解的可能性。参考-知乎 分类和回归的区别 解释1：分类问题的输出度量空间是定性的，只有分类“正确”与“错误”之分，不存在一个量来衡量正确或错误的程度是多大；而回归问题的输出度量空间是定量的，有一个量来衡量回归结果和真实情况的定量距离。参考-知乎 解释2：输入变量与输出变量均为连续变量的预测问题是回归问题；输出变量为有限个离散变量的预测问题成为分类问题；——李航《统计学习方法》 以目标检测的一个ROI的预测为例，对这个ROI所属类别的预测就是一个分类问题，要么预测对，要么预测错，而不说分类错误的程度有多大；对这个ROI的bounding box的预测就是一个回归问题，有定量的方法来度量预测框与实际框之间的距离（如MSE）。 平时所说的分类预测得到的是一个向量，代表预测为各个类别的概率，这实际上是一种分类问题的回归化处理，也给分类的损失函数的构造带来了方便（如交叉熵损失）。 结构 分组卷积 上采样(upsampling)、反池化(unpooling)、反卷积(deconvolution)的理解 待整理，参考CSDN、知乎、distill、Medium RNN，LSTM &amp;amp; GRU，同一位作者的两篇Medium博客，配合动画讲得很清晰。操作 gumble softmax 参考1 参考2 参考3" }, { "title": "How to inquire a dataset", "url": "/posts/How-to-inquire-a-dataset/", "categories": "专业积累, 杂记", "tags": "social", "date": "2021-01-11 16:38:00 +0800", "snippet": "One exampleDear Miss/Mr.,Good day.We have the honor to read the project of CMDFALL and are full of interest in the dataset.We extremely hope to be able to compare our new algorithm with other algorithms on your dataset, so could you please share the dataset with us through our account as soon as possible?Our account is xxx@sjtu.edu.cnThanks for your elaborate work. If you need us to provide other information, please feel free to contact us.Best regards.---------------------xxx (advisor: xxx)Institute of xxxShanghai Jiaotong University, Chinasome advicesDon’t be shy. Let’s get this out of the way first. If you go about it well, there’s no absolutely no harm in asking. Most researchers are keen to share and discuss their work. Never be afraid to reach out to big names.Make your purpose clear. Before you send your inquiry you should have a thorough understanding of your goal. Make sure you can explain this. Nobody wants to hand out a hard-earned dataset to a potential hoarder who’s unlikely to ever make good use of the data. Outline your intended project, describe how you’ll publish the results, or better yet, propose a collaboration. You might well have identified a win-win situation for everyone involved. What’s more, the authors are more willing to share their data when the replicator is perceived as trying to be helpful rather than cross-checking results.Make sure you’ve done your homework. Understand the implications of obtaining certain datasets, such as privacy concerns, risks to others, or repeatability of the experiment. If you’re basing your inquiry on a specific piece of work such as a paper, blog post, or open-source project, again, be concrete. Don’t say things like “in your recent paper” but name the exact context.Make your affiliation clear. Don’t use a random Gmail or Hotmail address (good luck particularly with the latter) when you have one that shows you’re in the Computer Science department of a well-known university. Identifying the exact group you’re working in, as well as who’s your adviser, also helps.Find the right point of contact. If you’re addressing the authors of a paper, start by contacting the first author. Some papers explicitly list the right contact for correspondence regarding the paper. Under no circumstances should you send an individual email to each listed contact in parallel.Be responsible. Understand limits if others ask you for the data you’ve received.Be respectful and grateful." }, { "title": "Enable Google Page Views", "url": "/posts/enable-google-pv/", "categories": "Blogging, Tutorial", "tags": "google analytics, pageviews", "date": "2021-01-04 07:32:00 +0800", "snippet": "This post is to enable Page Views on the Chirpy theme based blog that you just built. This requires technical knowledge and it’s recommended to keep the google_analytics.pv.* empty unless you have a good reason. If your website has low traffic, the page views count would discourage you to write more blogs. With that said, let’s start with the setup.Set up Google AnalyticsCreate GA account and propertyFirst, you need to set up your account on Google analytics. While you create your account, you must create your first Property as well. Head to https://analytics.google.com/ and click on Start Measuring Enter your desired Account Name and choose the desired checkboxes Enter your desired Property Name. This is the name of the tracker project that appears on your Google Analytics dashboard Enter the required information About your business Hit Create and accept any license popup to set up your Google Analytics account and create your propertyCreate Data StreamWith your property created, you now need to set up Data Stream to track your blog traffic. After you signup, the prompt should automatically take you to create your first Data Stream. If not, follow these steps: Go to Admin on the left column Select the desired property from the drop-down on the second column Click on Data Streams Add a stream and click on Web Enter your blog’s URLIt should look like this:Now, click on the new data stream and grab the Measurement ID. It should look something like G-V6XXXXXXXX. Copy this to your _config.yml file:google_analytics: id: &#39;G-V6XXXXXXX&#39; # fill in your Google Analytics ID # Google Analytics pageviews report settings pv: proxy_endpoint: # fill in the Google Analytics superProxy endpoint of Google App Engine cache_path: # the local PV cache data, friendly to visitors from GFW regionWhen you push these changes to your blog, you should start seeing the traffic on your Google Analytics. Play around with the Google Analytics dashboard to get familiar with the options available as it takes like 5 mins to pick up your changes. You should now be able to monitor your traffic in real time.Setup Page ViewsThere is a detailed tutorial available to set up Google Analytics superProxy. But, if you are interested to just quickly get your Chirpy-based blog display page views, follow along. These steps were tested on a Linux machine. If you are running Windows, you can use the Git bash terminal to run Unix-like commands.Setup Google App Engine Visit https://console.cloud.google.com/appengine Click on Create Application Click on Create Project Enter the name and choose the data center close to you Select Python language and Standard environment Enable billing account. Yeah, you have to link your credit card. But, you won’t be billed unless you exceed your free quota. For a simple blog, the free quota is more than sufficient. Go to your App Engine dashboard on your browser and select API &amp;amp; Services from the left navigation menu Click on Enable APIs and Services button on the top Enable the following APIs: Google Analytics API On the left, Click on OAuth Consent Screen and accept Configure Consent Screen. Select External since your blog is probably hosted for the public. Click on Publish under Publishing Status Click on Credentials on the left and create a new OAuth Client IDs credential. Make sure to add an entry under Authorized redirect URIs that matches: https://&amp;lt;project-id&amp;gt;.&amp;lt;region&amp;gt;.r.appspot.com/admin/auth Note down the Your Client ID and Your Client Secret. You’ll need this in the next section. Download and install the cloud SDK for your platform: https://cloud.google.com/sdk/docs/quickstart Run the following commands: [root@bc96abf71ef8 /]# gcloud init ~snip~ Go to the following link in your browser: https://accounts.google.com/o/oauth2/auth?response_type=code&amp;amp;client_id=XYZ.apps.googleusercontent.com&amp;amp;redirect_uri=ABCDEFG Enter verification code: &amp;lt;VERIFICATION CODE THAT YOU GET AFTER YOU VISIT AND AUTHENTICATE FROM THE ABOVE LINK&amp;gt; You are logged in as: [blah_blah@gmail.com]. Pick cloud project to use:[1] chirpy-test-300716[2] Create a new projectPlease enter numeric choice or text value (must exactly match listitem): 1[root@bc96abf71ef8 /]# gcloud info# Your selected project info should be displayed here Setup Google Analytics superProxy Clone the Google Analytics superProxy project on Github: https://github.com/googleanalytics/google-analytics-super-proxy to your local. Remove the first 2 lines in the src/app.yaml file: - application: your-project-id- version: 1 In src/config.py, add the OAUTH_CLIENT_ID and OAUTH_CLIENT_SECRET that you gathered from your App Engine Dashboard. Enter any random key for XSRF_KEY, your config.py should look similar to this #!/usr/bin/python2.7__author__ = &#39;pete.frisella@gmail.com (Pete Frisella)&#39;# OAuth 2.0 Client SettingsAUTH_CONFIG = { &#39;OAUTH_CLIENT_ID&#39;: &#39;YOUR_CLIENT_ID&#39;, &#39;OAUTH_CLIENT_SECRET&#39;: &#39;YOUR_CLIENT_SECRET&#39;, &#39;OAUTH_REDIRECT_URI&#39;: &#39;%s%s&#39; % ( &#39;https://chirpy-test-XXXXXX.ue.r.appspot.com&#39;, &#39;/admin/auth&#39; )}# XSRF SettingsXSRF_KEY = &#39;OnceUponATimeThereLivedALegend&#39; You can configure a custom domain instead of https://PROJECT_ID.REGION_ID.r.appspot.com.But, for the sake of keeping it simple, we will be using the Google provided default URL. From inside the src/ directory, deploy the app [root@bc96abf71ef8 src]# gcloud app deployServices to deploy: descriptor: [/tmp/google-analytics-super-proxy/src/app.yaml]source: [/tmp/google-analytics-super-proxy/src]target project: [chirpy-test-XXXX]target service: [default]target version: [VESRION_NUM]target url: [https://chirpy-test-XXXX.ue.r.appspot.com]Do you want to continue (Y/n)? Y Beginning deployment of service [default]...╔════════════════════════════════════════════════════════════╗╠═ Uploading 1 file to Google Cloud Storage ═╣╚════════════════════════════════════════════════════════════╝File upload done.Updating service [default]...done.Setting traffic split for service [default]...done.Deployed service [default] to [https://chirpy-test-XXXX.ue.r.appspot.com] You can stream logs from the command line by running:$ gcloud app logs tail -s default To view your application in the web browser run:$ gcloud app browse Visit the deployed service. Add a /admin to the end of the URL. Click on Authorize Users and make sure to add yourself as a managed user. If you get any errors, please Google it. The errors are self-explanatory and should be easy to fix. If everything went good, you’ll get this screen:Create Google Analytics QueryHead to https://PROJECT_ID.REGION_ID.r.appspot.com/admin and create a query after verifying the account. GA Core Reporting API query request can be created in Query Explorer.The query parameters are as follows: start-date: fill in the first day of blog posting end-date: fill in today (this is a parameter supported by GA Report, which means that it will always end according to the current query date) metrics: select ga:pageviews dimensions: select ga:pagePathIn order to reduce the returned results and reduce the network bandwidth, we add custom filtering rules 1: filters: fill in ga:pagePath=~^/posts/.*/$;ga:pagePath!@=. Among them, ; means using logical AND to concatenate two rules. If the site.baseurl is specified, change the first filtering rule to ga:pagePath=~^/BASE_URL/posts/.*/$, where BASE_URL is the value of site.baseurl. After Run Query, copy the generated contents of API Query URI at the bottom of the page and fill in the Encoded URI for the query of SuperProxy on GAE.After the query is saved on GAE, a Public Endpoint (public access address) will be generated, and we will get the query result in JSON format when accessing it. Finally, click Enable Endpoint in Public Request Endpoint to make the query effective, and click Start Scheduling in Scheduling to start the scheduled task.Configure Chirpy to Display Page ViewOnce all the hard part is done, it is very easy to enable the Page View on Chirpy theme. Your superProxy dashboard should look something like below and you can grab the required values.Update the _config.yml file of Chirpy project with the values from your dashboard, to look similar to the following:google_analytics: id: &#39;G-V6XXXXXXX&#39; # fill in your Google Analytics ID pv: proxy_endpoint: &#39;https://PROJECT_ID.REGION_ID.r.appspot.com/query?id=&amp;lt;ID FROM SUPER PROXY&amp;gt;&#39; cache_path: # the local PV cache data, friendly to visitors from GFW regionNow, you should see the Page View enabled on your blog.Reference Google Analytics Core Reporting API: Filters &amp;#8617; " }, { "title": "Python积累", "url": "/posts/Python%E7%A7%AF%E7%B4%AF/", "categories": "专业积累, 编程积累", "tags": "python", "date": "2020-12-11 14:15:00 +0800", "snippet": "关于本页一些Python的基本知识、操作积累，记录下方便日后查找使用。python文件读写模式参考：Stack Overflow | r r+ w w+ a a+------------------|--------------------------read | + + + +write | + + + + +write after seek | + + +create | + + + +truncate | + +position at start | + + + +position at end | + +Some example# writewith open(filename, &#39;w+&#39;) as f: f.write(&amp;lt;something&amp;gt;)# read - way1with open(filename, &#39;r&#39;) as f: for line in f.readlines(): str1, str2 = line.split()# read - way2file = open(filename)for line in file: str1, str2 = line.split()file.close()多次访问文件的话, 下次访问前需回到文件头file.seek(0) 杂记 使用map方便地把line.split()得到的str列表统一映射到其他数据格式&amp;gt; num1, num2, num3, num4 = map(float, line.split()) json文件的读写 # 读取json文件为字典with open(&amp;lt;json_path&amp;gt;, &#39;r&#39;) as f: loaded_dict = json.load(f) # 将new_dict写入json_pathwith open(&amp;lt;json_path&amp;gt;, &quot;w&quot;) as f: json.dump(new_dict, f) Python中的深拷贝与浅拷贝参考1 - runoob :star:参考2 - 掘金 直接赋值： 实质上就是对象的引用（别名）。 浅拷贝(copy)： 拷贝父对象，不会拷贝对象的内部的子对象。 深拷贝(deepcopy)： copy模块的deepcopy 方法，完全拷贝了父对象及其子对象。示例import copya = [1, 2, 3.0, {&#39;key&#39;: &#39;val&#39;}, [&#39;a&#39;, &#39;b&#39;]] # 原始对象 b = a # 赋值，传对象的引用c = copy.copy(a) # 浅拷贝, 也可简单使用 c = a.copy()d = copy.deepcopy(a) # 深拷贝 a.append(5) # 修改对象a (父对象)a[4].append(&#39;c&#39;) # 修改对象a中的[&#39;a&#39;, &#39;b&#39;]数组对象 (子对象)a[0] += 1# 从输出值验证一下print( &#39;a = &#39;, a ) print( &#39;b = &#39;, b ) print( &#39;c = &#39;, c ) print( &#39;d = &#39;, d ) &#39;&#39;&#39;outputa = [2, 2, 3.0, {&#39;key&#39;: &#39;val&#39;}, [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;], 5] b = [2, 2, 3.0, {&#39;key&#39;: &#39;val&#39;}, [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;], 5] # b为a的引用，与a共享同一块内存地址，因此与a同步改变c = [1, 2, 3.0, {&#39;key&#39;: &#39;val&#39;}, [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]] # c为浅拷贝，拷贝了父对象但没拷贝子对象d = [1, 2, 3.0, {&#39;key&#39;: &#39;val&#39;}, [&#39;a&#39;, &#39;b&#39;]] # d为深拷贝，拷贝了父对象也拷贝了子对象&#39;&#39;&#39;# 从内存地址验证一下， id()函数可返回变量的内存地址print(id(a), id(a[0]), id(a[1]), id(a[2]), id(a[3]), id(a[4]))print(id(b), id(b[0]), id(b[1]), id(b[2]), id(b[3]), id(b[4]))print(id(c), id(c[0]), id(c[1]), id(c[2]), id(c[3]), id(c[4]))print(id(d), id(d[0]), id(d[1]), id(d[2]), id(d[3]), id(d[4]))&#39;&#39;&#39;output 139864587372672 94459641159008 94459641159008 139864585514544 139864587372928 139864587371008139864587372672 94459641159008 94459641159008 139864585514544 139864587372928 139864587371008139864587370560 94459641158976 94459641159008 139864585514544 139864587372928 139864587371008139864587370944 94459641158976 94459641159008 139864585514544 139864587373440 139864587373696&#39;&#39;&#39;内存地址返回值单独分析下：对于每一个变量均返回了六个内存地址，第一个为了验证父对象地址的变化情况；第二个为了验证发生变化的整型数地址的变化情况；第三个，第四个分别验证未发生变化的整型数、浮点数的地址的变化情况；第五个，第六个分别验证类型为字典和列表的子对象的地址变化情况。可以看出： 引用(b)：父对象地址（第一个）和各子对象地址与原对象保持完全一致。 浅拷贝(c)：重新开辟了一个新的父对象地址（第一个），但列表/字典类型的子对象（第五、第六个）地址仍指向原对象。 深拷贝(d)：重新开辟了一个新的父对象地址（第一个），以及列表/字典类型的子对象（第五、第六个）地址。 第二、第三、第四个真是还存在疑惑。对单纯的整型数或浮点数似乎有些特殊：对值未更改的整型数、浮点数（第三、第四个），不管是深浅拷贝，作为子对象都会仍和原元素共享同一块地址；而似乎在对其进行重新赋值操作时（第二个），会自动重新为其分配一块新地址，可能是和Python的数据存储方式有关。欢迎讨论。最后验证一下过程中产生的一个细思极恐的问题:wave:a = 1b = aprint(id(a)) # 94767788772672print(id(b)) # 94767788772672a += 1print(id(a)) # 94767788772704print(id(b)) # 94767788772672整型数/浮点数的赋值操作，确实也是一个引用的过程。但是当对一个变量(a)进行修改后，从表现上看，它会剥离原内存空间，开辟一块新空间来保存修改后的数，另一个变量(b)保持在原来的内存空间，它的值不会受到相应的影响。换句话说，二者值的变化是自动解耦的。要不之前写了这么多整型数/浮点数的赋值操作却没有意识到错误，真就完犊子。Python之import的机理参考1 参考2 如果程序的入口文件在顶级目录，由于该顶级目录已经在sys.path里了，所以将导入路径从顶级目录一路写下来即可。比如，在该顶级目录（及其子目录）下的任意一文件中，导入其他文件 from mmt.evaluation_metrics import accuracy # mmt为顶级目录下的一个模块 如果希望从其他位置导入模块，可以 import syssys.path.append(&#39;&amp;lt;your_import_path&amp;gt;&#39;) python的引用分为绝对导入和相对导入，以下为一些相对导入的例子 from .package_name import module_name # 导入和自己同目录的包的模块。 如果要把一个文件夹封装为一个包，需要在该文件夹下建立一个__init__.py文件，即使为空文件。 格式化时间参考import time # 格式化成2016-03-20 11:45:39形式print(time.strftime(&quot;%Y-%m-%d %H:%M:%S&quot;, time.localtime())) # 格式化成Sat Mar 28 22:24:24 2016形式print(time.strftime(&quot;%a %b %d %H:%M:%S %Y&quot;, time.localtime())) # 将格式字符串转换为时间戳a = &quot;Sat Mar 28 22:24:24 2016&quot;print(time.mktime(time.strptime(a,&quot;%a %b %d %H:%M:%S %Y&quot;)))python给数字前面补零n = &quot;123&quot;s = n.zfill(5) # 或 s = &quot;%05d&quot; % nassert s == &quot;00123&quot;获得当前目录的父目录import oscwd = os.getcwd()parent_dir = os.path.dirname(cwd)print(cwd) # /parent_dir/child_dirprint(parent_dir) # /parent_dirPython相对路径与绝对路径相互转化import os# convert relative path to absolute pathabs_path = os.path.abspath(rel_path) # convert absolute path to relative pathrel_path = os.path.relpath(abs_path) 获取Python包的安装目录有的时候我们想查看或修改Python包的源码，它们在哪里呢？可通过import moduleprint(module.__file__)查看，如import torchprint(torch.__file__)理解Python中的if __name__ == ‘__main__’参考通俗地理解__name__ == __main__&#39;：假如你叫小明.py，在朋友眼中，你是小明(__name__ == &#39;小明&#39;)；在你自己眼中，你是你自己(__name__ == &#39;__main__&#39;)。if __name__ == &#39;__main__&#39;的意思是：当.py文件被直接运行时，if __name__ == &#39;__main__&#39;之下的代码块将被运行；当.py文件以模块形式被导入时，if __name__ == &#39;__main__&#39;之下的代码块不被运行。根据字符串返回函数 使用getattr getattr() 是 python 的内建函数，getattr(object,name) 就相当于object.name class Foo: def __init__(self): self.func1 = ... def call_func1(self): getattr(self, func1) # 返回 self.func1，执行需要getattr(self, func1)() def func2(self): print(&quot;I am func2&quot;) foo = Foo()getattr(foo, &quot;func2&quot;)() 使用eval def func1(): print(&quot;I am func1&quot;) eval(&quot;func1&quot;)()# out: I am func1 Python文件新建，删除，复制，移动import osimport shutil# 判断文件(夹)是否存在os.path.exists()# 新建文件夹os.makedirs()# 删除文件os.remove()# 删除文件夹shutil.rmtree()# 复制文件shutil.copy(src_file, dst_file_or_dir)# 复制文件夹shutil.copytree(src_dir, dst_dir)# 移动文件shutil.move(old_pos, new_pos)# 重命名os.rename(old_name, new_name)os.mkdir()与os.makedirs()os.makedirs()会递归地建立输入的路径，如果父级路径不存在，会自动建立。而os.mkdir()只能一级一级地建立目录，如果父级路径不存在，则会报错。with … as …用法参考with 语句适用于对资源进行访问的场合，确保不管使用过程中是否发生异常都会执行必要的“清理”操作，释放资源，比如文件使用后自动关闭／线程中锁的自动获取和释放等。如with open(&quot;1.txt&quot;) as file: data = file.read()是比file = open(&quot;1.txt&quot;)data = file.read()file.close()# 1. 文件读取发生异常，但没有进行任何处理；# 2. 可能忘记关闭文件句柄。更加优雅鲁棒的编程方式。工作原理先看例子：class Sample: def __init__(self): pass def __enter__(self): print (&quot;in __enter__&quot;) return &quot;Foo&quot; def __exit__(self, exc_type, exc_val, exc_tb): print (&quot;in __exit__&quot;)sample = Sample()with sample as s: print (&quot;sample: &quot;, s)输出：in __enter__Sample: Fooin __exit__可以看出，with ... as ...的工作原理为： 紧跟with后面变量的__enter__()方法首先被调用，其返回值赋给as后面的变量； 当with后面的代码块全部被执行完之后， 执行with后面变量的__exit__方法。Python运算符重载参考一些常见的记录 方法 重载 调用 __init__ 构造函数 X = ClassX(args) __len__ 获取长度 len(X) __repr__ 打印 print(X) __getitem__ 索引 X[i] __setitem__ 索引赋值 X[i] = value __call__ 把一个类当作函数调用 X(*args,**kargs) __enter__, __exit__ 环境管理器 with obj as var： 来看demo：class ToyClass: def __init__(self, data): self.data = data def __repr__(self): # return self.data # TypeError: __str__ returned non-string (type list) return str(self.data) def __len__(self): return len(self.data) def __getitem__(self, i): return self.data[i] def __setitem__(self, i, value): self.data[i] = value def __call__(self, param): print(&quot;__call__() is called, your input param is {}&quot;.format(param))toy = ToyClass(data=[&#39;I&#39;, &#39;LOVE&#39;, &#39;SJTU&#39;])# __repr__print(toy) # out: [&#39;I&#39;, &#39;LOVE&#39;, &#39;SJTU&#39;]# __len__print(len(toy)) # out: 3 # __getitem__print(toy[0]) # out: I print(toy[0:3]) # out: [&#39;I&#39;, &#39;LOVE&#39;, &#39;SJTU&#39;]# __setitem__toy[0] = &#39;U&#39; print(toy) # out: [&#39;U&#39;, &#39;LOVE&#39;, &#39;SJTU&#39;]# __call__toy(&#39;S&#39;) # out: __call__() is called, your input param is S土味断点——使用input()在需要程序暂停的地方插入一个input()，程序运行至此会暂停，按下回车后继续for i in range(10): print(i) input() # Loop continues after &amp;lt;Enter&amp;gt; is pressed一些基本语法遍历字典demo_dict = dict(...)# 遍历所有keyfor key in demo_dict.keys(): print(key)# .keys()返回dict_keys对象，不支持索引，如需索引，需要强制转换list(demo_dict.keys())，.values()同理# 遍历所有valuefor value in demo_dict.values(): print(value) # 遍历所有键值对for key, value in demo_dict.items(): print(key, value)字典相关 pop()： demo_dict.pop(key)，从demo_dict中删去键值为key的键值对 __contains__()：demo_dict.__contains__(key)，判断demo_dict中是否还有key键值，时间复杂度为O(1) Python3.X不支持has_key() 字符串相关 大写转小写：c.lower() 小写转大写：c.upper()assert语句使用方法：assert expression [, arguments]注意，当需要加arguments时，使用方法为assert expression, arguments，而不是assert (expression, arguments) （即不加括号）！！！原因在于assert是一个关键字，而不是一个函数。当使用assert (expression, arguments)时，(expression, arguments)会被解释为一个元组，而元组是True的，所以会得到 SyntaxWarning: assertion is always true, perhaps remove parentheses?的warning，assert实际上也就失效了。所以切记不能用错了。参考杂项 生成0~1区间内的随机数 import random rand = random.random() 一些使用trick排序 字典排序 按key排序：sorted(d.items(), key = lambda x : x[0]) 按value排序：sorted(d.items(), key = lambda x : x[1]) d = {&#39;a&#39; : 3, &#39;b&#39; : 2, &#39;c&#39; : 3}d_key_sorted = sorted(d.items(), key = lambda x : x[0]) # [(&#39;a&#39;, 3), (&#39;b&#39;, 2), (&#39;c&#39;, 1)]d_value_sorted = sorted(d.items(), key = lambda x : x[1]) # [(&#39;c&#39;, 1), (&#39;b&#39;, 2), (&#39;a&#39;, 3)] 多条件排序 intervals = [[0, 1], [2, 4], [2, 5]] # 第一个元素升序排列，第二个元素降序排列 intervals.sort(key = lambda x : (x[0], -x[1])) # [[0, 1], [2, 5], [2, 4]] python -c &amp;lt;command&amp;gt;不进入python解释器的交互模式，直接执行python代码。在进行一些简单的测试时很方便。$ python -c &quot;import torch&quot; # 查看torch是否能正常导入setup.py-A Practical Guide to Using Setup.py" }, { "title": "Ubuntu下nvidia driver和cuda版本管理", "url": "/posts/Ubuntu%E4%B8%8Bnvidia-driver%E5%92%8Ccuda%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86/", "categories": "实践, 环境配置", "tags": "ubuntu, nvidia driver, cuda", "date": "2020-12-05 18:15:00 +0800", "snippet": "动机记录下在Ubuntu 18.04下nvidia driver 455.45与cuda 10.1的安装过程。备后续重复安装时直接参照。版本对应关系cuda与nvidia driver可以看到，Driver对于CDUA版本是向下兼容的。表格内容可能过时，可以点击nvidia官网链接查看最新更新。cuda与pytorch较老版本的Pytorch：参见本页最新版Pytorch：Pytorch首页给出了最新版Pytorch可用的组合和对应的安装命令。以下以在Ubuntu 18.04下，安装nvidia driver 455.45与cuda 10.1 为例，记录安装过程。安装/更新英伟达驱动(nvidia driver)卸载原驱动(如若需要)sudo nvidia-uninstallsudo apt-get --purge remove nvidia-*# ./NVIDIA-Linux-x86_64-390.48.run --uninstall 第一个弹窗选yes 参考 20:43下载正确版本的驱动查看显卡型号 lspci | grep -i nvidia 部分老版本Ubuntu(如16.04)输出为一十六进制数，可以在The PCI ID RepositorySee also一栏下输入框查询该代码对应的显卡型号。下载相应的驱动版本在nvidia驱动下载页手动搜索并下载所需的驱动版本。安装新驱动预备首先，禁用系统自带的nouveau驱动#创建配置文件sudo vim /etc/modprobe.d/blacklist-nouveau.conf # 在打开的配置文件里添加如下内容blacklist nouveauoptions nouveau modeset=0 # 更新sudo update-initramfs -u Tips：打开vim后按inset开始编辑，编辑结束后vim的退出操作为：先按Esc，然后输入:wq重启。（重启后分辨率可能会暂时性下降。在安装NVIDIA驱动完成后，重启即可恢复）重启后在终端输入lsmod | grep nouveau如果输出为空，则禁用成功。开始安装进入刚才下载的驱动所在的目录，执行以下命令进行安装sudo sh NVIDIA-Linux-x86_64-&amp;lt;your-driver-version&amp;gt;.run按照默认设置，一路回车，不出意外便可顺利结束安装。测试nvidia-smi输出+-----------------------------------------------------------------------------+| NVIDIA-SMI 455.45.01 Driver Version: 455.45.01 CUDA Version: 11.1 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. || | | MIG M. ||===============================+======================+======================|| 0 GeForce GTX TIT... Off | 00000000:01:00.0 On | N/A || 22% 58C P8 21W / 250W | 90MiB / 12211MiB | 0% Default || | | N/A |+-------------------------------+----------------------+----------------------+实际安装过程遇到过的一些问题Failed CC version check. Bailing out!Compiler version check failed:The major and minor number of the compiler used tocompile the kernel:gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04.4) does not match the compiler used here:cc (Ubuntu 4.9.4-2ubuntu1~14.04.1) 4.9.4将本机的gcc默认版本切换为当前编译所需版本即可(此处为将4.9.4切换为4.8.4)。参考1-ubuntu下多个gcc版本切换参考2-Ubuntu 16.04 GCC 7 &amp;amp; G++ 7 安装。 查看当前使用的gcc版本：gcc -v 查看本机已安装哪些gcc版本：ls /usr/bin/gcc* 切换gcc版本：sudo update-alternatives --config gcc 根据输出，输入希望切换至的gcc版本对应的编号，即可完成切换。 no ccsudo apt updatesudo apt install build-essential对ubuntu系统自动更新的尝试这种方法很是简便，但在我的电脑上尝试时，虽然可以获取相应更新，但是重启后驱动无法连接，也无法正常加载桌面。但也记录一下，没有行通可能只是我电脑本身的原因。首先，更新系统软件源信息sudo add-apt-repository ppa:graphics-drivers/ppa &amp;amp;&amp;amp; sudo apt update然后来到设置-软件和更新-附加驱动，选择希望更新至的驱动版本，点击应用更改，完成后重启。安装/切换cuda版本参考链接下载并安装cuda从CUDA Toolkit Archive找到所需的cuda版本，选择runfile类型文件下载。进入刚才下载文件所在的目录，执行以下命令进行安装 （执行本命令前请先看下面的注）sudo ./cuda_&amp;lt;your_version&amp;gt;_linux.run 希望一切顺利！注：如果已经安装了nvidia驱动（常发生在安装新版本cuda的情况），这一步不需要再安装，取消默认选中的NVIDIA Driver（不取消不仅可能会安装失败，还可能导致重启时循环登录），如下CUDA Installer ││ - [ ] Driver ││ [ ] 450.51.06 ││ + [X] CUDA Toolkit 11.0 ││ [ ] CUDA Samples 11.0 ││ [ ] CUDA Demo Suite 11.0 ││ [ ] CUDA Documentation 11.0 ││ Options ││ Install 按回车取消其他选项，仅保留CUDA Toolkit 11.0，选择Install继续安装。cuda环境设置在~/.bashrc末尾添加export PATH=/usr/local/cuda/bin${PATH:+:${PATH}}export LD_LIBRARY_PATH=/usr/local/cuda/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}export CUDA_HOME=/usr/local/cuda 上面的路径都是指向/usr/local/cuda的软连接，而不是指向某一个具体的cuda版本，这样方便于后面进行cuda版本的切换。执行以下命令使改动立即生效source ~/.bashrc叮叮，安装完成。切换cuda版本查看当前使用的cuda版本：nvcc -V 当电脑里安装了多个cuda版本，而想从一个版本切换到另外一个版本时，可以将软链接重指向至新版本：cd /usr/local/ # 切到cuda所在目录下sudo rm -rf ./cuda # 删去原先使用的cuda版本的软链接sudo ln -s cuda-10.1 cuda # 创建一个新的软链接到希望切换到的cuda版本再使用nvcc -V 看看cuda使用版本是否已经发生改变叭。卸载cudaTo uninstall the CUDA Toolkit, run cuda-uninstaller in /usr/local/cuda-11.1/bin使用过程中出现过的问题及解决方案 NVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver 安装cuda后出现图形化界面循环登录的现象。 参考 这是因为运行cuda安装文件时，没有取消安装NVIDIA驱动选项导致的。 解决： 卸载安装的CUDA sudo /usr/local/cuda-10.1/bin/cuda-uninstaller 重新安装CUDA（注意取消安装NVIDIA驱动的选项） 其他显示Nvidia显卡型号（已安装Nvidia驱动后）：nvidia-smi -L" }, { "title": "VS code使用", "url": "/posts/VS-code%E4%BD%BF%E7%94%A8/", "categories": "实践, 硬技能", "tags": "vs code", "date": "2020-11-20 12:15:00 +0800", "snippet": "VS code Remote ssh配置这是官方介绍文档。但感觉讲得太过简略，跟着做的话便是一头雾水。网上有不少配置的经验，我按照这个进行配置，最终成功了。将具体步骤记录下来，以备日后参照。配置私钥 在本地C:\\Users\\用户名\\.ssh目录下，用ssh-keygen命令生成密钥。 将本地生成的id_rsa.pub文件上传到远程主目录的~/.ssh/文件夹下。 .ssh是一个隐藏文件夹，一个简单的办法是，直接通过路径名进入。 在远程将本地上传的id_rsa.pub加入到authorized_keys中。具体做法为 cd ~/.ssh cat id_rsa.pub &amp;gt;&amp;gt; authorized_keys 这时.ssh文件夹下多出一个authorized_keys文件。 (maybe optional) 在本地.ssh目录下，使用私钥登录远程 ssh &amp;lt;username&amp;gt;@&amp;lt;hostname&amp;gt; -p &amp;lt;port&amp;gt; -i id_rsa 这次登录无需密码。 VS Code 配置安装Remote Development插件在VS code左边栏扩展商店Extensions搜索Remote Development插件并安装。添加配置文件点击左边栏Remote Explorer的图标后，再点击箭头所指的齿轮会弹出菜单让你选择需要编辑的配置文件，选择第一个选择之后可以按照下图添加配置信息 参数含义为 Host: 连接的主机的名称，可自定 Hostname: 远程主机的IP地址 User: 用于登录远程主机的用户名 Port: 用于登录远程主机的端口 IdentityFile: 本地的id_rsa的路径需要多个连接的话，可以按照如上配置多个。 配置完成并保存后，左侧连接栏中便出现了所设定的的远程主机。右键点击Connect便可连接。在VS Code上编译Latex配置 在清华源下载安装texlive.iso，安装。 安装VS Code插件Latex Workshop。 配置VS Code: 在 VS Code 界面下按下 F1，然后键入“setjson”，点击Preference: Open Settings(JSON); 在打开的settings.json文件中，将以下大括号内的配置内容复制进去(参考，各选项含义可参考原文): { // latex-workshop config starts &quot;latex-workshop.latex.autoBuild.run&quot;: &quot;never&quot;, &quot;latex-workshop.showContextMenu&quot;: true, &quot;latex-workshop.intellisense.package.enabled&quot;: true, &quot;latex-workshop.message.error.show&quot;: false, &quot;latex-workshop.message.warning.show&quot;: false, &quot;latex-workshop.latex.tools&quot;: [ { &quot;name&quot;: &quot;xelatex&quot;, &quot;command&quot;: &quot;xelatex&quot;, &quot;args&quot;: [ &quot;-synctex=1&quot;, &quot;-interaction=nonstopmode&quot;, &quot;-file-line-error&quot;, &quot;%DOCFILE%&quot; ] }, { &quot;name&quot;: &quot;pdflatex&quot;, &quot;command&quot;: &quot;pdflatex&quot;, &quot;args&quot;: [ &quot;-synctex=1&quot;, &quot;-interaction=nonstopmode&quot;, &quot;-file-line-error&quot;, &quot;%DOCFILE%&quot; ] }, { &quot;name&quot;: &quot;latexmk&quot;, &quot;command&quot;: &quot;latexmk&quot;, &quot;args&quot;: [ &quot;-synctex=1&quot;, &quot;-interaction=nonstopmode&quot;, &quot;-file-line-error&quot;, &quot;-pdf&quot;, &quot;-outdir=%OUTDIR%&quot;, &quot;%DOCFILE%&quot; ] }, { &quot;name&quot;: &quot;bibtex&quot;, &quot;command&quot;: &quot;bibtex&quot;, &quot;args&quot;: [ &quot;%DOCFILE%&quot; ] } ], &quot;latex-workshop.latex.recipes&quot;: [ { &quot;name&quot;: &quot;XeLaTeX&quot;, &quot;tools&quot;: [ &quot;xelatex&quot; ] }, { &quot;name&quot;: &quot;PDFLaTeX&quot;, &quot;tools&quot;: [ &quot;pdflatex&quot; ] }, { &quot;name&quot;: &quot;BibTeX&quot;, &quot;tools&quot;: [ &quot;bibtex&quot; ] }, { &quot;name&quot;: &quot;LaTeXmk&quot;, &quot;tools&quot;: [ &quot;latexmk&quot; ] }, { &quot;name&quot;: &quot;xelatex -&amp;gt; bibtex -&amp;gt; xelatex*2&quot;, &quot;tools&quot;: [ &quot;xelatex&quot;, &quot;bibtex&quot;, &quot;xelatex&quot;, &quot;xelatex&quot; ] }, { &quot;name&quot;: &quot;pdflatex -&amp;gt; bibtex -&amp;gt; pdflatex*2&quot;, &quot;tools&quot;: [ &quot;pdflatex&quot;, &quot;bibtex&quot;, &quot;pdflatex&quot;, &quot;pdflatex&quot; ] }, ], &quot;latex-workshop.latex.clean.fileTypes&quot;: [ &quot;*.aux&quot;, &quot;*.bbl&quot;, &quot;*.blg&quot;, &quot;*.idx&quot;, &quot;*.ind&quot;, &quot;*.lof&quot;, &quot;*.lot&quot;, &quot;*.out&quot;, &quot;*.toc&quot;, &quot;*.acn&quot;, &quot;*.acr&quot;, &quot;*.alg&quot;, &quot;*.glg&quot;, &quot;*.glo&quot;, &quot;*.gls&quot;, &quot;*.ist&quot;, &quot;*.fls&quot;, &quot;*.log&quot;, &quot;*.fdb_latexmk&quot; ], &quot;latex-workshop.latex.autoClean.run&quot;: &quot;onFailed&quot;, &quot;latex-workshop.latex.recipe.default&quot;: &quot;lastUsed&quot;, &quot;latex-workshop.view.pdf.internal.synctex.keybinding&quot;: &quot;double-click&quot;, // latex-workshop config ends} 使用 选择编译链点击进行编译，编译完成后预览PDF文件，如下图所示。 如果发现.tex文件不能自动换行，alt + Z即可。使用技巧快捷键 :rocket: 根据文件名打开最近打开过的文件：ctrl + p （Mac为cmd + p） 调出设置对话框: ctrl + shift + p 文件内方法/类的展开/折叠 折叠全部：Ctrl + K Ctrl + 0 展开全部：Ctrl + K Ctrl + J 更多和细粒度的方式见参考 跳转到指定行：按ctrl + G，输入行数 函数跳转与返回：按住alt后点击函数或变量定义可实现函数跳转，跳转后需要返回可以：Alt+\\(\\leftarrow\\)从visx手动安装插件有的时候VS Code常规的插件安装方法，因为网络等原因无法正常安装，可尝试手动下载visx进行安装。步骤为 进入 Marketplace: https://marketplace.visualstudio.com/VSCode 搜索并进入要安装插件的页面，点击Download Extensions 在VS Code侧边栏插件页面，点击Install from VSIX，在弹出的对话框中选择刚下载的VSIX文件即可。个性化配置增加终端最大显示行数VS Code的终端默认最大显示行数为1000，有时候输出信息很长时会显示不全，可以增大最大显示行数解决，方法为：点击左下角齿轮选择settings，在弹出设置搜索框中输入scrollback，将其从1000调整至需要的更大数值。保存文件时，自动删除多余的空格设置-搜索“trailing”，勾选“Trim Trailing Whitespace”取消文件自动定位到侧边栏https://www.zhihu.com/question/65048697/answer/823033651在线VS code项目repo在github访问某个仓库时，把网页链接的github后面加上个 1s，即可以网页版的VS code形式预览该仓库。(我直呼+1s :eyeglasses:)使用示例：把原先的github网址链接：https://github.com/conwnet/github1s改为https://github1s.com/conwnet/github1s" }, { "title": "Git学习", "url": "/posts/Git%E5%AD%A6%E4%B9%A0/", "categories": "实践, 硬技能", "tags": "git, github", "date": "2020-10-18 21:55:00 +0800", "snippet": "动机久仰Git版本管理的大名，自己在做一些项目时也发现版本的管理是个头疼的问题。因此作为一项硬技能，有必要好好学一下。Git原理首先理解一下工作区、版本库的概念。 工作区：仓库目录下除.git之外的，用户进行一系列新建、修改文件的区域。 版本库：仓库目录下的一个(隐藏)目录.git，这个就是Git的版本库。 版本库中有暂存区(stage或index)、(自动创建的)master分支和指向该分支的指针HEAD。 Git的工作区与版本库创建版本库在目标目录下执行git init，把目标目录变为Git可以管理的仓库。执行后便可发现当前目录自动生成了一个.git目录，这个目录是Git来跟踪管理版本库的。 没事不要手动修改这个目录里面的文件，不然改乱了，就把Git仓库给破坏了。执行git add &amp;lt;file&amp;gt;，把文件添加到暂存区。执行git commit -m &quot;message&quot;，把暂存区的文件提交。-m后面输入的是本次提交的说明，当需要查看历史记录时显示的就是这个说明，所以要写得有意义些，便于区分。 可以一次add许多文件到缓存区，如git add file1.txt, file2.txt，然后一次性commit提交到仓库中。 git add .：add所有文件的所有变化，在Git 2.x中与git add -A等效（在Git 1.x中不等效，区别在于git add .不包含文件删除操作，可参见Stackoverflow）几个重要且常用的命令 git status：查看当前仓库状态信息。 git ls-files：列出Git跟踪了哪些内容。 git rm --cached &amp;lt;file&amp;gt;：如果一些文件已经被track了，但实际上并不想track，可以使用该命令将其移去。参考使用.gitignore文件.gitignore为一纯文本文件，显式地告诉Git哪些文件不要被track，可参见bmcblog。一个文件示例如下：# Binaries for programs and plugins*.exe*.dll*.so# Dependency directoriesvendor/版本管理撤销在暂存区的内容已经执行了git add把文件加到了暂存区，但是想撤销这次操作，可使用：参考git reset回退版本该操作把分支内容直接回退回老版本。Git版本回退一般有两种方式： 使用HEAD标识。在Git中，用HEAD表示上一次commit后的版本 git reset --hard HEAD 使用commit id。git log和git reflog可获取历史版本的commit id，使用 git reset --hard &amp;lt;commit id&amp;gt; 可以恢复到对应的版本。commit id不用写全，一般写前面5位就可以了，Git会自动补全。 使用git reflog查看commit id git reset只会回滚modified的文件，新增的文件在reset后仍存在。几个重要且常用的命令 git log：查看历史修改记录。 git log --pretty=oneline:：历史记录简洁输出。 输出的前面一串数字是commit id(版本号)，是标识commit版本的依据。 当回退到旧版本时，使用该命令将只会输出该旧版本之前的历史记录（像是时光穿梭到了旧版本对应的时刻），如果想要查看包含以往每一次的操作，可使用下面的这条git reflog命令。 git reflog：查看使用过的每一次命令以及对应的版本号。分支管理Git分支原理Git把每次的提交串成一条时间线，一条时间线就是一个分支，分支间的切换实质上是HEAD指向的切换。详见廖雪峰-创建与合并分支。Git中鼓励大量使用分支。Git分支原理创建与切换分支 创建分支：git branch &amp;lt;branch_name&amp;gt; 切换至分支：git checkout &amp;lt;branch_name&amp;gt; 或 git switch &amp;lt;branch_name&amp;gt; 创建并切换至该分支(等效与上述两条命令)：git branch -b &amp;lt;branch_name&amp;gt; 或 git switch -c &amp;lt;branch_name&amp;gt; 查看当前所有分支：git branch 当前分支前会标有星号。 合并分支使用git merge &amp;lt;branch_A&amp;gt;将指定分支branch_A合并到当前分支上。默认合并模式是Fast-forward模式。Fast-foward模式可以无痛作用于在一个分支上进行单线延伸后的合并。 fast forward 和 no fast foward合并模式区别 如果执行了fast forward，开发者根本看不到被合并的分支，就像在master上直接commit一样(参考-segmentfault)。 合并分支时，加上--no-ff参数就可以用普通模式合并，即git merge --no-ff &amp;lt;branch_A&amp;gt; 合并过程中的冲突解决当从同一节点分叉出的两个分支分别在同一位置做了不同的修改，那么直接git merge将提示存在文件出现冲突。此时必须手动解决冲突后再提交，具体操作为： 直接在本地打开提示冲突的文件，在发生冲突的地方进行编辑，解决冲突； git merge的合并冲突结果会直接反映在工作区，也就是说在工作区冲突文件对应位置处增加了显式的提示，具体格式为：用&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;，=======，&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;标记出不同分支的内容，比如 &amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt; HEAD Creating a new branch is quick &amp;amp; simple. ======= Creating a new branch is quick AND simple. &amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; feature1 进行正常的提交操作，即依次进行git add和git commit即可。删除分支使用git branch -d &amp;lt;branch_name&amp;gt;删除指定分支。分支管理策略实际开发中，应该按照几个基本原则进行分支管理： master分支应该是非常稳定的，始终拥有(存档)着最稳健的版本； dev分支是不稳定的，有了稳定的功能后再合并到master分支上； 团队合作时，每个人都在dev分支上干活，每个人都有自己的分支，时不时地往dev分支上合并。团队合作的分支“修改操作”的复制想象一个场景，master分支上有一个bug，于是在master分支上创建临时分支issue-101，然后在issue-101分支上修复后提交(对应&amp;lt;commit_id&amp;gt;)，然后将切换到master分支，将issue-101分支合并。master分支上的bug修复了，但是因为dev分支是从早期master分支分出来的，所以也存在这个bug，这个时候可以切换到dev分支，执行git cherry-pick &amp;lt;commit_id&amp;gt;将issue-101在&amp;lt;commit_id&amp;gt;对应的那次提交里所作的“修改操作”复制到dev分支上。dev分支上的那个bug也就被解决了。 :satisfied: 这里有一个可视化交互的Git分支教程，挺有意思。标签管理创建标签Git的标签(tag)实际上就是一个指向某个commit的指针，与commit_id作用实际上是一样的，但是更适合人类记忆。另外，对应里程碑性的提交，也可以特意为其打上一个tag。打tag的方法为git tag &amp;lt;tag_name&amp;gt;默认tag是打在最新提交的commit上的。如果想要为某一个commit_id打上tag，可以git tag &amp;lt;tag_name&amp;gt; &amp;lt;commit_id&amp;gt;查看标签使用git tag查看当前所有标签。标签不是按时间顺序列出，而是按字母排序的。使用git show &amp;lt;tag_name&amp;gt;查看标签信息（该标签对应的commit_id、提交作者、日期、所作的修改等）。操作标签使用git tag -d &amp;lt;tag_name&amp;gt;删除标签。使用git push origin &amp;lt;tag_name&amp;gt;将标签对应的提交推送到远程。远程仓库初次使用Windows初次使用自己的远程仓库需要创建SSH key来在github上进行Git仓库托管。(注：Windows下主目录为C:\\Users\\用户名\\)Ubuntu同样需要创建SSH key来在github上进行Git仓库托管。(注： Ubuntu的ssh文件在~/.ssh)Git操作原理图添加远程库当先有本地仓库，希望同步到远程仓库时： 在github上新建一个远程仓库； 把本地仓库和远程仓库关联； git remote add origin git@github.com:&amp;lt;account_name&amp;gt;/&amp;lt;repo_name&amp;gt;.git 这里的origin是设定的远程库的名字，后面跟的是仓库的地址。origin是Git默认的叫法，也可以改为其他名字。 把本地库的所有内容推送到远程库。 git push -u origin master 即将本地master分支的内容推送到远程库origin的master分支上(在本地master分支推送就推到远程的master分支上，在本地dev分支推送就对应推送到远程的dev分支上)。由于初次提交时，远程库是空的，这里还加了参数-u，它把本地的master分支和远程的master分支关联起来（注意这里有一个本地分支和远程分支的关联问题，后面会提到多次），简化后续推送的命令。在以后的推送，只需 git push origin master 克隆远程仓库当已有远程仓库，希望克隆到本地时：git clone &amp;lt;repo_addr&amp;gt;当从远程仓库克隆时，在下载文件的同时，也自动把本地的master分支和远程的master分支关联起来了，并且，远程仓库的默认名称是origin。github为仓库提供了多个地址(https协议、ssh协议)，不过优先选用ssh协议，速度更快而且方便。比如git clone git@github.com:ZhangGe6/onnx-modifier.git 可能遇到的问题：fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. 解决：检查本地电脑是否已经创建SSH key(新电脑有时会忘记)。 远程克隆别人的仓库时，默认只能看到master分支，如果需要看到其他分支，比如dev分支时，则需要git checkout -b dev origin/dev远程分支管理查看远程库的信息：使用git remote查看远程库的信息， 加上-v参数可以显示更详细的信息，即pull和push的权限。$ git remoteorigin$ git remote -vorigin git@github.com:michaelliao/learngit.git (fetch)origin git@github.com:michaelliao/learngit.git (push)推送分支推送分支，就是把指定分支上的所有本地提交推送到远程库，使用git push origin &amp;lt;branch_name&amp;gt;即将&amp;lt;branch_name&amp;gt;指定的分支推送到远程对应分支。 注：实际推送哪个分支与当前在哪个分支无关。也就是说，即便当前处于dev分支，执行git push origin master会把本地的master分支推送到远程master分支，而不是远程dev分支云云。抓取分支抓取分支，就是把远程库中的（当前工作区所在分支在远程库中关联的）分支直接抓取下来到工作区，使用git pull如果出现There is no tracking information for the current branch....If you wish to set tracking information for this branch you can do so with: git branch --set-upstream-to=origin/&amp;lt;branch&amp;gt; &amp;lt;cur_local_branch&amp;gt;则按照提示执行git branch --set-upstream-to=origin/&amp;lt;remote_branch&amp;gt; &amp;lt;cur_local_branch&amp;gt;即将远程库中的remote_branch分支和本地cur_local_branch关联起来，以后在cur_local_branch下执行git pull操作时都会从关联的remote_branch进行抓取。冲突解决推送冲突解决当两次push发生冲突（比如两次push在对应同一个位置做了不同的修改）时，则需要把上一次的推送pull下来，在本地解决冲突后，重新push上去。抓取冲突解决抓取冲突的解决，实质上就是本地的冲突解决了。在对应冲突的地方做好增删保留即可。协作： How to create a pull request in GitHub Checking out pull requests locally致谢以上内容根据廖雪峰老师的Git教程总结摘取而来。廖雪峰老师的这份教程使用具体实例，深入浅出地讲解了常用的Git的原理和常用操作，非常适合初学者，跟着实际操作一番后收获颇丰。感谢廖老师的工作！其他操作以下是一些平时使用中积累的其他操作。Git 版本升级(至最新版本)参考 查看当前Git版本：git --version 升级Git版本： 添加源：sudo add-apt-repository ppa:git-core/ppa 更新安装列表：sudo apt-get update 升级Git：sudo apt-get install git Partial Clone参考使用Github镜像Gitee廖雪峰的官方网站-使用Gitee亲测clone那真是快得不是一点两点..Git状态码参考 M: modified A: added D: deleted R: renamed C: copied U: updated but unmergedGit从历史记录中删除大文件和查看文件大小参考查看个人在github的所有comment点击这个神奇的链接： https://github.com/notifications/subscriptions?reason=comment参考trouble shootssh: Could not resolve hostname github.com: Name or service not knownssh -T git@github.com参考" }, { "title": "Ubuntu基础操作", "url": "/posts/Ubuntu%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/", "categories": "实践, 基础", "tags": "ubuntu", "date": "2020-10-14 17:06:00 +0800", "snippet": "常用命令行 删除：rm -rf &amp;lt;file_or_folder_name&amp;gt; 复制：ctrl+insert / 光标选中 粘贴：Shift + insert / 滚轮按下 / 触摸板双指 重命名/移动文件：mv A B（将A重命名为/移动到B） 查看内存使用：top 查看本机硬盘信息 查看已挂载的硬盘信息：df -h 查看本机上所有（包括未挂载的）硬盘信息：lsblk 关于硬盘的挂载与卸载，可以参考这里 修改文件权限：chmod Linux下文件的权限类型一般包括读r，写w，执行x三种；Linux下权限的粒度有拥有者u 、群组g 、其它组o三种。使用数字表示权限：规定数字 4 、2 和 1表示读、写、执行权限，则同时增加读、写、执行则对应数字4+2+1=7，为拥有者u 、群组g 、其它组o均赋予上述权限的命令为。 参考：Linux权限详解 给某一文件增加权限：chmod 777 &amp;lt;fileName&amp;gt; 给文件夹下所有文件增加权限：chmod -R 777 &amp;lt;dirName&amp;gt; 刷新ldconfig动态库缓存文件：ldconfig 不管做了什么关于library的变动后，尤其是最好都ldconfig一下，否则可能发生找不到链接库的错误。 查看程序运行所需要的共享库：ldd &amp;lt;exe_name&amp;gt; 常用来解决因为缺少某些库文件或链接至错误库导致程序不能正常运行的问题。 通过符号(symbol)来还原函数名称：c++filt &amp;lt;symbol&amp;gt; 比如遇到undefined symbol报错时可根据后面给出的symbol进行函数还原，进而进行错误定位。 参考：C++中的名字修饰 实时查看GPU使用情况：watch nvidia-smi 清空终端： clear 或reset 软链接： 创建：ln -s 源文件 目标文件 删除：unlink 已有的软连接 使用ssh登录远程服务器：ssh &amp;lt;username&amp;gt;@&amp;lt;ip_addr&amp;gt; -p &amp;lt;port&amp;gt; 文件信息相关： 显示隐藏文件：ls -a 查看文件大小(权限等)：ll -h (递归)显示当前目录下所有文件夹大小：du -h 显示当前目录的所有文件总大小：du -sh 仅显示第一层级的目录大小：du -h --max-depth=1 网络相关 查看网络接口配置信息：ifconfig [-a] 参考 查看进程信息 top命令 按c可展开显示具体的执行命令command 根据进程号pid查看进程信息：ps -p &amp;lt;pid&amp;gt; -f -p为--pid的缩写，后跟进程号；-f指定展开显示执行的命令（It causes the command arguments to be printed.） 参考 一些工具命令grep命令从files中寻找满足pattern的文件(参考)：grep -rn &amp;lt;pattern&amp;gt; &amp;lt;files&amp;gt;find命令在path下搜寻文件名为name的文件：find &amp;lt;path&amp;gt; -name &amp;lt;name&amp;gt;scp命令参考scp是 secure copy 的缩写，相当于cp命令 + SSH，用来在两台主机之间加密传送文件。基本用法为：scp source destination**示例**# 从远程复制到本地scp [-P xxx] user@host:&amp;lt;scr_path&amp;gt; &amp;lt;dst_path&amp;gt;# 从本地复制到远程scp &amp;lt;scr_path&amp;gt; user@host:&amp;lt;dst_path&amp;gt; 解压和压缩文件参考 Tips：包含大量小文件的文件夹，打包/压缩后传输可极大提升传输效率。.tar.gz文件、 .tgz文件# .tar.gz 和 .tgztar -zxvf FileName.tar.gz # 解压tar -zcvf FileName.tar.gz DirName # 压缩DirName和其下所有文件（夹）tar -C DesDirName -zxvf FileName.tar.gz # 解压到目标路径.rar文件# linux并没有自带rar，需要先下载: sudo apt install rarrar x FileName.rar # 解压rar a FileName.rar DirName # 压缩.zip文件# .zip文件unzip FileName.zip # 解压zip FileName.zip DirName # 压缩aptapt(Advanced Package Tool)命令是一个功能强大的命令行工具，它不仅可以更新软件包列表索引、执行安装新软件包、升级现有软件包，还能够升级整个 Ubuntu 系统(apt 是 Debian 系操作系统的包管理工具)。它的一些常用命令记录如下 安装包：sudo apt install &amp;lt;package_name&amp;gt; 安装本地的deb文件：sudo apt install name.deb 删除(使用apt install安装的)包：sudo apt remove &amp;lt;package_name&amp;gt;screen——管理你的远程程序当本地关闭终端时，会kill掉远程在该终端运行的程序。screen工具可以将程序放入后台运行，让用户安全地在本地关闭终端。这对程序运行时间较长，但本地需要频繁开关终端的情况尤其有帮助。妈妈再也不用担心我边骑自行车边监视笔记本终端了（雾。创建screen窗口并运行程序 打开并命名一个新的screen窗口：screen -S &amp;lt;screen_name&amp;gt; 也可直接screen创建一个默认窗口，该窗口会获得一个自动分配的窗口名，但该窗口名不便于记忆。 在该screen窗口内运行你的程序：python a.py 此时可以安全关掉该终端（或 Ctrl+a，再按下d离开该screen窗口） 恢复程序 查看后台运行程序：screen -ls 恢复某个程序，可采用两种方法： 根据pid：screen -r &amp;lt;pid&amp;gt;（pid对应list前的数字） 根据screen窗口名：screen -r &amp;lt;screen_name&amp;gt; list中正常可恢复程序状态为detached，若程序状态因不明原因变为attached，则无法正常恢复，此时使用screen -D -r &amp;lt;pid&amp;gt;，先踢掉前一用户，再登陆。link 据说nohup命令也可实现上述类似效果，而且可以将输出写入一个日志文件。需要时可以了解下。销毁screen窗口当前screen窗口工作完成后，在该screen窗口下执行$ exit即可销毁并退出该窗口。其他当screen窗口下无法上下翻行时，先按下 Ctrl+a，再按下[即可翻行。信息查询查看程序信息 查看哪些路径安装了该程序：whereis &amp;lt;exe_name&amp;gt; 查看默认选择的版本路径：which &amp;lt;exe_name&amp;gt; 查看默认选择的版本：&amp;lt;exe_name&amp;gt; --version 使用pip查看程序信息总览：pip show &amp;lt;package_name&amp;gt; 查看文件传输进度：progress工具 github repo，可监控cp, mv, dd, tar, gzip/gunzip, cat的进度 使用方法，如在终端输入watch progress -q即可显示正在监控的进程进度 查看cpu信息：lscpu 查看GPU信息（NVIDIA）： lspci | grep -i nvidia （如果输出版本号为十六进制数，则到The PCI ID Repository See also一栏进一步查询） nvidia-smi 根据进程号查看进程详细信息：ll /proc/&amp;lt;PID&amp;gt;查看系统信息 查看系统版本：more /etc/lsb-release or：cat /etc/issue 其他更改硬盘名称参考（在桌面环境下）Show Applications -&amp;gt; Disks -&amp;gt; select disk to change name -&amp;gt; setting icon -&amp;gt; edit filesystem -&amp;gt; change -&amp;gt; donekworker &amp;amp; ksoftirqd using high CPU constantly重新安装系统后，发现kworker和ksoftirqd持续占用80+%和20+%的内存。这个问题竟然在插入装系统的U盘后就自动消失。但是我的U盘也不能一直插在电脑上，所以找了一下解决办法，尝试有效（disable USB autosuspend）参考1 参考2Ubuntu 18.04 gnome-shell high CPU usage参考可以尝试重启gnome，做法：Alt+F2，然后在弹出的对话框中输入r，点击确认。Linux环境变量关于Linux的环境变量，可参考这篇终端历史命令自动补全参考 打开/etc/inputrc文件； 取消history-search-backward和history-search-forward两行前的注释。No space left on the device临时解决办法：去~/.cache/中删除一些缓存文件。" }, { "title": "Anaconda使用", "url": "/posts/Anaconda%E4%BD%BF%E7%94%A8/", "categories": "实践, 环境配置", "tags": "anaconda", "date": "2020-10-14 14:53:00 +0800", "snippet": "安装与配置Anaconda安装Anaconda参考 下载.sh安装文件：清华镜像源 运行.sh文件安装 sh ./Anaconda&amp;lt;_version_&amp;gt;-Linux-x86_64.sh 安装过程会询问安装位置。建议安装位置尽量不要放在默认的主目录，而是放在一个空间更加宽裕的其他地方。否则日后使用可能会占满主目录，影响机器使用。 安装过程会询问：‘Do you wish the installer to initialize Anaconda3…’,，默认是no，但是官方建议选择yes 参考 更新环境变量 source ~/.bashrc 测试 conda Anaconda配置 添加清华源 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --set show_channel_urls yes 创建新环境 conda create -n &amp;lt;env_name&amp;gt; python=3.8 通过复制已有的环境创建新环境 conda create -n BBB --clone AAA 其中BBB是要创建的新环境，AAA是已有的环境。 进入/退出环境 conda activate env_name conda deactivate env_name 删除环境 conda env remove -n &amp;lt;env_name&amp;gt; 查看目前创建的所有环境列表 conda env list 修改镜像源(参考1 参考2) 显示当前源：conda config --show 删除当前源：conda config --remove channels &amp;lt;channel_to_remove&amp;gt; 添加新源：conda config --add channels &amp;lt;channel_to_add&amp;gt; 可选的其他源(除上述清华源外) 上海交通大学镜像源 https://mirrors.sjtug.sjtu.edu.cn/anaconda/pkgs/main/https://mirrors.sjtug.sjtu.edu.cn/anaconda/pkgs/free/https://mirrors.sjtug.sjtu.edu.cn/anaconda/cloud/conda-forge/ 科大镜像源 https://mirrors.ustc.edu.cn/anaconda/pkgs/main/https://mirrors.ustc.edu.cn/anaconda/pkgs/free/https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/ 使用过程中遇到的问题及解决 ModuleNotFoundError: No module named &#39;conda&#39; 参考 【原因】：（每一个版本的conda都有其对应的python版本号，）在base环境安装的Python包使python版本发生改变，从而导致conda不能够正常使用。 【解决】：在原安装位置（安装过程中指定）重新安装原先版本的Anaconda，但是为了保留我们用Anaconda建的虚拟环境，即Anaconda/envs下的所有文件，这样我们就不用重新搭建之前的虚拟环境。只要在安装的时候使用参数：-u 即可，如 sh Anaconda3-5.2.0-Linux-x86_64.sh -u 【经验】：不要在base环境下安装可能导致Python版本号发生改变的包。比如这次就是在base环境下安装Pytorch 1.7.1，使得Python版本从3.7变为3.6导致的。 conda瘦身 conda clean -p # 删除没用的包conda clean -t # 删除保存的tar包 参考 使用Ubuntu的话，Anaconda默认安装在主目录下。当Anaconda文件夹太大影响到机器使用，尽管这个命令可以解燃眉之急。但最好是安装时就把Anaconda安装在其他有较大空间的位置。 安装conda后，启动终端或者执行conda需要漫长的等待时间。 解决办法 我自己电脑上的经验：如果仍然不行，尝试先conda activate &amp;lt;env&amp;gt;，等待激活成功后再（根据提示）conda init一下，一般可以解决。 " }, { "title": "g++编译学习", "url": "/posts/g++%E7%BC%96%E8%AF%91%E5%AD%A6%E4%B9%A0/", "categories": "实践, 硬技能", "tags": "g++", "date": "2020-10-10 20:00:00 +0800", "snippet": "动机之前对g++/gcc编译过程和编译选项只有浅显了解，因为项目需要，将其学习一下。以下是学习笔记。g++和gcc GNU Compiler Collection即GUN 编译器集合，它可以编译C、C++、JAV、Fortran、Pascal、Object-C、Ada等语言。gcc是GCC中的GUN C Compiler（C 编译器）；g++是GCC中的GUN C++ Compiler（C++编译器）。 编译时g++会自动链接标准库STL，而gcc不会自动链接STL(使用STL，需要加参数 –lstdc++)。 g++编译过程 g++编译程序主要经过四个过程：1.预处理（Pre-Processing）2.编译 （Compiling）3.汇编 （Assembling）g++调用汇编器进行汇编，生成目标文件.o。4.链接 （Linking）将程序所需要的目标文件进行链接生成可执行文件。g++编译常用选项 选项名 作用 -o 指定目标名称 (缺省的时候,gcc 编译出来的文件是a.out) -c 只激活预处理,编译,和汇编,也就是只把程序做成obj文件 -Idir 将dir目录加入搜索头文件的目录路径 -Ldir 将dir目录加入搜索库的目录路径 -llib 链接库目录(-Ldir指定)下的lib库 -Wall 启用大部分警告信息 举几个栗子g++ -E hello.c -o hello.i # 对hello.c文件进行预处理，生成了hello.i 文件g++ -S hello.i -o hello.s # 对预处理文件进行编译，生成了汇编文件g++ -c hello.s -o hello.o # 对汇编文件进行编译，生成了【目标文件】g++ hello.o -o hello # 对目标文件进行链接，生成【可执行文件】g++ hello.c -o hello # 直接编译链接成【可执行目标文件】g++ test1.c test2.c -o test # 编译test1.c test2.c并链接生成可执行文件test头文件/库文件链接开发软件时，完全不使用第三方函数库的情况是比较少见的，通常来讲都需要借助许多函数库的支持才能够完成相应的功能。路径一般头文件的位置在 /usr/include及其子目录底下的include文件夹 /usr/local/include及其子目录底下的include文件夹库文件的位置在 /usr/lib /usr/local/lib但也有的时候，我们要用的头文件和库文件不在这些目录下，这时候需要我们使用-I和-L编译选项来指定搜索路径，比如：程序test.c是在linux上使用c连接mysql，这个时候我们需要去mysql官网下载MySQL Connectors的C库，下载下来解压之后，有一个include文件夹，里面包含头文件，还有一个lib文件夹，里面包含二进制so文件。那么头文件搜索目录为$(workPath)/include，lib文件夹是为$(workPath)/lib动态库/静态库Linux下的库文件分为两大类，分别是动态链接库（通常以.so结尾）和静态链接库（通常以.a结尾），二者的区别仅在于程序执行时所需的代码是在运行时动态加载的，还是在编译时静态加载的。 静态库（.a）：程序在编译链接的时候把库的代码链接到可执行文件中。程序运行的时候将不再需要静态库。 动态库（.so或.sa）：程序在运行的时候才去链接共享库的代码，多个程序共享使用库的代码，这样就减少了程序的体积。参考gcc和g++是什么关系？–知乎Linux编译工具：gcc入门Linux GCC常用命令gcc编译选项总结" }, { "title": "makefile学习", "url": "/posts/Makefile%E5%AD%A6%E4%B9%A0/", "categories": "实践, 硬技能", "tags": "makefile", "date": "2020-10-09 15:18:00 +0800", "snippet": "本文是一些自己学习makefile时的笔记。以下内容主要摘取自陈皓编写的跟我一起写Makefile，这是一个很好的学习资料。makefile介绍参考makefile的核心规则target : prerequisites commandtarget是目标文件，prerequisites是生成target的依赖文件。当target不存在，或prerequisites中如果有一个以上的文件比target文件要新的话，command所定义的命令就会被执行，command命令一定要由一个Tab键开头。在makefile末尾一般会看到，用来删除编译产生的中间文件等：clean : rm ...这里clean 不是一个文件，它只不过是一个动作名字（伪目标），其冒号后什么也没有，那么，make就不会自动去找它的依赖性，也就不会自动执行其后所定义的命令。要执行其后的命令，就要在make命令后明显得指出这个label的名字，如make clean。make是如何工作的make会在当前目录下找名字叫“Makefile”或“makefile”的文件，并根据该文件一层一层地去找文件的依赖关系，直到最终编译出第一个目标文件。详细可参见这里和这里。书写规则参考规则举例伪目标多目标与静态模式、$@ 表示目标的集合，就像一个数组， $@ 依次取出目标，并执于命令，可用于多目标。参考这里。静态模式可以更加容易地定义多目标的规则。其语法为&amp;lt;targets ...&amp;gt; : &amp;lt;target-pattern&amp;gt; : &amp;lt;prereq-patterns ...&amp;gt; &amp;lt;commands&amp;gt;举例：objects = foo.o bar.oall: $(objects)$(objects): %.o: %.c &amp;lt;command&amp;gt;其中%.o即为target-pattern，%.c为prereq-patterns。%.o: %.c的含义为：将objects中所有以.o的目标，后缀换为.c组成新的集合。使用变量参考变量基础 变量定义 VAL = ... 变量使用 $(VAL) 或 ${VAL} 变量会在使用时精确展开。 定义变量值时的=与:= =允许使用当前尚未定义好的变量作为右值，但可能出现递归定义的问题； :=强制要求只能使用前面已定义好了的变量作为右值。 变量高级用法 变量替换 # 定义变量foo := a.o b.o # 变量替换bar := $(foo:.o=.c) # bar = a.c b.c# 或bar := $(foo:%.o=%.c) # bar = a.c b.c 变量追加 foo := a.o b.ofoo += c.o # foo = a.o b.o c.o 自动化变量 $@：当前规则中，目标（或目标集）。 $^：当前规则中，所有依赖组成的的集合。 $&amp;lt;：当前规则中，第一个依赖。 举个栗子，对 exc: library.cpp main.cpp 那么，$@ 就指代 exc，$^指代library.cpp main.cpp，$&amp;lt;指代library.cpp。 如果@$指代的目标有多个（即是一个目标集）时，则将目标依次取出展开，执行command，参考 这种方式貌似对于多个相互独立的文件分别编译非常方便，尤其当文件非常多时。 举个栗子，对 objects = foo.o bar.o all: $(objects) $(objects): %.o: %.c $(CC) -c $(CFLAGS) $&amp;lt; -o $@ 展开后对应执行 foo.o : foo.c $(CC) -c $(CFLAGS) foo.c -o foo.obar.o : bar.c $(CC) -c $(CFLAGS) bar.c -o bar.o 使用条件判断参考 关键字ifeq，ifneq，判断是否相等。示例： ifeq ($(CC),gcc) $(CC) -o foo $(objects) $(libs_for_gcc) else $(CC) -o foo $(objects) $(normal_libs) endif 关键字ifdef，ifndef，判断变量是否有值。示例： foo =ifdef foo frobozz = yeselse frobozz = noendif # 该例中frobozz为no 使用函数函数的调用语法为：$(&amp;lt;function&amp;gt; &amp;lt;arguments&amp;gt;)# 或是${&amp;lt;function&amp;gt; &amp;lt;arguments&amp;gt;}makefile支持的函数可参照：字符串处理函数，文件名操作函数还有其他。" }, { "title": "Customize the Favicon", "url": "/posts/customize-the-favicon/", "categories": "Blogging, Tutorial", "tags": "favicon", "date": "2019-08-11 00:34:00 +0800", "snippet": "The favicons of Chirpy are placed in the directory assets/img/favicons/. You may want to replace them with your own. The following sections will guide you to create and replace the default favicons.Generate the faviconPrepare a square image (PNG, JPG, or SVG) with a size of 512x512 or more, and then go to the online tool Real Favicon Generator and click the button Select your Favicon image to upload your image file.In the next step, the webpage will show all usage scenarios. You can keep the default options, scroll to the bottom of the page, and click the button Generate your Favicons and HTML code to generate the favicon.As Real Favicon Generator only allows to create favicons from pictures that are up to 2 MB in size, the tool websiteplanet allows to create favicons from pictures that are up to 5 MB from either JPG, PNG or GIF or even from a gallery. It is worthy to try this if we care about the favicon size limit. (Thanks Emma for recommanding the tool !)website planet and click the button Generate!Download &amp;amp; ReplaceDownload the generated package, unzip and delete the following two from the extracted files: browserconfig.xml site.webmanifestAnd then copy the remaining image files (.PNG and .ICO) to cover the original files in the directory assets/img/favicons/ of your Jekyll site. If your Jekyll site doesn’t have this directory yet, just create one.The following table will help you understand the changes to the favicon files: File(s) From Online Tool From Chirpy *.PNG ✓ ✗ *.ICO ✓ ✗ ✓ means keep, ✗ means delete.The next time you build the site, the favicon will be replaced with a customized edition." }, { "title": "Getting Started", "url": "/posts/getting-started/", "categories": "Blogging, Tutorial", "tags": "getting started", "date": "2019-08-09 20:55:00 +0800", "snippet": "PrerequisitesFollow the instructions in the Jekyll Docs to complete the installation of Ruby, RubyGems, Jekyll, and Bundler. In addition, Git is also required to be installed.InstallationCreating a New SiteThere are two ways to create a new repository for this theme: Using the Chirpy Starter - Easy to upgrade, isolates irrelevant project files so you can focus on writing. Forking on GitHub - Convenient for custom development, but difficult to upgrade. Unless you are familiar with Jekyll and are determined to tweak or contribute to this project, this approach is not recommended.Option 1. Using the Chirpy StarterCreate a new repository from the Chirpy Starter and name it &amp;lt;GH_USERNAME&amp;gt;.github.io, where GH_USERNAME represents your GitHub username.Option 2. Forking on GitHubFork Chirpy on GitHub and rename it to &amp;lt;GH_USERNAME&amp;gt;.github.io. Please note that the default branch code is in development. If you want the site to be stable, please switch to the latest tag and start writing.And then execute:$ bash tools/init.sh If you don’t want to deploy your site on GitHub Pages, append option --no-gh at the end of the above command.The above command will: Removes some files or directories from your repository: .travis.yml files under _posts If the option --no-gh is provided, the directory .github will be deleted. Otherwise, set up the GitHub Action workflow by removing the extension .hook of .github/workflows/pages-deploy.yml.hook, and then remove the other files and directories in the folder .github. Removes item Gemfile.lock from .gitignore. Creates a new commit to save the changes automatically.Installing DependenciesBefore running for the first time, go to the root directory of your site, and install dependencies as follows:$ bundleUsageConfigurationUpdate the variables of _config.yml as needed. Some of them are typical options: url avatar timezone langCustoming StylesheetIf you need to customize the stylesheet, copy the theme’s assets/css/style.scss to the same path on your Jekyll site, and then add the custom style at the end of the style file.Starting from v4.1.0, if you want to overwrite the SASS variables defined in _sass/addon/variables.scss, create a new file _sass/variables-hook.scss and assign new values to the target variable in it.Customing Static AssetsStatic assets configuration was introduced in version 5.1.0. The CDN of the static assets is defined by file _data/assets/cross_origin.yml, and you can replace some of them according to the network conditions in the region where your website is published.Also, if you’d like to self-host the static assets, please refer to the chirpy-static-assets.Running Local ServerYou may want to preview the site contents before publishing, so just run it by:$ bundle exec jekyll sOr run the site on Docker with the following command:$ docker run -it --rm \\ --volume=&quot;$PWD:/srv/jekyll&quot; \\ -p 4000:4000 jekyll/jekyll \\ jekyll serveAfter a while, the local service will be published at http://127.0.0.1:4000.DeploymentBefore the deployment begins, check out the file _config.yml and make sure the url is configured correctly. Furthermore, if you prefer the project site and don’t use a custom domain, or you want to visit your website with a base URL on a web server other than GitHub Pages, remember to change the baseurl to your project name that starts with a slash, e.g, /project-name.Now you can choose ONE of the following methods to deploy your Jekyll site.Deploy by Using Github ActionsFor security reasons, GitHub Pages build runs on safe mode, which restricts us from using plugins to generate additional page files. Therefore, we can use GitHub Actions to build the site, store the built site files on a new branch, and use that branch as the source of the GitHub Pages service.Quickly check the files needed for GitHub Actions build: Ensure your Jekyll site has the file .github/workflows/pages-deploy.yml. Otherwise, create a new one and fill in the contents of the sample file, and the value of the on.push.branches should be the same as your repo’s default branch name. Ensure your Jekyll site has file tools/deploy.sh. Otherwise, copy it from here to your Jekyll site. Furthermore, if you have committed Gemfile.lock to the repo, and your runtime system is not Linux, don’t forget to update the platform list in the lock file: $ bundle lock --add-platform x86_64-linux After the above steps, rename your repository to &amp;lt;GH_USERNAME&amp;gt;.github.io on GitHub.Now publish your Jekyll site by: Push any commit to remote to trigger the GitHub Actions workflow. Once the build is complete and successful, a new remote branch named gh-pages will appear to store the built site files. Browse to your repository on GitHub. Select the tab Settings, then click Pages in the left navigation bar, and then in the section Source of GitHub Pages, select the /(root) directory of branch gh-pages as the publishing source. Remember to click Save before leaving. Visit your website at the address indicated by GitHub. Manually Build and DeployOn self-hosted servers, you cannot enjoy the convenience of GitHub Actions. Therefore, you should build the site on your local machine and then upload the site files to the server.Go to the root of the source project, and build your site as follows:$ JEKYLL_ENV=production bundle exec jekyll bOr build the site on Docker:$ docker run -it --rm \\ --env JEKYLL_ENV=production \\ --volume=&quot;$PWD:/srv/jekyll&quot; \\ jekyll/jekyll \\ jekyll buildUnless you specified the output path, the generated site files will be placed in folder _site of the project’s root directory. Now you should upload those files to the target server.UpgradingIt depends on how you use the theme: If you are using the theme gem (there will be gem &quot;jekyll-theme-chirpy&quot; in the Gemfile), editing the Gemfile and update the version number of the theme gem, for example: - gem &quot;jekyll-theme-chirpy&quot;, &quot;~&amp;gt; 3.2&quot;, &quot;&amp;gt;= 3.2.1&quot;+ gem &quot;jekyll-theme-chirpy&quot;, &quot;~&amp;gt; 3.3&quot;, &quot;&amp;gt;= 3.3.0&quot; And then execute the following command: $ bundle update jekyll-theme-chirpy As the version upgrades, the critical files (for details, see the Startup Template) and configuration options will change. Please refer to the Upgrade Guide to keep your repo’s files in sync with the latest version of the theme. If you forked from the source project (there will be gemspec in the Gemfile of your site), then merge the latest upstream tags into your Jekyll site to complete the upgrade.The merge is likely to conflict with your local modifications. Please be patient and careful to resolve these conflicts. " }, { "title": "Writing a New Post", "url": "/posts/write-a-new-post/", "categories": "Blogging, Tutorial", "tags": "writing", "date": "2019-08-08 14:10:00 +0800", "snippet": "This post will guide you how to write a post on Chirpy theme. Even if you have previous experience with Jekyll, this article is worth reading, because many features require specific variables to be set.Naming and PathCreate a new file named YYYY-MM-DD-TITLE.EXTENSION and put it in the _posts of the root directory. Please note that the EXTENSION must be one of md and markdown. If you want to save time of creating files, please consider using the plugin Jekyll-Compose to accomplish this.Front MatterBasically, you need to fill the Front Matter as below at the top of the post:---title: TITLEdate: YYYY-MM-DD HH:MM:SS +/-TTTTcategories: [TOP_CATEGORIE, SUB_CATEGORIE]tags: [TAG] # TAG names should always be lowercase--- The posts’ layout has been set to post by default, so there is no need to add the variable layout in the Front Matter block.Timezone of DateIn order to accurately record the release date of a post, you should not only set up the timezone of _config.yml but also provide the post’s timezone in variable date of its Front Matter block. Format: +/-TTTT, e.g. +0800.Categories and TagsThe categories of each post are designed to contain up to two elements, and the number of elements in tags can be zero to infinity. For instance:---categories: [Animal, Insect]tags: [bee]---Author InformationThe author information of the post usually does not need to be filled in the Front Matter , they will be obtained from variables social.name and the first entry of social.links of the configuration file by default. But you can also override it as follows:Add author information in _data/authors.yml (If your website doesn’t have this file, don’t hesitate to create one.)&amp;lt;author_id&amp;gt;: name: &amp;lt;full name&amp;gt; twitter: &amp;lt;twitter_of_author&amp;gt; url: &amp;lt;homepage_of_author&amp;gt;And then set up the custom author in the post’s YAML block:---author: &amp;lt;author_id&amp;gt;--- Another benefit of reading the author information from the file _data/authors.yml is that the page will have the meta tag twitter:creator, which enriches the Twitter Cards and is good for SEO.Table of ContentsBy default, the Table of Contents (TOC) is displayed on the right panel of the post. If you want to turn it off globally, go to _config.yml and set the value of variable toc to false. If you want to turn off TOC for a specific post, add the following to the post’s Front Matter:---toc: false---CommentsThe global switch of comments is defined by variable comments.active in the file _config.yml. After selecting a comment system for this variable, comments will be turned on for all posts.If you want to close the comment for a specific post, add the following to the Front Matter of the post:---comments: false---MathematicsFor website performance reasons, the mathematical feature won’t be loaded by default. But it can be enabled by:---math: true---MermaidMermaid is a great diagrams generation tool. To enable it on your post, add the following to the YAML block:---mermaid: true---Then you can use it like other markdown languages: surround the graph code with ```mermaid and ```.ImagesCaptionAdd italics to the next line of an image，then it will become the caption and appear at the bottom of the image:![img-description](/path/to/image)_Image Caption_SizeIn order to prevent the page content layout from shifting when the image is loaded, we should set the width and height for each image:![Desktop View](/assets/img/sample/mockup.png){: width=&quot;700&quot; height=&quot;400&quot; }Starting from Chirpy v5.0.0, height and width support abbreviations (height → h, width → w). The following example has the same effect as the above:![Desktop View](/assets/img/sample/mockup.png){: w=&quot;700&quot; h=&quot;400&quot; }PositionBy default, the image is centered, but you can specify the position by using one of the classes normal, left, and right. Once the position is specified, the image caption should not be added. Normal position Image will be left aligned in below sample: ![Desktop View](/assets/img/sample/mockup.png){: .normal } Float to the left ![Desktop View](/assets/img/sample/mockup.png){: .left } Float to the right ![Desktop View](/assets/img/sample/mockup.png){: .right } ShadowThe screenshots of the program window can be considered to show the shadow effect, and the shadow will be visible in the light mode:![Desktop View](/assets/img/sample/mockup.png){: .shadow }CDN URLIf you host the images on the CDN, you can save the time of repeatedly writing the CDN URL by assigning the variable img_cdn of _config.yml file:img_cdn: https://cdn.comOnce img_cdn is assigned, the CDN URL will be added to the path of all images (images of site avatar and posts) starting with /.For instance, when using images:![The flower](/path/to/flower.png)The parsing result will automatically add the CDN prefix https://cdn.com before the image path:&amp;lt;img src=&quot;https://cdn.com/path/to/flower.png&quot; alt=&quot;The flower&quot;&amp;gt;Image PathWhen a post contains many images, it will be a time-consuming task to repeatedly define the path of the images. To solve this, we can define this path in the YAML block of the post:---img_path: /img/path/---And then, the image source of Markdown can write the file name directly:![The flower](flower.png)The output will be:&amp;lt;img src=&quot;/img/path/flower.png&quot; alt=&quot;The flower&quot;&amp;gt;Preview ImageIf you want to add an image to the top of the post contents, specify the attribute path, width, height, and alt for the image:---image: path: /path/to/image/file width: 1000 # in pixels height: 400 # in pixels alt: image alternative text---Except for alt, all other options are necessary, especially the width and height, which are related to user experience and web page loading performance. The above section “Size” also mentions this.Starting from Chirpy v5.0.0, the attributes height and width can be abbreviated: height → h, width → w. In addition, the img_path can also be passed to the preview image, that is, when it has been set, the attribute path only needs the image file name.Pinned PostsYou can pin one or more posts to the top of the home page, and the fixed posts are sorted in reverse order according to their release date. Enable by:---pin: true---PromptsThere are several types of prompts: tip, info, warning, and danger. They can be generated by adding the class prompt-{type} to the blockquote. For example, define a prompt of type info as follows:&amp;gt; Example line for prompt.{: .prompt-info }SyntaxInline Code`inline code part`Filepath Hightlight`/path/to/a/file.extend`{: .filepath}Code BlockMarkdown symbols ``` can easily create a code block as follows:```This is a plaintext code snippet.```Specifying LanguageUsing ```{language} you will get a code block with syntax highlight:```yamlkey: value``` The Jekyll tag {% highlight %} is not compatible with this theme.Line NumberBy default, all languages except plaintext, console, and terminal will display line numbers. When you want to hide the line number of a code block, add the class nolineno to it:```shellecho &#39;No more line numbers!&#39;```{: .nolineno }Specifying the FilenameYou may have noticed that the code language will be displayed at the top of the code block. If you want to replace it with the file name, you can add the attribute file to achieve this:```shell# content```{: file=&quot;path/to/file&quot; }Liquid CodesIf you want to display the Liquid snippet, surround the liquid code with {% raw %} and {% endraw %}:{% raw %}```liquid{% if product.title contains &#39;Pack&#39; %} This product&#39;s title contains the word Pack.{% endif %}```{% endraw %}Or adding render_with_liquid: false (Requires Jekyll 4.0 or higher) to the post’s YAML block.Learn MoreFor more knowledge about Jekyll posts, visit the Jekyll Docs: Posts." }, { "title": "Text and Typography", "url": "/posts/text-and-typography/", "categories": "Blogging, Demo", "tags": "typography", "date": "2019-08-08 11:33:00 +0800", "snippet": "This post is to show Markdown syntax rendering on Chirpy, you can also use it as an example of writing. Now, let’s start looking at text and typography.TitlesH1 - headingH2 - headingH3 - headingH4 - headingParagraphI wandered lonely as a cloudThat floats on high o’er vales and hills,When all at once I saw a crowd,A host, of golden daffodils;Beside the lake, beneath the trees,Fluttering and dancing in the breeze.ListsOrdered list Firstly Secondly ThirdlyUnordered list Chapter Section Paragraph Task list TODO Completed Defeat COVID-19 Vaccine production Economic recovery People smile again Description list Sun the star around which the earth orbits Moon the natural satellite of the earth, visible by reflected light from the sunBlock Quote This line shows the block quote.Prompts An example showing the tip type prompt. An example showing the info type prompt. An example showing the warning type prompt. An example showing the danger type prompt.Tables Company Contact Country Alfreds Futterkiste Maria Anders Germany Island Trading Helen Bennett UK Magazzini Alimentari Riuniti Giovanni Rovelli Italy Linkshttp://127.0.0.1:4000FootnoteClick the hook will locate the footnote1, and here is another footnote2.Images Default (with caption)Full screen width and center alignment Shadowshadow effect (visible in light mode) Left aligned Float to left “A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space.” Float to right “A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space.” Mermaid SVG gantt title Adding GANTT diagram functionality to mermaid apple :a, 2017-07-20, 1w banana :crit, b, 2017-07-23, 1d cherry :active, c, after b a, 1dMathematicsThe mathematics powered by MathJax:\\[\\sum_{n=1}^\\infty 1/n^2 = \\frac{\\pi^2}{6}\\]When $a \\ne 0$, there are two solutions to $ax^2 + bx + c = 0$ and they are\\[x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}\\]Inline codeThis is an example of Inline Code.FilepathHere is the /path/to/the/file.extend.Code blockCommonThis is a common code snippet, without syntax highlight and line number.Specific LanguagesConsole$ env |grep SHELLSHELL=/usr/local/bin/bashPYENV_SHELL=bashShellif [ $? -ne 0 ]; then echo &quot;The command was not successful.&quot;; #do the needful / exitfi;Specific filename@import &quot;colors/light-typography&quot;, &quot;colors/dark-typography&quot;Reverse Footnote The footnote source &amp;#8617; The 2nd footnote source &amp;#8617; " } ]
